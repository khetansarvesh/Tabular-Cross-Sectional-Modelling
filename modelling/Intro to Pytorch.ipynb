{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["axQdElTuFkB0","d5MYlxfyFyHE","OLdSN9ZXvrM0","c78_3AMEyvJd","QuLzDzsoytM2","dhtcBgum3OZ3","t6tqf7v38vbi","7LEjeR24MLkN","VgpkRn527zSr","JBt6e4xZT3zr","Y1vtN4Dy8FAG","_M1U_RTpBhl2","V7BMktFFAkRA","GltnmDzeIXJM","Re8xiL37eAja","pYLWqKIoaOyd","5IfLaLOu1ks9","gFF0N2TU7S7Q","dms7G4nkTGe5","0MQNTY0eTGe6","hib1NYrSarL2","huKZ6QlYTGe7","gUjkB2AX7Upz","BdiWvoAi7UjL","Sk_6Dd7L7Uce","aJ4DDmo1TGe-","nEPqVL7fTGfC","h8ZaW0Bq7rCm","7gU3ubCrUkI-","hxIIM7t27rQ-","u6RjOsOokieC"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"g7rzELDljcPB"},"source":["#importing library to print multiple lines in one code shell\n","from IPython.core.interactiveshell import InteractiveShell\n","InteractiveShell.ast_node_interactivity = \"all\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"axQdElTuFkB0"},"source":["# **Without Using Any Library**"]},{"cell_type":"code","metadata":{"id":"bJmpofrOFo_x"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d5MYlxfyFyHE"},"source":["# **Using SKLearn Library**"]},{"cell_type":"code","metadata":{"id":"4aLwP-x5F12t"},"source":[],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"H6oqGiIXvrMl"},"source":["# **Using PyTorch Library**\n","\n","[Github Repo](https://github.com/pytorch) |\n","[Official PyTorch Documentation](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) | [PyTorch](https://pytorch.org/) | [Discussion Forum](https://discuss.pytorch.org/)\n"]},{"cell_type":"code","metadata":{"id":"u0ukr7quvrMx"},"source":["import torch"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OLdSN9ZXvrM0"},"source":["## **Tensors**\n","\n","Tensors here refer to the math tensor (scalar -> vector -> matrix -> Tensor).A combination of scalar put togethere forms a vector, a combination of vectors put togethere forms a matric and hence a combination of matrix put togethere forms a tensor. Tensors are similar to 3D arrays.\n","\n","![scalar vector matrix tensor and what they look like](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/00-scalar-vector-matrix-tensor.png)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gd-cWhsoipsy","executionInfo":{"status":"ok","timestamp":1625922647708,"user_tz":-330,"elapsed":514,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"8c570647-015d-474e-b128-e63fcc266d55"},"source":["print(3) # a rank 0 tensor -- scalar; this is a scalar with shape []\n","print([1. ,2., 3.]) # a rank 1 tensor -- 1d array; this is a vector with shape [3]\n","print([[1., 2., 3.], [4., 5., 6.]]) # a rank 2 tensor -- 2d array; a matrix with shape [2, 3]\n","print([[[1., 2., 3.]], [[7., 8., 9.]]]) # a rank 3 tensor -- 3d array with shape [2, 1, 3]"],"execution_count":null,"outputs":[{"output_type":"stream","text":["3\n","[1.0, 2.0, 3.0]\n","[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]\n","[[[1.0, 2.0, 3.0]], [[7.0, 8.0, 9.0]]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"c6aWvTEy7lgG"},"source":["### **Tensor Initialization**\n","There are several ways to instantiate tensors in `PyTorch`, which we will go through next."]},{"cell_type":"markdown","metadata":{"id":"c78_3AMEyvJd"},"source":["#### **From a Python List**\n","\n","We can initalize a tensor from a `Python` list, which could include sublists. The dimensions and the data types will be automatically inferred by `PyTorch` when we use [`torch.tensor()`](https://pytorch.org/docs/stable/generated/torch.tensor.html).\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":231},"id":"lsjIW9I_ztiO","executionInfo":{"status":"error","timestamp":1626903035549,"user_tz":-330,"elapsed":387,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"e03a1bdd-e779-4e7b-d179-236f3ea522d4"},"source":["# Initialize a tensor from a Python List\n","data = [\n","        [0, 1],\n","        [2, 3],\n","        [4, 5]\n","       ]\n","x_python = torch.tensor(data)\n","\n","# Print the tensor\n","x_python"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-4f1b31218585>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m        ]\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mx_python\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Print the tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"Iv6ZEoZ0RWb5"},"source":["We can also call `torch.tensor()` with the optional `dtype` parameter, which will set the data type. Some useful datatypes to be familiar with are: `torch.bool`, `torch.float`, and `torch.long`."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":197},"id":"0bQF5IhsD7-n","executionInfo":{"status":"error","timestamp":1626903035970,"user_tz":-330,"elapsed":6,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"a9dbc499-3f33-4251-d22c-145ef0a3b191"},"source":["# We are using the dtype to create a tensor of particular type\n","x_float = torch.tensor(data, dtype=torch.float)\n","x_float"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-cd4dce351816>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# We are using the dtype to create a tensor of particular type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx_float\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mx_float\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"]}]},{"cell_type":"code","metadata":{"id":"16VoILaaE-_j"},"source":["# We are using the dtype to create a tensor of particular type\n","x_bool = torch.tensor(data, dtype=torch.bool)\n","x_bool"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j4HccPWFEQUB"},"source":["We can also get the same tensor in our specified data type using methods such as `float()`, `long()` etc."]},{"cell_type":"code","metadata":{"id":"nh_yq0SuTS_W"},"source":["x_python.float()#astype"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kFiS1OFdTlKE"},"source":["We can also use `tensor.FloatTensor`, `tensor.LongTensor`, `tensor.Tensor` classes to instantiate a tensor of particular type. `LongTensor`s are particularly important in NLP as many methods that deal with indices require the indices to be passed as a `LongTensor`, which is a 64 bit integer."]},{"cell_type":"code","metadata":{"id":"hXXWZ1H2TkNN"},"source":["# `torch.Tensor` defaults to float\n","# Same as torch.FloatTensor(data)\n","x = torch.Tensor(data)\n","x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QuLzDzsoytM2"},"source":["#### **From a NumPy Array**\n","We can also initialize a tensor from a `NumPy` array."]},{"cell_type":"code","metadata":{"id":"1ZvwcEc-qCsV"},"source":["type([[1,2],[2,3]])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YtSNe8X-2Pox"},"source":["import numpy as np\n","\n","# Initialize a tensor from a NumPy array\n","ndarray = np.array(data) # converting list to array\n","x_numpy = torch.from_numpy(ndarray) # converting array to tensor\n","\n","# Print the tensor\n","x_numpy"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dhtcBgum3OZ3"},"source":["#### **From a Tensor**\n","We can also initialize a tensor from another tensor, using the following methods:\n","\n","* `torch.ones_like(old_tensor)`: Initializes a tensor of `1s`.\n","* `torch.zeros_like(old_tensor)`: Initializes a tensor of `0s`.\n","* `torch.rand_like(old_tensor)`: Initializes a tensor where all the elements are sampled from a uniform distribution between `0` and `1`.\n","* `torch.randn_like(old_tensor)`: Initializes a tensor where all the elements are sampled from a normal distribution.\n","\n","All of these methods preserve the tensor properties of the original tensor passed in, such as the `shape` and `device`, which we will cover in a bit."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zoKVhcLh2yqe","executionInfo":{"status":"ok","timestamp":1625845910222,"user_tz":-330,"elapsed":8,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"1c022a5b-adf3-4728-87b9-3ed6c36d0a2e"},"source":["# creating a base tensor using 'python list'\n","x = torch.tensor([[1., 2], [3, 4]])\n","x"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 2.],\n","        [3., 4.]])"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FncfGN6z7ELA","executionInfo":{"status":"ok","timestamp":1625845910223,"user_tz":-330,"elapsed":8,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"d39193a9-998f-4b30-ae06-95745153bcd7"},"source":["# Initialize a tensor of 0s\n","x_zeros = torch.zeros_like(x)\n","x_zeros"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0., 0.],\n","        [0., 0.]])"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D993dpnP6iA8","executionInfo":{"status":"ok","timestamp":1625845910223,"user_tz":-330,"elapsed":7,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"c43576ef-becf-4ce5-d0ee-32e0a1f61326"},"source":["# Initialize a tensor of 1s\n","x_ones = torch.ones_like(x)\n","x_ones"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 1.],\n","        [1., 1.]])"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VBUDeEm97IqW","executionInfo":{"status":"ok","timestamp":1625845910223,"user_tz":-330,"elapsed":7,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"133bc4d6-6743-43d8-ce94-79afd00349a4"},"source":["# Initialize a tensor where each element is sampled from a uniform distribution\n","# between 0 and 1\n","x_rand = torch.rand_like(x)\n","x_rand"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0.4899, 0.1224],\n","        [0.8056, 0.7657]])"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"WYsE3lKt7IEX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1625845910224,"user_tz":-330,"elapsed":7,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"64e10ccb-e8de-497b-a9d7-45cdce0b145a"},"source":["# Initialize a tensor where each element is sampled from a normal distribution\n","x_randn = torch.randn_like(x)\n","x_randn"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 1.2210, -0.2371],\n","        [-2.3569,  0.9598]])"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"t6tqf7v38vbi"},"source":["#### **By Specifying a Shape**\n","We can also instantiate tensors by specifying their shapes (which we will cover in more detail in a bit). The methods we could use follow the ones in the previous section:\n","* `torch.zeros()`\n","* `torch.ones()`\n","* `torch.rand()`\n","* `torch.randn()`"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dh4I4Npz-dZ4","executionInfo":{"status":"ok","timestamp":1625845911628,"user_tz":-330,"elapsed":1410,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"e80e7844-5e42-483a-c988-05ceb0305cac"},"source":["# Initialize a 2x3x2 tensor of 0s -- 2 matries having 3 rows and 2 columns\n","shape = (2, 3, 2)\n","x_zeros = torch.zeros(shape) # x_zeros = torch.zeros(2, 3, 2) is an alternative\n","x_zeros"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[0., 0.],\n","         [0., 0.],\n","         [0., 0.]],\n","\n","        [[0., 0.],\n","         [0., 0.],\n","         [0., 0.]]])"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"7LEjeR24MLkN"},"source":["#### **With `torch.arange()`**\n","We can also create a tensor with `torch.arange(end)`, which returns a `1-D` tensor with elements ranging from `0` to `end-1`. We can use the optional `start` and `step` parameters to create tensors with different ranges.  "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1EjARl2aM7pA","executionInfo":{"status":"ok","timestamp":1625845911629,"user_tz":-330,"elapsed":34,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"ef7cca3a-4997-4535-aeca-5c7cfb57c966"},"source":["# Create a tensor with values 0-9\n","x = torch.arange(10)\n","x"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"VgpkRn527zSr"},"source":["### **Tensor Properties**\n","\n","Tensors have a few properties that are important for us to cover. These are namely `datatype`, `shape` and the `device` properties."]},{"cell_type":"markdown","metadata":{"id":"JBt6e4xZT3zr"},"source":["#### **Data Type**\n","\n","The `dtype` property lets us see the data type of a tensor."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rlF3k3eUT_hQ","executionInfo":{"status":"ok","timestamp":1625845911629,"user_tz":-330,"elapsed":34,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"dcfa97c5-6b76-42d2-fe13-2ee17d27ff12"},"source":["# Initialize a single 3x2 tensor, with 3 rows and 2 columns\n","x = torch.ones(3, 2)\n","x.dtype"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.float32"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"Y1vtN4Dy8FAG"},"source":["#### **Shape**\n","\n","The `shape` property tells us the shape of our tensor. This can help us identify how many dimensional our tensor is as well as how many elements exist in each dimension."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"24gXLJcn7Pxs","executionInfo":{"status":"ok","timestamp":1625845911629,"user_tz":-330,"elapsed":33,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"d7ac7ee1-2967-492e-c0e5-b6caee36637d"},"source":["# Initialize a single tensor with 3 rows and 2 columns\n","x = torch.Tensor([[1, 2], [3, 4], [5, 6]])\n","x"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 2.],\n","        [3., 4.],\n","        [5., 6.]])"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vV0cE1cXAEHP","executionInfo":{"status":"ok","timestamp":1625845911630,"user_tz":-330,"elapsed":33,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"f19ea7c5-364a-416c-9c0b-7b11b0bdffa3"},"source":["# Print out its shape\n","x.shape # Alternative to this is x.size()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([3, 2])"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MA3vnJnaAQlc","executionInfo":{"status":"ok","timestamp":1625845911630,"user_tz":-330,"elapsed":33,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"482aae2b-81f4-403f-c9ea-a909ddffa43d"},"source":["# Print out the number of elements in a particular dimension\n","# 0th dimension corresponds to the rows\n","x.shape[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"uxXCX6y6BvhH"},"source":["We can also get the size of a particular dimension with the `size()` method.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZapQmydxBVuy","executionInfo":{"status":"ok","timestamp":1625845911630,"user_tz":-330,"elapsed":32,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"6256a586-ff20-4f58-e233-777688e68e4f"},"source":["# Get the size of the 0th dimension\n","x.size(0)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["3"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"XCQm7ToPOveH"},"source":["We can change the shape of a tensor with the `view()` method."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J1JH3fiNO5Gu","executionInfo":{"status":"ok","timestamp":1625845911630,"user_tz":-330,"elapsed":31,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"1e83c83b-f7b8-4a29-ed52-38ef111413a9"},"source":["# Example use of view()\n","# x_view shares the same memory as x, so changing one changes the other\n","x_view = x.view(2,3)\n","x_view"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 2., 3.],\n","        [4., 5., 6.]])"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3C3x4seqPGEI","executionInfo":{"status":"ok","timestamp":1625845911631,"user_tz":-330,"elapsed":31,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"a5415952-d303-422b-b7b0-214a91823aec"},"source":["# We can ask PyTorch to automatically infer the size of a dimension with -1\n","x_view = x.view(3, -1)\n","x_view"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 2.],\n","        [3., 4.],\n","        [5., 6.]])"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"TYSCEesPITpf"},"source":["We can also use `torch.reshape()` method for a similar purpose. There is a subtle difference between `reshape()` and `view()`: `view()` requires the data to be stored contiguously in the memory. You can refer to [this](https://stackoverflow.com/questions/49643225/whats-the-difference-between-reshape-and-view-in-pytorch) StackOverflow answer for more information. In simple terms, contiguous means that the way our data is laid out in the memory is the same as the way we would read elements from it. This happens because some methods, such as `transpose()` and `view()`, do not actually change how our data is stored in the memory. They just change the meta information about out tensor, so that when we use it we will see the elements in the order we expect.\n","\n","`reshape()` calls `view()` internally if the data is stored contiguously, if not, it returns a copy. The difference here isn't too important for basic tensors, but if you perform operations that make the underlying storage of the data non-contiguous (such as taking a transpose), you will have issues using `view()`. If you would like to match the way your tensor is stored in the memory to how it is used, you can use the `contiguous()` method.  "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gLGcGYE4Llom","executionInfo":{"status":"ok","timestamp":1625845911631,"user_tz":-330,"elapsed":31,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"eec6f7d6-1a7f-4b94-a96f-991035068f94"},"source":["# Change the shape of x to be 3x2\n","# x_reshaped could be a reference to or copy of x\n","x_reshaped = torch.reshape(x, (2, 3))\n","x_reshaped"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 2., 3.],\n","        [4., 5., 6.]])"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"markdown","metadata":{"id":"XWNTZKZZQ9i6"},"source":["We can use `torch.unsqueeze(x, dim)` function to add a dimension of size `1` to the provided `dim`, where `x` is the tensor. We can also use the corresponding use `torch.squeeze(x)`, which removes the dimensions of size `1`.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3_IYojrJRh-m","executionInfo":{"status":"ok","timestamp":1625845911631,"user_tz":-330,"elapsed":30,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"1fb9e0ee-4e41-4208-afe3-d492ccb0559f"},"source":["# Initialize a 1x5x2 tensor, with 5 rows and 2 columns\n","x = torch.arange(10).reshape(5, 2)\n","x"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0, 1],\n","        [2, 3],\n","        [4, 5],\n","        [6, 7],\n","        [8, 9]])"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pLhg_oZ4SHh-","executionInfo":{"status":"ok","timestamp":1625845911631,"user_tz":-330,"elapsed":30,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"274d9ee8-9134-4121-b9a2-c0e0bf3e84b4"},"source":["# Add a new dimension of size 1 at the 1st dimension\n","x = x.unsqueeze(1)\n","x\n","x.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([5, 1, 2])"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YoGYGbMRSo-J","executionInfo":{"status":"ok","timestamp":1625845911632,"user_tz":-330,"elapsed":30,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"cf759a31-fa46-4cde-af9b-c47e1ae85b5c"},"source":["# Squeeze the dimensions of x by getting rid of all the dimensions with 1 element\n","x = x.squeeze()\n","x.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([5, 2])"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"markdown","metadata":{"id":"EQpZ4556B3lb"},"source":["If we want to get the total number of elements in a tensor, we can use the `numel()` method."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P-irUWlxTB6a","executionInfo":{"status":"ok","timestamp":1625845911632,"user_tz":-330,"elapsed":30,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"77f24895-33d7-438d-9c4d-c71d85cb2a38"},"source":["x"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[0, 1],\n","        [2, 3],\n","        [4, 5],\n","        [6, 7],\n","        [8, 9]])"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"76yVMMg_CA0Q","executionInfo":{"status":"ok","timestamp":1625845911632,"user_tz":-330,"elapsed":29,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"02bc6e6b-ae1a-4092-e5ec-19acd4e0957f"},"source":["# Get the number of elements in tensor.\n","x.numel()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["10"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"markdown","metadata":{"id":"_M1U_RTpBhl2"},"source":["#### **Device**\n","Device property tells `PyTorch` where to store our tensor. Where a tensor is stored determines which device, `GPU` (also called CUDA) or `CPU`, would be handling the computations involving it. We can find the device of a tensor with the `device` property."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XYRGhIbnCl3b","executionInfo":{"status":"ok","timestamp":1625845911632,"user_tz":-330,"elapsed":29,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"acac4a49-7c2b-462f-9510-8c3fdb628fe5"},"source":["# Initialize an example tensor\n","x = torch.Tensor([[1, 2], [3, 4]])\n","x"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 2.],\n","        [3., 4.]])"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"byEnJyKdBgjl","executionInfo":{"status":"ok","timestamp":1625845911633,"user_tz":-330,"elapsed":29,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"b8c5c2ae-b7b9-41f5-dda5-854b939ed4c8"},"source":["# Get the device of the tensor\n","x.device"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cpu')"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"markdown","metadata":{"id":"-vqf_-NADFX8"},"source":["We can move a tensor from one device to another with the method `to(device)`."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mIdQLW68w_Gy","executionInfo":{"status":"ok","timestamp":1625845911633,"user_tz":-330,"elapsed":29,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"5bf35c92-08b3-42f1-fd2b-443e9d4ee0ff"},"source":["torch.cuda.is_available() #Bool"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"id":"GzA6zqkXDEt1"},"source":["# Check if a GPU is available, if so, move the tensor to the GPU\n","if torch.cuda.is_available():\n","  x.to('cuda')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V7BMktFFAkRA"},"source":["### **Tensor Indexing**\n","In `PyTorch` we can index tensors, similar to `NumPy`."]},{"cell_type":"code","metadata":{"id":"HQK-l-ILwfsW"},"source":["x = [1,2,3,4,5] #x[-1] #x[1:] #x[1:3] - indexing in numpy example"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fRJN7ovWDsKV","executionInfo":{"status":"ok","timestamp":1625845911634,"user_tz":-330,"elapsed":28,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"0c5eec27-a5c0-4355-c4c2-e55ac50629c8"},"source":["# Initialize an example tensor\n","x = torch.Tensor([\n","                  [[1, 2], [3, 4]],\n","                  [[5, 6], [7, 8]],\n","                  [[9, 10], [11, 12]]\n","                 ])\n","x"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[ 1.,  2.],\n","         [ 3.,  4.]],\n","\n","        [[ 5.,  6.],\n","         [ 7.,  8.]],\n","\n","        [[ 9., 10.],\n","         [11., 12.]]])"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M67ZiOF1Heyc","executionInfo":{"status":"ok","timestamp":1625845911634,"user_tz":-330,"elapsed":28,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"61be1fce-b3e7-4e81-b439-055f73e6dcf0"},"source":["x.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([3, 2, 2])"]},"metadata":{"tags":[]},"execution_count":35}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"guXKE7m8AX1K","executionInfo":{"status":"ok","timestamp":1625845911634,"user_tz":-330,"elapsed":27,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"d3c125a4-8b7e-4934-8750-603a582745a7"},"source":["# Access the 0th element, which is the first row\n","x[0] # Equivalent to x[0, :]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[1., 2.],\n","        [3., 4.]])"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"markdown","metadata":{"id":"g8m8EyVvES4-"},"source":["We can also index into multiple dimensions with `:`."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2Z6GFUcuEL85","executionInfo":{"status":"ok","timestamp":1625845911635,"user_tz":-330,"elapsed":28,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"33159548-84ad-4d30-9ae0-08c3497a253e"},"source":["# Get the top left element of each element in our tensor\n","x[:, 0, 0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([1., 5., 9.])"]},"metadata":{"tags":[]},"execution_count":37}]},{"cell_type":"markdown","metadata":{"id":"Rm8vc3nuXaEw"},"source":["We can also access arbitrary elements in each dimension."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eYhcH9gaWHyW","executionInfo":{"status":"ok","timestamp":1625845911635,"user_tz":-330,"elapsed":27,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"119520d5-e389-4999-f385-839729ccf2a7"},"source":["# Print x again to see our tensor\n","x"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[ 1.,  2.],\n","         [ 3.,  4.]],\n","\n","        [[ 5.,  6.],\n","         [ 7.,  8.]],\n","\n","        [[ 9., 10.],\n","         [11., 12.]]])"]},"metadata":{"tags":[]},"execution_count":38}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R4xl6CW3RrEw","executionInfo":{"status":"ok","timestamp":1625845911635,"user_tz":-330,"elapsed":26,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"bd29e100-900b-41cc-b16f-27b332ea356d"},"source":["# Let's access the 0th and 1st elements, each twice\n","i = torch.tensor([0, 0, 1, 1])\n","x[i]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[1., 2.],\n","         [3., 4.]],\n","\n","        [[1., 2.],\n","         [3., 4.]],\n","\n","        [[5., 6.],\n","         [7., 8.]],\n","\n","        [[5., 6.],\n","         [7., 8.]]])"]},"metadata":{"tags":[]},"execution_count":39}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A3QYZ8k7Wvqp","executionInfo":{"status":"ok","timestamp":1625845911635,"user_tz":-330,"elapsed":26,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"faf46614-3bce-47dc-be89-e9babcb1a012"},"source":["# Let's access the 0th elements of the 1st and 2nd elements\n","i = torch.tensor([1, 2])\n","j = torch.tensor([0])\n","x[i, j]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[ 5.,  6.],\n","        [ 9., 10.]])"]},"metadata":{"tags":[]},"execution_count":40}]},{"cell_type":"markdown","metadata":{"id":"WAELXC--IHS7"},"source":["We can get a `Python` scalar value from a tensor with `item()`."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BM-ZujN2IGaQ","executionInfo":{"status":"ok","timestamp":1625845911636,"user_tz":-330,"elapsed":26,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"c789e426-0ab6-4274-a1bc-3b59009c4fda"},"source":["x[0, 0, 0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(1.)"]},"metadata":{"tags":[]},"execution_count":41}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6NwxK7d_Ycgs","executionInfo":{"status":"ok","timestamp":1625845911636,"user_tz":-330,"elapsed":26,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"06db996a-b77c-4d48-8000-978f2edb0909"},"source":["x[0, 0, 0].item()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1.0"]},"metadata":{"tags":[]},"execution_count":42}]},{"cell_type":"markdown","metadata":{"id":"GltnmDzeIXJM"},"source":["### **Tensor Operations**\n","PyTorch operations are very similar to those of `NumPy`. We can work with both scalars and other tensors.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"W9KBzcA0G6v9","executionInfo":{"status":"ok","timestamp":1625845911636,"user_tz":-330,"elapsed":22,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"c1531564-b8da-4102-bf0c-2fc154eafb4a"},"source":["# Create an example tensor\n","x = torch.ones((3,2,2))\n","x"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[1., 1.],\n","         [1., 1.]],\n","\n","        [[1., 1.],\n","         [1., 1.]],\n","\n","        [[1., 1.],\n","         [1., 1.]]])"]},"metadata":{"tags":[]},"execution_count":43}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AUw8MAHqKuzs","executionInfo":{"status":"ok","timestamp":1625845911636,"user_tz":-330,"elapsed":21,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"3855e5bb-b06a-4359-8bfd-fc1d6f9d421d"},"source":["# Perform elementwise addition\n","# Use - for subtraction\n","x + 2"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[3., 3.],\n","         [3., 3.]],\n","\n","        [[3., 3.],\n","         [3., 3.]],\n","\n","        [[3., 3.],\n","         [3., 3.]]])"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QAfMsaz1Gw5v","executionInfo":{"status":"ok","timestamp":1625845911637,"user_tz":-330,"elapsed":22,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"c8c3a234-4e84-4eb4-a312-1bc16a5587ed"},"source":["# Perform elementwise multiplication\n","# Use / for division\n","x * 2"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[2., 2.],\n","         [2., 2.]],\n","\n","        [[2., 2.],\n","         [2., 2.]],\n","\n","        [[2., 2.],\n","         [2., 2.]]])"]},"metadata":{"tags":[]},"execution_count":45}]},{"cell_type":"markdown","metadata":{"id":"0aq89FU7OOe7"},"source":["We can apply the same operations between different tensors of compatible sizes.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BGhz62wILIfN","executionInfo":{"status":"ok","timestamp":1625845911637,"user_tz":-330,"elapsed":21,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"33a80839-9249-477e-f348-fc24d89fe4a8"},"source":["# Create a 4x3 tensor of 6s\n","a = torch.ones((4,3)) * 6\n","a"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[6., 6., 6.],\n","        [6., 6., 6.],\n","        [6., 6., 6.],\n","        [6., 6., 6.]])"]},"metadata":{"tags":[]},"execution_count":46}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tvDC1OzzPyLV","executionInfo":{"status":"ok","timestamp":1625845911637,"user_tz":-330,"elapsed":21,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"6421ed14-b916-4405-cbbd-a5f5e85a04fd"},"source":["# Create a 1D tensor of 2s\n","b = torch.ones(3) * 2\n","b"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([2., 2., 2.])"]},"metadata":{"tags":[]},"execution_count":47}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HUF9noMTP5NI","executionInfo":{"status":"ok","timestamp":1625845911637,"user_tz":-330,"elapsed":20,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"684afbdc-0a48-4c9c-ef16-894addc49958"},"source":["# Divide a by b\n","a / b"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[3., 3., 3.],\n","        [3., 3., 3.],\n","        [3., 3., 3.],\n","        [3., 3., 3.]])"]},"metadata":{"tags":[]},"execution_count":48}]},{"cell_type":"markdown","metadata":{"id":"YTiVVbukXRct"},"source":["We can use `tensor.matmul(other_tensor)` for matrix multiplication and `tensor.T` for transpose. Matrix multiplication can also be performed with `@`."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bPoC_WcbXCw5","executionInfo":{"status":"ok","timestamp":1625845911637,"user_tz":-330,"elapsed":20,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"06547fca-6c05-4bcc-a723-2854e42a5524"},"source":["# Alternative to a.matmul(b)\n","# a @ b.T returns the same result since b is 1D tensor and the 2nd dimension\n","# is inferred\n","a @ b #a.b.T"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([36., 36., 36., 36.])"]},"metadata":{"tags":[]},"execution_count":49}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e2WEoQjVYBQ8","executionInfo":{"status":"ok","timestamp":1625845911638,"user_tz":-330,"elapsed":20,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"bb5608d5-9ed2-4d38-90df-343345b0cb69"},"source":["# Import pprint, module we use for making our print statements prettier\n","import pprint\n","pp = pprint.PrettyPrinter()\n","\n","pp.pprint(a.shape)\n","pp.pprint(a.T.shape)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["torch.Size([4, 3])\n","torch.Size([3, 4])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0PibNpxbYYf2"},"source":["We can take the mean and standard deviation along a certain dimension with the methods `mean(dim)` and `std(dim)`. That is, if we want to get the mean `3x2` matrix in a `4x3x2` matrix, we would set the `dim` to be 0. We can call these methods with no parameter to get the mean and standard deviation for the whole tensor. To use `mean` and `std` our tensor should be a floating point type."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a987teCtYg7R","executionInfo":{"status":"ok","timestamp":1625845911638,"user_tz":-330,"elapsed":20,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"baa63055-4857-4b87-8b81-12b1ec44eaea"},"source":["# Create an example tensor\n","m = torch.tensor(\n","    [\n","     [1., 1.],\n","     [2., 2.],\n","     [3., 3.],\n","     [4., 4.]\n","    ]\n",")\n","\n","pp.pprint(\"Mean: {}\".format(m.mean()))\n","pp.pprint(\"Mean in the 0th dimension: {}\".format(m.mean(0)))\n","pp.pprint(\"Mean in the 1st dimension: {}\".format(m.mean(1)))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["'Mean: 2.5'\n","'Mean in the 0th dimension: tensor([2.5000, 2.5000])'\n","'Mean in the 1st dimension: tensor([1., 2., 3., 4.])'\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Rd77stQ5VQVT"},"source":["We can concatenate tensors using `torch.cat`.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"advfDCOPK9Gw","executionInfo":{"status":"ok","timestamp":1625845911638,"user_tz":-330,"elapsed":19,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"2ad4f9c7-21a1-45ea-cf72-d6396c3f725c"},"source":["# Concatenate in dimension 0 and 1\n","a_cat0 = torch.cat([a, a, a], dim=0)\n","a_cat1 = torch.cat([a, a, a], dim=1)\n","\n","print(\"Initial shape: {}\".format(a.shape))\n","print(\"Shape after concatenation in dimension 0: {}\".format(a_cat0.shape))\n","print(\"Shape after concatenation in dimension 1: {}\".format(a_cat1.shape))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Initial shape: torch.Size([4, 3])\n","Shape after concatenation in dimension 0: torch.Size([12, 3])\n","Shape after concatenation in dimension 1: torch.Size([4, 9])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BveswZMOjtff"},"source":["Most of the operations in `PyTorch` are not in place. However, `PyTorch` offers the in place versions of operations available by adding an underscore (`_`) at the end of the method name."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ebr7nn-DaU3B","executionInfo":{"status":"ok","timestamp":1625845911638,"user_tz":-330,"elapsed":19,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"209da6e7-bfdf-475f-9f86-588e8c896894"},"source":["# Print our tensor\n","a"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[6., 6., 6.],\n","        [6., 6., 6.],\n","        [6., 6., 6.],\n","        [6., 6., 6.]])"]},"metadata":{"tags":[]},"execution_count":53}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mP8-VtoHaKAc","executionInfo":{"status":"ok","timestamp":1625845911639,"user_tz":-330,"elapsed":19,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"44a3a7a7-5ba2-46f2-d57a-26198c7e2483"},"source":["# add() is not in place\n","a.add(a)\n","a"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[6., 6., 6.],\n","        [6., 6., 6.],\n","        [6., 6., 6.],\n","        [6., 6., 6.]])"]},"metadata":{"tags":[]},"execution_count":54}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mY0ojINbaayp","executionInfo":{"status":"ok","timestamp":1625845911639,"user_tz":-330,"elapsed":18,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"21b41da5-787b-4a34-dfdc-c0ee257513ed"},"source":["# add_() is in place\n","a.add_(a)\n","a"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[12., 12., 12.],\n","        [12., 12., 12.],\n","        [12., 12., 12.],\n","        [12., 12., 12.]])"]},"metadata":{"tags":[]},"execution_count":55}]},{"cell_type":"markdown","metadata":{"id":"Re8xiL37eAja"},"source":["## **Autograd** - Automatic Differentiation(Gradient) Library\n","\n","Given that we have defined the set of operations that need to be performed, the framework itself can figure out how to compute the derivatives(gradients). We can call the `backward()` method to ask `PyTorch` to calculate the gradiends, which are then stored in the `grad` attribute."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aca7GJJ9-YYx","executionInfo":{"status":"ok","timestamp":1625846344728,"user_tz":-330,"elapsed":344,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"cdce41db-d3d5-443e-805d-605ebb199ee6"},"source":["# Create an example tensor requires_grad parameter tells PyTorch to store gradients\n","x = torch.tensor([2.], requires_grad=True)\n","# Print the gradient if it is calculated, Currently None is printed because we have not calculated gradient yet so there is nothing stored\n","pp.pprint(x.grad)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["None\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DTJazZXkgthP","executionInfo":{"status":"ok","timestamp":1625846346061,"user_tz":-330,"elapsed":2,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"9b63cfc1-6da1-4548-d07a-1990af14e17c"},"source":["# Calculating the gradient of y with respect to x\n","y = x * x * 3 # 3x^2\n","y.backward() # d(y)/d(x) = d(3x^2)/d(x) = 6x = 12\n","pp.pprint(x.grad) #now since we have calcuated the gradient it has stored it and hence none will not be printed"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([12.])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3Hqc2oM3iV6a"},"source":["Let's run backprop from a different tensor again to see what happens."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K--Az0Xiic_z","executionInfo":{"status":"ok","timestamp":1625846350569,"user_tz":-330,"elapsed":347,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"0c64025a-8790-4b38-df23-3edeba0fdfed"},"source":["z = x * x * 3 # 3x^2\n","z.backward()\n","pp.pprint(x.grad) # (12+12 = 24)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([24.])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HhjPkiE6i7ja"},"source":["We can see that the `x.grad` is updated to be the sum of the gradients calculated so far. When we run backprop in a neural network, we sum up all the gradients for a particular neuron before making an update. This is exactly what is happening here! This is also the reason why we need to run `zero_grad()` in every training iteration (more on this later). Otherwise our gradients would keep building up from one training iteration to the other, which would cause our updates to be wrong."]},{"cell_type":"markdown","metadata":{"id":"pYLWqKIoaOyd"},"source":["## **Linear Perceptron**"]},{"cell_type":"code","metadata":{"id":"qUmrDpbhV4Tn"},"source":["# importing required libraries\n","import torch.nn as nn\n","import pandas as pd\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kwuPCkOx0mBe"},"source":["#reading the dataset\n","dataset = pd.read_csv('/content/drive/MyDrive/001 My Skills/002 CS Engineering   Automated Math (BPHC)/004 Data Science (DS)   Artificial Intelligence (AI)/002 Tabular Cross Sectional Data (Structured Data) (Non Sequential Data)/002 Machine Learning (ML)/001 Supervised Learning/001 Regression Problem/001 Single Target Variable (Univariate)/002 Deep Learning/001 Linear Perceptron/insurance.txt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0by_AJ3Z038O"},"source":["# before this step do all the data preprocessing you want to do, i am not doing here just cause i am lazy\n","# spilling the data into x and y, ideally you should do train test split also but i am not doing the same cause i am lazy\n","x_train = torch.FloatTensor(dataset.iloc[:,:-1].values)\n","y_train = torch.FloatTensor(dataset.iloc[:,-1].values)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GNcGUXBc1XMc","executionInfo":{"status":"ok","timestamp":1625893995867,"user_tz":-330,"elapsed":550,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"54624216-87e2-4522-8ab2-fb6630f093ff"},"source":["x_train"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[19.0000, 27.9000,  0.0000],\n","        [18.0000, 33.7700,  1.0000],\n","        [28.0000, 33.0000,  3.0000],\n","        ...,\n","        [18.0000, 36.8500,  0.0000],\n","        [21.0000, 25.8000,  0.0000],\n","        [61.0000, 29.0700,  0.0000]])"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DNjURWLX1ZBx","executionInfo":{"status":"ok","timestamp":1625894003684,"user_tz":-330,"elapsed":382,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"ba3e07f8-220e-4097-f098-67428e0b5c2f"},"source":["y_train"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([16884.9238,  1725.5522,  4449.4619,  ...,  1629.8335,  2007.9449,\n","        29141.3594])"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"_lC-7gU11viG"},"source":["# creating a no hidden layer FFNN for regression\n","class SarveshANN(nn.Module):\n","\n","  def __init__(self, input_size, output_size):\n","\n","    #Call to the __init__ function of the super class\n","    super(SarveshANN, self).__init__()\n","\n","    #Bookkeeping: Saving the initialization parameters\n","    self.input_size = input_size\n","    self.output_size = output_size\n","\n","    #Defining the model\n","    self.model = nn.Sequential(\n","        nn.Linear(input_size, output_size), # output layer - If we don't want the linear layer to learn the bias parameters, we can add a parameter here bias=False.\n","        nn.Identity() #output activation function\n","    )\n","\n","  def forward(self, x):\n","    output = self.model(x)\n","    return output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-OttZ8TZ2iEa","executionInfo":{"status":"ok","timestamp":1625894751191,"user_tz":-330,"elapsed":594,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"7d69ffae-1ceb-4ddc-d7ee-5c452d10694f"},"source":["#instantiating our model\n","model = SarveshANN(3,1)\n","print(model)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["SarveshANN(\n","  (model): Sequential(\n","    (0): Linear(in_features=3, out_features=1, bias=True)\n","    (1): Identity()\n","  )\n",")\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"659rgjtV4jO3","executionInfo":{"status":"ok","timestamp":1625894830875,"user_tz":-330,"elapsed":825,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"8a232261-ce8e-43b0-d7bd-67106d4535a7"},"source":["list(model.named_parameters()) #seeing what initial random weights our model will take"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('model.0.weight', Parameter containing:\n","  tensor([[ 0.4055,  0.1502, -0.4115]], requires_grad=True)),\n"," ('model.0.bias', Parameter containing:\n","  tensor([0.0973], requires_grad=True))]"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"SDdJxQxm5_1Z"},"source":["# Define loss function - we can either define our own loss function or use one of the predefined loss function in pytorch\n","loss_function = nn.MSELoss() #MSE = mean squared error loss function\n","# or use nn.BCELoss() which denotes binary cross entropy loss function"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PvaoYzP04YP9"},"source":["# Define the optimizer\n","import torch.optim as optim\n","adam = optim.Adam(model.parameters(), lr=1e-1) # or you could use optim.SGD and many others\n","# model.parameter() tells the optimizers which values it has to optimize and lr denotes learning rate"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JRjoYXO76IUN","executionInfo":{"status":"ok","timestamp":1625896685306,"user_tz":-330,"elapsed":1339,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"b58bda82-6e6f-4838-a10e-2d856b892062"},"source":["for epoch in range(200):\n","\n","  #forward pass\n","  y_pred = model(x_train)\n","  loss = loss_function(y_pred,y_train)\n","  print(f\"Epoch {epoch}: traing loss: {loss}\")\n","\n","  #backward pass\n","  loss.backward()\n","\n","  #updating weights\n","  adam.step()\n","\n","  #to get rid of summing of gradients we need to do this after each pass\n","  adam.zero_grad()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([1338])) that is different to the input size (torch.Size([1338, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 0: traing loss: 233440768.0\n","Epoch 1: traing loss: 233321232.0\n","Epoch 2: traing loss: 233202896.0\n","Epoch 3: traing loss: 233085664.0\n","Epoch 4: traing loss: 232969408.0\n","Epoch 5: traing loss: 232854080.0\n","Epoch 6: traing loss: 232739568.0\n","Epoch 7: traing loss: 232625840.0\n","Epoch 8: traing loss: 232512768.0\n","Epoch 9: traing loss: 232400352.0\n","Epoch 10: traing loss: 232288544.0\n","Epoch 11: traing loss: 232177248.0\n","Epoch 12: traing loss: 232066448.0\n","Epoch 13: traing loss: 231956080.0\n","Epoch 14: traing loss: 231846128.0\n","Epoch 15: traing loss: 231736592.0\n","Epoch 16: traing loss: 231627424.0\n","Epoch 17: traing loss: 231518576.0\n","Epoch 18: traing loss: 231410064.0\n","Epoch 19: traing loss: 231301824.0\n","Epoch 20: traing loss: 231193872.0\n","Epoch 21: traing loss: 231086160.0\n","Epoch 22: traing loss: 230978704.0\n","Epoch 23: traing loss: 230871472.0\n","Epoch 24: traing loss: 230764480.0\n","Epoch 25: traing loss: 230657696.0\n","Epoch 26: traing loss: 230551072.0\n","Epoch 27: traing loss: 230444688.0\n","Epoch 28: traing loss: 230338448.0\n","Epoch 29: traing loss: 230232400.0\n","Epoch 30: traing loss: 230126496.0\n","Epoch 31: traing loss: 230020800.0\n","Epoch 32: traing loss: 229915216.0\n","Epoch 33: traing loss: 229809808.0\n","Epoch 34: traing loss: 229704528.0\n","Epoch 35: traing loss: 229599376.0\n","Epoch 36: traing loss: 229494400.0\n","Epoch 37: traing loss: 229389568.0\n","Epoch 38: traing loss: 229284864.0\n","Epoch 39: traing loss: 229180272.0\n","Epoch 40: traing loss: 229075808.0\n","Epoch 41: traing loss: 228971504.0\n","Epoch 42: traing loss: 228867296.0\n","Epoch 43: traing loss: 228763200.0\n","Epoch 44: traing loss: 228659264.0\n","Epoch 45: traing loss: 228555392.0\n","Epoch 46: traing loss: 228451712.0\n","Epoch 47: traing loss: 228348096.0\n","Epoch 48: traing loss: 228244640.0\n","Epoch 49: traing loss: 228141264.0\n","Epoch 50: traing loss: 228038016.0\n","Epoch 51: traing loss: 227934896.0\n","Epoch 52: traing loss: 227831840.0\n","Epoch 53: traing loss: 227728944.0\n","Epoch 54: traing loss: 227626160.0\n","Epoch 55: traing loss: 227523488.0\n","Epoch 56: traing loss: 227420896.0\n","Epoch 57: traing loss: 227318432.0\n","Epoch 58: traing loss: 227216080.0\n","Epoch 59: traing loss: 227113856.0\n","Epoch 60: traing loss: 227011728.0\n","Epoch 61: traing loss: 226909712.0\n","Epoch 62: traing loss: 226807808.0\n","Epoch 63: traing loss: 226706016.0\n","Epoch 64: traing loss: 226604304.0\n","Epoch 65: traing loss: 226502720.0\n","Epoch 66: traing loss: 226401264.0\n","Epoch 67: traing loss: 226299904.0\n","Epoch 68: traing loss: 226198656.0\n","Epoch 69: traing loss: 226097488.0\n","Epoch 70: traing loss: 225996448.0\n","Epoch 71: traing loss: 225895504.0\n","Epoch 72: traing loss: 225794688.0\n","Epoch 73: traing loss: 225693968.0\n","Epoch 74: traing loss: 225593344.0\n","Epoch 75: traing loss: 225492832.0\n","Epoch 76: traing loss: 225392464.0\n","Epoch 77: traing loss: 225292176.0\n","Epoch 78: traing loss: 225191968.0\n","Epoch 79: traing loss: 225091888.0\n","Epoch 80: traing loss: 224991936.0\n","Epoch 81: traing loss: 224892080.0\n","Epoch 82: traing loss: 224792304.0\n","Epoch 83: traing loss: 224692640.0\n","Epoch 84: traing loss: 224593104.0\n","Epoch 85: traing loss: 224493680.0\n","Epoch 86: traing loss: 224394336.0\n","Epoch 87: traing loss: 224295104.0\n","Epoch 88: traing loss: 224196000.0\n","Epoch 89: traing loss: 224096976.0\n","Epoch 90: traing loss: 223998064.0\n","Epoch 91: traing loss: 223899280.0\n","Epoch 92: traing loss: 223800576.0\n","Epoch 93: traing loss: 223701984.0\n","Epoch 94: traing loss: 223603472.0\n","Epoch 95: traing loss: 223505088.0\n","Epoch 96: traing loss: 223406848.0\n","Epoch 97: traing loss: 223308624.0\n","Epoch 98: traing loss: 223210576.0\n","Epoch 99: traing loss: 223112592.0\n","Epoch 100: traing loss: 223014752.0\n","Epoch 101: traing loss: 222916992.0\n","Epoch 102: traing loss: 222819344.0\n","Epoch 103: traing loss: 222721792.0\n","Epoch 104: traing loss: 222624384.0\n","Epoch 105: traing loss: 222527024.0\n","Epoch 106: traing loss: 222429808.0\n","Epoch 107: traing loss: 222332672.0\n","Epoch 108: traing loss: 222235632.0\n","Epoch 109: traing loss: 222138736.0\n","Epoch 110: traing loss: 222041904.0\n","Epoch 111: traing loss: 221945232.0\n","Epoch 112: traing loss: 221848608.0\n","Epoch 113: traing loss: 221752096.0\n","Epoch 114: traing loss: 221655696.0\n","Epoch 115: traing loss: 221559392.0\n","Epoch 116: traing loss: 221463200.0\n","Epoch 117: traing loss: 221367104.0\n","Epoch 118: traing loss: 221271120.0\n","Epoch 119: traing loss: 221175248.0\n","Epoch 120: traing loss: 221079472.0\n","Epoch 121: traing loss: 220983792.0\n","Epoch 122: traing loss: 220888224.0\n","Epoch 123: traing loss: 220792752.0\n","Epoch 124: traing loss: 220697392.0\n","Epoch 125: traing loss: 220602128.0\n","Epoch 126: traing loss: 220506992.0\n","Epoch 127: traing loss: 220411920.0\n","Epoch 128: traing loss: 220316960.0\n","Epoch 129: traing loss: 220222112.0\n","Epoch 130: traing loss: 220127392.0\n","Epoch 131: traing loss: 220032736.0\n","Epoch 132: traing loss: 219938192.0\n","Epoch 133: traing loss: 219843728.0\n","Epoch 134: traing loss: 219749408.0\n","Epoch 135: traing loss: 219655184.0\n","Epoch 136: traing loss: 219561056.0\n","Epoch 137: traing loss: 219467008.0\n","Epoch 138: traing loss: 219373104.0\n","Epoch 139: traing loss: 219279232.0\n","Epoch 140: traing loss: 219185536.0\n","Epoch 141: traing loss: 219091904.0\n","Epoch 142: traing loss: 218998384.0\n","Epoch 143: traing loss: 218904960.0\n","Epoch 144: traing loss: 218811632.0\n","Epoch 145: traing loss: 218718432.0\n","Epoch 146: traing loss: 218625344.0\n","Epoch 147: traing loss: 218532304.0\n","Epoch 148: traing loss: 218439408.0\n","Epoch 149: traing loss: 218346608.0\n","Epoch 150: traing loss: 218253904.0\n","Epoch 151: traing loss: 218161248.0\n","Epoch 152: traing loss: 218068752.0\n","Epoch 153: traing loss: 217976352.0\n","Epoch 154: traing loss: 217884032.0\n","Epoch 155: traing loss: 217791872.0\n","Epoch 156: traing loss: 217699744.0\n","Epoch 157: traing loss: 217607760.0\n","Epoch 158: traing loss: 217515856.0\n","Epoch 159: traing loss: 217424064.0\n","Epoch 160: traing loss: 217332336.0\n","Epoch 161: traing loss: 217240736.0\n","Epoch 162: traing loss: 217149232.0\n","Epoch 163: traing loss: 217057856.0\n","Epoch 164: traing loss: 216966544.0\n","Epoch 165: traing loss: 216875344.0\n","Epoch 166: traing loss: 216784224.0\n","Epoch 167: traing loss: 216693248.0\n","Epoch 168: traing loss: 216602368.0\n","Epoch 169: traing loss: 216511552.0\n","Epoch 170: traing loss: 216420848.0\n","Epoch 171: traing loss: 216330240.0\n","Epoch 172: traing loss: 216239760.0\n","Epoch 173: traing loss: 216149376.0\n","Epoch 174: traing loss: 216059072.0\n","Epoch 175: traing loss: 215968880.0\n","Epoch 176: traing loss: 215878752.0\n","Epoch 177: traing loss: 215788768.0\n","Epoch 178: traing loss: 215698848.0\n","Epoch 179: traing loss: 215609056.0\n","Epoch 180: traing loss: 215519344.0\n","Epoch 181: traing loss: 215429760.0\n","Epoch 182: traing loss: 215340240.0\n","Epoch 183: traing loss: 215250832.0\n","Epoch 184: traing loss: 215161504.0\n","Epoch 185: traing loss: 215072320.0\n","Epoch 186: traing loss: 214983200.0\n","Epoch 187: traing loss: 214894192.0\n","Epoch 188: traing loss: 214805280.0\n","Epoch 189: traing loss: 214716464.0\n","Epoch 190: traing loss: 214627728.0\n","Epoch 191: traing loss: 214539168.0\n","Epoch 192: traing loss: 214450624.0\n","Epoch 193: traing loss: 214362208.0\n","Epoch 194: traing loss: 214273888.0\n","Epoch 195: traing loss: 214185632.0\n","Epoch 196: traing loss: 214097520.0\n","Epoch 197: traing loss: 214009488.0\n","Epoch 198: traing loss: 213921568.0\n","Epoch 199: traing loss: 213833760.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6HYz7xNz-YC0","executionInfo":{"status":"ok","timestamp":1625896375780,"user_tz":-330,"elapsed":532,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"bdbd54af-20f0-4fa7-cc89-2c5585103791"},"source":["list(model.parameters()) #showing parameters learnt after training"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Parameter containing:\n"," tensor([[55.4532, 55.4763, 54.8747]], requires_grad=True),\n"," Parameter containing:\n"," tensor([55.5348], requires_grad=True)]"]},"metadata":{"tags":[]},"execution_count":25}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JVVyw4aW-k0G","executionInfo":{"status":"ok","timestamp":1625896420054,"user_tz":-330,"elapsed":360,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"922f81fe-3da5-4fc8-f0c2-fb52895e77f9"},"source":["# See how our model performs on the training data\n","y_pred_train = model(x_train)\n","y_pred_train"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[2656.9346],\n","        [2982.0024],\n","        [3603.5664],\n","        ...,\n","        [3097.9946],\n","        [2651.3406],\n","        [5050.8740]], grad_fn=<AddmmBackward>)"]},"metadata":{"tags":[]},"execution_count":26}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tKDEhDwE-qFt","executionInfo":{"status":"ok","timestamp":1625896435299,"user_tz":-330,"elapsed":361,"user":{"displayName":"Sarvesh Khetan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiYvgp8tsqLQjHL82OfSEL-YYRULs9eO8SLPw2rDg=s64","userId":"17514303311129929608"}},"outputId":"cc2b3462-e2b1-4738-f2b2-f9a3488ecad6"},"source":["y_train"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([16884.9238,  1725.5522,  4449.4619,  ...,  1629.8335,  2007.9449,\n","        29141.3594])"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"markdown","source":[],"metadata":{"id":"Kz62PnhM1pcm"}},{"cell_type":"markdown","source":["# More on Pytorch Tensors"],"metadata":{"id":"5IfLaLOu1ks9"}},{"cell_type":"markdown","metadata":{"id":"i-33BKR16iWc"},"source":["## Introduction to tensors\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"gFF0N2TU7S7Q"},"source":["### Creating tensors\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YUDgG2zk7Us5","outputId":"12c750a2-018c-444a-98c4-f933a4168c22","executionInfo":{"status":"ok","timestamp":1704909005418,"user_tz":-330,"elapsed":11,"user":{"displayName":"Sarvesh Khetan","userId":"05805229168588767785"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(7)\n","0\n"]},{"output_type":"execute_result","data":{"text/plain":["7"]},"metadata":{},"execution_count":3}],"source":["# Scalar\n","scalar = torch.tensor(7)\n","\n","# printing the scale\n","print(scalar)\n","\n","# print dimensions of the tensor => hence scalar is a 0-dimension tensor\n","print(scalar.ndim)\n","\n","# Turn scalar tensor back to python integer (only works with one-element tensors)\n","scalar.item()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-IZF6ASs8QH9","outputId":"26d353fa-8815-4c53-a623-4ce8833a5f7a","executionInfo":{"status":"ok","timestamp":1704909261340,"user_tz":-330,"elapsed":1052,"user":{"displayName":"Sarvesh Khetan","userId":"05805229168588767785"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([7, 7])\n","1\n","torch.Size([2])\n"]}],"source":["# Vector\n","vector = torch.tensor([7, 7])\n","\n","# printing the vector\n","print(vector)\n","\n","# dimensions of vector => hence vector is a 1-dimension tensor. You can tell the number of dimensions a tensor in PyTorch has by the number of square brackets on the outside (`[`) and you only need to count one side.\n","print(vector.ndim)\n","\n","# Check shape of vector => shape tells you how the elements inside them are arranged => here it is two because 2 elements are placed inside the [] brackets\n","print(vector.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D5iNwCYL8QO9","outputId":"88fc63a7-4130-4c7a-a574-c61e85d2e99e"},"outputs":[{"data":{"text/plain":["tensor([[ 7,  8],\n","        [ 9, 10]])"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["# Matrix\n","MATRIX = torch.tensor([[7, 8],\n","                       [9, 10]])\n","MATRIX\n","\n","# Check number of dimensions\n","MATRIX.ndim\n","\n","MATRIX.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wEMDQr188QWW","outputId":"4230e6bd-1844-4210-eea8-245bb8b8b265"},"outputs":[{"data":{"text/plain":["tensor([[[1, 2, 3],\n","         [3, 6, 9],\n","         [2, 4, 5]]])"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["# Tensor\n","TENSOR = torch.tensor([[[1, 2, 3],\n","                        [3, 6, 9],\n","                        [2, 4, 5]]])\n","TENSOR\n","\n","# Check number of dimensions for TENSOR\n","TENSOR.ndim\n","\n","# Check shape of TENSOR\n","TENSOR.shape"]},{"cell_type":"code","source":["# Create a random tensor\n","random_tensor = torch.rand(size=(3, 4)) # this will create a tensor of size 3x4 but you can manipulate the shape how you want"],"metadata":{"id":"zh7lFn-AIFwV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zxk8GU7oTGe5"},"source":["Alright, it outputs `torch.Size([1, 3, 3])`.\n","\n","The dimensions go outer to inner.\n","\n","That means there's 1 dimension of 3 by 3.\n","\n","![example of different tensor dimensions](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/00-pytorch-different-tensor-dimensions.png)\n"]},{"cell_type":"markdown","metadata":{"id":"dms7G4nkTGe5"},"source":["### Random tensors"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EOJEtDx--GnK","outputId":"2680d44b-e31c-4ab1-d5b1-c0cd76706a0d"},"outputs":[{"data":{"text/plain":["(tensor([[0.6541, 0.4807, 0.2162, 0.6168],\n","         [0.4428, 0.6608, 0.6194, 0.8620],\n","         [0.2795, 0.6055, 0.4958, 0.5483]]),\n"," torch.float32)"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["# Create a random tensor of size (3, 4)\n","random_tensor = torch.rand(size=(3, 4))\n","random_tensor, random_tensor.dtype\n","\n","# Create a random tensor of size (224, 224, 3)\n","random_image_size_tensor = torch.rand(size=(224, 224, 3))\n","random_image_size_tensor.shape, random_image_size_tensor.ndim"]},{"cell_type":"markdown","metadata":{"id":"0MQNTY0eTGe6"},"source":["### Zeros and ones\n","\n","Sometimes you'll just want to fill tensors with zeros or ones.\n","\n","This happens a lot with masking (like masking some of the values in one tensor with zeros to let a model know not to learn them).\n","\n","Let's create a tensor full of zeros with [`torch.zeros()`](https://pytorch.org/docs/stable/generated/torch.zeros.html)\n","\n","Again, the `size` parameter comes into play."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oCzhd0hl9Vp6","outputId":"9c8ec87f-d8c9-4751-a13e-6a5e986daaa9"},"outputs":[{"data":{"text/plain":["(tensor([[0., 0., 0., 0.],\n","         [0., 0., 0., 0.],\n","         [0., 0., 0., 0.]]),\n"," torch.float32)"]},"execution_count":16,"metadata":{},"output_type":"execute_result"}],"source":["# Create a tensor of all zeros\n","zeros = torch.zeros(size=(3, 4))\n","zeros, zeros.dtype"]},{"cell_type":"markdown","metadata":{"id":"WDQBZJRUZWTN"},"source":["We can do the same to create a tensor of all ones except using [`torch.ones()` ](https://pytorch.org/docs/stable/generated/torch.ones.html) instead."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HRe6sSXiTGe6","outputId":"3f45b0b8-7f65-423d-c664-f5b5f7866fd2"},"outputs":[{"data":{"text/plain":["(tensor([[1., 1., 1., 1.],\n","         [1., 1., 1., 1.],\n","         [1., 1., 1., 1.]]),\n"," torch.float32)"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["# Create a tensor of all ones\n","ones = torch.ones(size=(3, 4))\n","ones, ones.dtype"]},{"cell_type":"markdown","metadata":{"id":"hib1NYrSarL2"},"source":["### Creating a range and tensors like\n","\n","Sometimes you might want a range of numbers, such as 1 to 10 or 0 to 100.\n","\n","You can use `torch.arange(start, end, step)` to do so.\n","\n","Where:\n","* `start` = start of range (e.g. 0)\n","* `end` = end of range (e.g. 10)\n","* `step` = how many steps in between each value (e.g. 1)\n","\n","> **Note:** In Python, you can use `range()` to create a range. However in PyTorch, `torch.range()` is deprecated and may show an error in the future."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1IqUs81d9W4W","outputId":"2a6f0c08-052e-4b36-b4eb-6a537239026f"},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_3695928/193451495.py:2: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n","  zero_to_ten_deprecated = torch.range(0, 10) # Note: this may return an error in the future\n"]},{"data":{"text/plain":["tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["# Use torch.arange(), torch.range() is deprecated\n","zero_to_ten_deprecated = torch.range(0, 10) # Note: this may return an error in the future\n","\n","# Create a range of values 0 to 10\n","zero_to_ten = torch.arange(start=0, end=10, step=1)\n","zero_to_ten"]},{"cell_type":"markdown","metadata":{"id":"i-bXf0Ugbh-D"},"source":["Sometimes you might want one tensor of a certain type with the same shape as another tensor.\n","\n","For example, a tensor of all zeros with the same shape as a previous tensor.\n","\n","To do so you can use [`torch.zeros_like(input)`](https://pytorch.org/docs/stable/generated/torch.zeros_like.html) or [`torch.ones_like(input)`](https://pytorch.org/docs/1.9.1/generated/torch.ones_like.html) which return a tensor filled with zeros or ones in the same shape as the `input` respectively."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZvXwUut5BhHq","outputId":"096b2f8e-8c21-4ace-97b9-c36b92b2fe77"},"outputs":[{"data":{"text/plain":["tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["# Can also create a tensor of zeros similar to another tensor\n","ten_zeros = torch.zeros_like(input=zero_to_ten) # will have same shape\n","ten_zeros"]},{"cell_type":"markdown","metadata":{"id":"huKZ6QlYTGe7"},"source":["### Tensor datatypes\n","\n","There are many different [tensor datatypes available in PyTorch](https://pytorch.org/docs/stable/tensors.html#data-types).\n","\n","Some are specific for CPU and some are better for GPU.\n","\n","Getting to know which is which can take some time.\n","\n","Generally if you see `torch.cuda` anywhere, the tensor is being used for GPU (since Nvidia GPUs use a computing toolkit called CUDA).\n","\n","The most common type (and generally the default) is `torch.float32` or `torch.float`.\n","\n","This is referred to as \"32-bit floating point\".\n","\n","But there's also 16-bit floating point (`torch.float16` or `torch.half`) and 64-bit floating point (`torch.float64` or `torch.double`).\n","\n","And to confuse things even more there's also 8-bit, 16-bit, 32-bit and 64-bit integers.\n","\n","Plus more!\n","\n","> **Note:** An integer is a flat round number like `7` whereas a float has a decimal `7.0`.\n","\n","The reason for all of these is to do with **precision in computing**.\n","\n","Precision is the amount of detail used to describe a number.\n","\n","The higher the precision value (8, 16, 32), the more detail and hence data used to express a number.\n","\n","This matters in deep learning and numerical computing because you're making so many operations, the more detail you have to calculate on, the more compute you have to use.\n","\n","So lower precision datatypes are generally faster to compute on but sacrifice some performance on evaluation metrics like accuracy (faster to compute but less accurate).\n","\n","> **Resources:**\n","  * See the [PyTorch documentation for a list of all available tensor datatypes](https://pytorch.org/docs/stable/tensors.html#data-types).\n","  * Read the [Wikipedia page for an overview of what precision in computing](https://en.wikipedia.org/wiki/Precision_(computer_science)) is.\n","\n","Let's see how to create some tensors with specific datatypes. We can do so using the `dtype` parameter."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q3MoGnpw9XaF","outputId":"61070939-8c52-4ac6-bed7-e64b3ce24615"},"outputs":[{"data":{"text/plain":["(torch.Size([3]), torch.float32, device(type='cpu'))"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["# Default datatype for tensors is float32\n","float_32_tensor = torch.tensor([3.0, 6.0, 9.0],\n","                               dtype=None, # defaults to None, which is torch.float32 or whatever datatype is passed\n","                               device=None, # defaults to None, which uses the default tensor type\n","                               requires_grad=False) # if True, operations performed on the tensor are recorded\n","\n","float_32_tensor.shape, float_32_tensor.dtype, float_32_tensor.device"]},{"cell_type":"markdown","metadata":{"id":"MhP8kzDfe_ty"},"source":["Aside from shape issues (tensor shapes don't match up), two of the other most common issues you'll come across in PyTorch are datatype and device issues.\n","\n","For example, one of tensors is `torch.float32` and the other is `torch.float16` (PyTorch often likes tensors to be the same format).\n","\n","Or one of your tensors is on the CPU and the other is on the GPU (PyTorch likes calculations between tensors to be on the same device).\n","\n","We'll see more of this device talk later on.\n","\n","For now let's create a tensor with `dtype=torch.float16`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PKSuajld_09s","outputId":"cbac29d9-3371-4fe1-b47c-3af4623b5fbf"},"outputs":[{"data":{"text/plain":["torch.float16"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["float_16_tensor = torch.tensor([3.0, 6.0, 9.0],\n","                               dtype=torch.float16) # torch.half would also work\n","\n","float_16_tensor.dtype"]},{"cell_type":"markdown","metadata":{"id":"gUjkB2AX7Upz"},"source":["## Getting information from tensors\n","\n","Once you've created tensors (or someone else or a PyTorch module has created them for you), you might want to get some information from them.\n","\n","We've seen these before but three of the most common attributes you'll want to find out about tensors are:\n","* `shape` - what shape is the tensor? (some operations require specific shape rules)\n","* `dtype` - what datatype are the elements within the tensor stored in?\n","* `device` - what device is the tensor stored on? (usually GPU or CPU)\n","\n","Let's create a random tensor and find out details about it."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hd_X4D0j7Umq","outputId":"86045713-ab36-4c8e-840c-e788f80c5266"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[0.4688, 0.0055, 0.8551, 0.0646],\n","        [0.6538, 0.5157, 0.4071, 0.2109],\n","        [0.9960, 0.3061, 0.9369, 0.7008]])\n","Shape of tensor: torch.Size([3, 4])\n","Datatype of tensor: torch.float32\n","Device tensor is stored on: cpu\n"]}],"source":["# Create a tensor\n","some_tensor = torch.rand(3, 4)\n","\n","# Find out details about it\n","print(some_tensor)\n","print(f\"Shape of tensor: {some_tensor.shape}\")\n","print(f\"Datatype of tensor: {some_tensor.dtype}\")\n","print(f\"Device tensor is stored on: {some_tensor.device}\") # will default to CPU"]},{"cell_type":"markdown","metadata":{"id":"45K-E5uPg6cj"},"source":["> **Note:** When you run into issues in PyTorch, it's very often one to do with one of the three attributes above. So when the error messages show up, sing yourself a little song called \"what, what, where\":\n","  * \"*what shape are my tensors? what datatype are they and where are they stored? what shape, what datatype, where where where*\""]},{"cell_type":"markdown","metadata":{"id":"BdiWvoAi7UjL"},"source":["## Manipulating tensors (tensor operations)\n","\n","In deep learning, data (images, text, video, audio, protein structures, etc) gets represented as tensors.\n","\n","A model learns by investigating those tensors and performing a series of operations (could be 1,000,000s+) on tensors to create a representation of the patterns in the input data.\n","\n","These operations are often a wonderful dance between:\n","* Addition\n","* Substraction\n","* Multiplication (element-wise)\n","* Division\n","* Matrix multiplication\n","\n","And that's it. Sure there are a few more here and there but these are the basic building blocks of neural networks.\n","\n","Stacking these building blocks in the right way, you can create the most sophisticated of neural networks (just like lego!)."]},{"cell_type":"markdown","metadata":{"id":"Sk_6Dd7L7Uce"},"source":["### Basic operations\n","\n","Let's start with a few of the fundamental operations, addition (`+`), subtraction (`-`), mutliplication (`*`).\n","\n","They work just as you think they would."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X71WpQoPD7a4","outputId":"ab30f13e-fc67-4ae4-c5ce-1006410dba07"},"outputs":[{"data":{"text/plain":["tensor([11, 12, 13])"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["# Create a tensor of values and add a number to it\n","tensor = torch.tensor([1, 2, 3])\n","tensor + 10"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sp4TlTWWEFeO","outputId":"ce7d2296-881f-4eb3-802e-fd12bc25d6ea"},"outputs":[{"data":{"text/plain":["tensor([10, 20, 30])"]},"execution_count":24,"metadata":{},"output_type":"execute_result"}],"source":["# Multiply it by 10\n","tensor * 10"]},{"cell_type":"markdown","metadata":{"id":"-1VEHnuRkn8Q"},"source":["Notice how the tensor values above didn't end up being `tensor([110, 120, 130])`, this is because the values inside the tensor don't change unless they're reassigned."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XuB1UjCIEJIA","outputId":"57cae862-c145-4681-d74b-fe6d77f2125a"},"outputs":[{"data":{"text/plain":["tensor([1, 2, 3])"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["# Tensors don't change unless reassigned\n","tensor"]},{"cell_type":"markdown","metadata":{"id":"VYvqGpUTk1o6"},"source":["Let's subtract a number and this time we'll reassign the `tensor` variable."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U4iWKoLsENry","outputId":"14d6771d-eb57-4b11-88a7-b1bb308ddc6e"},"outputs":[{"data":{"text/plain":["tensor([-9, -8, -7])"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["# Subtract and reassign\n","tensor = tensor - 10\n","tensor"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tFgZY-PaFNXa","outputId":"3536ea54-a056-444c-cd5d-6d438ddda965"},"outputs":[{"data":{"text/plain":["tensor([1, 2, 3])"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["# Add and reassign\n","tensor = tensor + 10\n","tensor"]},{"cell_type":"markdown","metadata":{"id":"CYXDoIOzk-6I"},"source":["PyTorch also has a bunch of built-in functions like [`torch.mul()`](https://pytorch.org/docs/stable/generated/torch.mul.html#torch.mul) (short for multiplication) and [`torch.add()`](https://pytorch.org/docs/stable/generated/torch.add.html) to perform basic operations."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uVysdk3kFWbY","outputId":"3a5bf687-cf24-4224-9e76-975f84638ca8"},"outputs":[{"data":{"text/plain":["tensor([10, 20, 30])"]},"execution_count":28,"metadata":{},"output_type":"execute_result"}],"source":["# Can also use torch functions\n","torch.multiply(tensor, 10)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IxuPJIpNFbqO","outputId":"f04cafd9-eaea-4254-df1a-5ab3b524d74e"},"outputs":[{"data":{"text/plain":["tensor([1, 2, 3])"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["# Original tensor is still unchanged\n","tensor"]},{"cell_type":"markdown","metadata":{"id":"70UNL33AlVQq"},"source":["However, it's more common to use the operator symbols like `*` instead of `torch.mul()`"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S5v3RkR0F2Jq","outputId":"0137caab-5ea1-4d95-f4c5-a0baa0fd652d"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([1, 2, 3]) * tensor([1, 2, 3])\n","Equals: tensor([1, 4, 9])\n"]}],"source":["# Element-wise multiplication (each element multiplies its equivalent, index 0->0, 1->1, 2->2)\n","print(tensor, \"*\", tensor)\n","print(\"Equals:\", tensor * tensor)"]},{"cell_type":"markdown","metadata":{"id":"TT5fVuyu7q5z"},"source":["### Matrix multiplication (is all you need)\n","\n","One of the most common operations in machine learning and deep learning algorithms (like neural networks) is [matrix multiplication](https://www.mathsisfun.com/algebra/matrix-multiplying.html).\n","\n","PyTorch implements matrix multiplication functionality in the [`torch.matmul()`](https://pytorch.org/docs/stable/generated/torch.matmul.html) method.\n","\n","The main two rules for matrix multiplication to remember are:\n","\n","1. The **inner dimensions** must match:\n","  * `(3, 2) @ (3, 2)` won't work\n","  * `(2, 3) @ (3, 2)` will work\n","  * `(3, 2) @ (2, 3)` will work\n","2. The resulting matrix has the shape of the **outer dimensions**:\n"," * `(2, 3) @ (3, 2)` -> `(2, 2)`\n"," * `(3, 2) @ (2, 3)` -> `(3, 3)`\n","\n","> **Note:** \"`@`\" in Python is the symbol for matrix multiplication.\n","\n","> **Resource:** You can see all of the rules for matrix multiplication using `torch.matmul()` [in the PyTorch documentation](https://pytorch.org/docs/stable/generated/torch.matmul.html).\n","\n","Let's create a tensor and perform element-wise multiplication and matrix multiplication on it.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZE7loucmDlEM","outputId":"44032bf9-c1f7-42fc-c842-dbe7a5c1221a"},"outputs":[{"data":{"text/plain":["torch.Size([3])"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","tensor = torch.tensor([1, 2, 3])\n","tensor.shape"]},{"cell_type":"markdown","metadata":{"id":"VUAZ3_b0vOKv"},"source":["The difference between element-wise multiplication and matrix multiplication is the addition of values.\n","\n","For our `tensor` variable with values `[1, 2, 3]`:\n","\n","| Operation | Calculation | Code |\n","| ----- | ----- | ----- |\n","| **Element-wise multiplication** | `[1*1, 2*2, 3*3]` = `[1, 4, 9]` | `tensor * tensor` |\n","| **Matrix multiplication** | `[1*1 + 2*2 + 3*3]` = `[14]` | `tensor.matmul(tensor)` |\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i42gkUeHvI_1","outputId":"18a630ce-bb56-4c40-81b4-9fdbb2ed7a4f"},"outputs":[{"data":{"text/plain":["tensor([1, 4, 9])"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["# Element-wise matrix multiplication\n","tensor * tensor"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PvCBiiTTDk8y","outputId":"cf623247-8f1b-49f1-e788-16da3ed1e59c"},"outputs":[{"data":{"text/plain":["tensor(14)"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["# Matrix multiplication\n","torch.matmul(tensor, tensor)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m4E_pROBDk2r","outputId":"a09af00f-277b-479e-b0a2-ad6311ee5413"},"outputs":[{"data":{"text/plain":["tensor(14)"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":["# Can also use the \"@\" symbol for matrix multiplication, though not recommended\n","tensor @ tensor"]},{"cell_type":"code","source":["# Multiply two random tensors\n","random_tensor_1 = torch.rand(size=(3, 4))\n","random_tensor_2 = torch.rand(size=(3, 4))\n","random_tensor_3 = random_tensor_1 * random_tensor_2 # PyTorch has support for most math operators in Python (+, *, -, /)"],"metadata":{"id":"ayZAKUUhIS3K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"obbginUMv43A"},"source":["You can do matrix multiplication by hand but it's not recommended.\n","\n","The in-built `torch.matmul()` method is faster."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6qMSaLOoJscL","outputId":"8bcad8a2-c900-4966-e13c-ff2cc02b9207"},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: user 773 µs, sys: 0 ns, total: 773 µs\n","Wall time: 499 µs\n"]},{"data":{"text/plain":["tensor(14)"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["%%time\n","# Matrix multiplication by hand\n","# (avoid doing operations with for loops at all cost, they are computationally expensive)\n","value = 0\n","for i in range(len(tensor)):\n","  value += tensor[i] * tensor[i]\n","value"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vVWiKB0KwH74","outputId":"fce58235-5c09-49ec-f34b-a90e5640281e"},"outputs":[{"name":"stdout","output_type":"stream","text":["CPU times: user 146 µs, sys: 83 µs, total: 229 µs\n","Wall time: 171 µs\n"]},{"data":{"text/plain":["tensor(14)"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["%%time\n","torch.matmul(tensor, tensor)"]},{"cell_type":"markdown","metadata":{"id":"aJ4DDmo1TGe-"},"source":["## One of the most common errors in deep learning (shape errors)\n","\n","Because much of deep learning is multiplying and performing operations on matrices and matrices have a strict rule about what shapes and sizes can be combined, one of the most common errors you'll run into in deep learning is shape mismatches."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":204},"id":"rN5RcoD4Jo6y","outputId":"20f6c65b-86f4-4903-d253-f6cbf0583934"},"outputs":[{"ename":"RuntimeError","evalue":"mat1 and mat2 shapes cannot be multiplied (3x2 and 3x2)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[1;32m/home/daniel/code/pytorch/pytorch-course/pytorch-deep-learning/00_pytorch_fundamentals.ipynb Cell 75\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22544954414e2d525458227d/home/daniel/code/pytorch/pytorch-course/pytorch-deep-learning/00_pytorch_fundamentals.ipynb#Y134sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m tensor_A \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([[\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m],\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22544954414e2d525458227d/home/daniel/code/pytorch/pytorch-course/pytorch-deep-learning/00_pytorch_fundamentals.ipynb#Y134sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m                          [\u001b[39m3\u001b[39m, \u001b[39m4\u001b[39m],\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22544954414e2d525458227d/home/daniel/code/pytorch/pytorch-course/pytorch-deep-learning/00_pytorch_fundamentals.ipynb#Y134sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m                          [\u001b[39m5\u001b[39m, \u001b[39m6\u001b[39m]], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22544954414e2d525458227d/home/daniel/code/pytorch/pytorch-course/pytorch-deep-learning/00_pytorch_fundamentals.ipynb#Y134sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m tensor_B \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([[\u001b[39m7\u001b[39m, \u001b[39m10\u001b[39m],\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22544954414e2d525458227d/home/daniel/code/pytorch/pytorch-course/pytorch-deep-learning/00_pytorch_fundamentals.ipynb#Y134sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m                          [\u001b[39m8\u001b[39m, \u001b[39m11\u001b[39m], \n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22544954414e2d525458227d/home/daniel/code/pytorch/pytorch-course/pytorch-deep-learning/00_pytorch_fundamentals.ipynb#Y134sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m                          [\u001b[39m9\u001b[39m, \u001b[39m12\u001b[39m]], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22544954414e2d525458227d/home/daniel/code/pytorch/pytorch-course/pytorch-deep-learning/00_pytorch_fundamentals.ipynb#Y134sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m torch\u001b[39m.\u001b[39;49mmatmul(tensor_A, tensor_B)\n","\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (3x2 and 3x2)"]}],"source":["# Shapes need to be in the right way\n","tensor_A = torch.tensor([[1, 2],\n","                         [3, 4],\n","                         [5, 6]], dtype=torch.float32)\n","\n","tensor_B = torch.tensor([[7, 10],\n","                         [8, 11],\n","                         [9, 12]], dtype=torch.float32)\n","\n","torch.matmul(tensor_A, tensor_B) # (this will error)"]},{"cell_type":"markdown","metadata":{"id":"HNA6MZEFxWVt"},"source":["We can make matrix multiplication work between `tensor_A` and `tensor_B` by making their inner dimensions match.\n","\n","One of the ways to do this is with a **transpose** (switch the dimensions of a given tensor).\n","\n","You can perform transposes in PyTorch using either:\n","* `torch.transpose(input, dim0, dim1)` - where `input` is the desired tensor to transpose and `dim0` and `dim1` are the dimensions to be swapped.\n","* `tensor.T` - where `tensor` is the desired tensor to transpose.\n","\n","Let's try the latter."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lUqgaANiy1wq","outputId":"e48bbf0c-8008-434e-d372-caa658b2f36b"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[1., 2.],\n","        [3., 4.],\n","        [5., 6.]])\n","tensor([[ 7., 10.],\n","        [ 8., 11.],\n","        [ 9., 12.]])\n"]}],"source":["# View tensor_A and tensor_B\n","print(tensor_A)\n","print(tensor_B)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DveqxO7iy_Fi","outputId":"1bd2e85b-ea4d-4948-c408-8eb46ef3534c"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[1., 2.],\n","        [3., 4.],\n","        [5., 6.]])\n","tensor([[ 7.,  8.,  9.],\n","        [10., 11., 12.]])\n"]}],"source":["# View tensor_A and tensor_B.T\n","print(tensor_A)\n","print(tensor_B.T)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"35rEIu-NKtVE","outputId":"0b32c7f1-556e-45d4-de22-388419e93dc2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Original shapes: tensor_A = torch.Size([3, 2]), tensor_B = torch.Size([3, 2])\n","\n","New shapes: tensor_A = torch.Size([3, 2]) (same as above), tensor_B.T = torch.Size([2, 3])\n","\n","Multiplying: torch.Size([3, 2]) * torch.Size([2, 3]) <- inner dimensions match\n","\n","Output:\n","\n","tensor([[ 27.,  30.,  33.],\n","        [ 61.,  68.,  75.],\n","        [ 95., 106., 117.]])\n","\n","Output shape: torch.Size([3, 3])\n"]}],"source":["# The operation works when tensor_B is transposed\n","print(f\"Original shapes: tensor_A = {tensor_A.shape}, tensor_B = {tensor_B.shape}\\n\")\n","print(f\"New shapes: tensor_A = {tensor_A.shape} (same as above), tensor_B.T = {tensor_B.T.shape}\\n\")\n","print(f\"Multiplying: {tensor_A.shape} * {tensor_B.T.shape} <- inner dimensions match\\n\")\n","print(\"Output:\\n\")\n","output = torch.matmul(tensor_A, tensor_B.T)\n","print(output)\n","print(f\"\\nOutput shape: {output.shape}\")"]},{"cell_type":"markdown","metadata":{"id":"MfcFEqfLjN24"},"source":["You can also use [`torch.mm()`](https://pytorch.org/docs/stable/generated/torch.mm.html) which is a short for `torch.matmul()`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x3rJvW_TTGe_","outputId":"2c501972-20bf-4a83-ad4a-b5f1b2424097"},"outputs":[{"data":{"text/plain":["tensor([[ 27.,  30.,  33.],\n","        [ 61.,  68.,  75.],\n","        [ 95., 106., 117.]])"]},"execution_count":41,"metadata":{},"output_type":"execute_result"}],"source":["# torch.mm is a shortcut for matmul\n","torch.mm(tensor_A, tensor_B.T)"]},{"cell_type":"markdown","metadata":{"id":"bXKozI4T0hFi"},"source":["Without the transpose, the rules of matrix mulitplication aren't fulfilled and we get an error like above.\n","\n","How about a visual?\n","\n","![visual demo of matrix multiplication](https://github.com/mrdbourke/pytorch-deep-learning/raw/main/images/00-matrix-multiply-crop.gif)\n","\n","You can create your own matrix multiplication visuals like this at http://matrixmultiplication.xyz/.\n","\n","> **Note:** A matrix multiplication like this is also referred to as the [**dot product**](https://www.mathsisfun.com/algebra/vectors-dot-product.html) of two matrices.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"hA64Z4DmkB31"},"source":["Neural networks are full of matrix multiplications and dot products.\n","\n","The [`torch.nn.Linear()`](https://pytorch.org/docs/1.9.1/generated/torch.nn.Linear.html) module (we'll see this in action later on), also known as a feed-forward layer or fully connected layer, implements a matrix multiplication between an input `x` and a weights matrix `A`.\n","\n","$$\n","y = x\\cdot{A^T} + b\n","$$\n","\n","Where:\n","* `x` is the input to the layer (deep learning is a stack of layers like `torch.nn.Linear()` and others on top of each other).\n","* `A` is the weights matrix created by the layer, this starts out as random numbers that get adjusted as a neural network learns to better represent patterns in the data (notice the \"`T`\", that's because the weights matrix gets transposed).\n","  * **Note:** You might also often see `W` or another letter like `X` used to showcase the weights matrix.\n","* `b` is the bias term used to slightly offset the weights and inputs.\n","* `y` is the output (a manipulation of the input in the hopes to discover patterns in it).\n","\n","This is a linear function (you may have seen something like $y = mx+b$ in high school or elsewhere), and can be used to draw a straight line!\n","\n","Let's play around with a linear layer.\n","\n","Try changing the values of `in_features` and `out_features` below and see what happens.\n","\n","Do you notice anything to do with the shapes?"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mC_MjKW1LX7T","outputId":"768f75d2-c978-4df3-e18a-4684d46bdfa9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Input shape: torch.Size([3, 2])\n","\n","Output:\n","tensor([[2.2368, 1.2292, 0.4714, 0.3864, 0.1309, 0.9838],\n","        [4.4919, 2.1970, 0.4469, 0.5285, 0.3401, 2.4777],\n","        [6.7469, 3.1648, 0.4224, 0.6705, 0.5493, 3.9716]],\n","       grad_fn=<AddmmBackward0>)\n","\n","Output shape: torch.Size([3, 6])\n"]}],"source":["# Since the linear layer starts with a random weights matrix, let's make it reproducible (more on this later)\n","torch.manual_seed(42)\n","# This uses matrix multiplication\n","linear = torch.nn.Linear(in_features=2, # in_features = matches inner dimension of input\n","                         out_features=6) # out_features = describes outer value\n","x = tensor_A\n","output = linear(x)\n","print(f\"Input shape: {x.shape}\\n\")\n","print(f\"Output:\\n{output}\\n\\nOutput shape: {output.shape}\")"]},{"cell_type":"markdown","metadata":{"id":"zIGrP5j1pN7j"},"source":["> **Question:** What happens if you change `in_features` from 2 to 3 above? Does it error? How could you change the shape of the input (`x`) to accomodate to the error? Hint: what did we have to do to `tensor_B` above?"]},{"cell_type":"markdown","metadata":{"id":"EPNF0nMWoGEj"},"source":["If you've never done it before, matrix multiplication can be a confusing topic at first.\n","\n","But after you've played around with it a few times and even cracked open a few neural networks, you'll notice it's everywhere.\n","\n","Remember, matrix multiplication is all you need.\n","\n","![matrix multiplication is all you need](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/00_matrix_multiplication_is_all_you_need.jpeg)\n","\n","*When you start digging into neural network layers and building your own, you'll find matrix multiplications everywhere. **Source:** https://marksaroufim.substack.com/p/working-class-deep-learner*"]},{"cell_type":"markdown","metadata":{"id":"pjMmrJOOPv5e"},"source":["### Finding the min, max, mean, sum, etc (aggregation)\n","\n","Now we've seen a few ways to manipulate tensors, let's run through a few ways to aggregate them (go from more values to less values).\n","\n","First we'll create a tensor and then find the max, min, mean and sum of it.\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jrFQbe5fP1Rk","outputId":"034013c1-b384-4a0d-edf8-295ed3a456f1"},"outputs":[{"data":{"text/plain":["tensor([ 0, 10, 20, 30, 40, 50, 60, 70, 80, 90])"]},"execution_count":43,"metadata":{},"output_type":"execute_result"}],"source":["# Create a tensor\n","x = torch.arange(0, 100, 10)\n","x"]},{"cell_type":"markdown","metadata":{"id":"-J-wfMdlsEco"},"source":["Now let's perform some aggregation."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e5wSP9YKP3Lb","outputId":"3aa238c7-646f-434f-a55c-292aabef7227"},"outputs":[{"name":"stdout","output_type":"stream","text":["Minimum: 0\n","Maximum: 90\n","Mean: 45.0\n","Sum: 450\n"]}],"source":["print(f\"Minimum: {x.min()}\")\n","print(f\"Maximum: {x.max()}\")\n","# print(f\"Mean: {x.mean()}\") # this will error\n","print(f\"Mean: {x.type(torch.float32).mean()}\") # won't work without float datatype\n","print(f\"Sum: {x.sum()}\")"]},{"cell_type":"markdown","metadata":{"id":"JHoKpsg3sKQE"},"source":["> **Note:** You may find some methods such as `torch.mean()` require tensors to be in `torch.float32` (the most common) or another specific datatype, otherwise the operation will fail.\n","\n","You can also do the same as above with `torch` methods."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0Cr23Y9uP3HO","outputId":"9c86d805-eef2-465c-e2c8-2bccd515e6d5"},"outputs":[{"data":{"text/plain":["(tensor(90), tensor(0), tensor(45.), tensor(450))"]},"execution_count":45,"metadata":{},"output_type":"execute_result"}],"source":["torch.max(x), torch.min(x), torch.mean(x.type(torch.float32)), torch.sum(x)"]},{"cell_type":"markdown","metadata":{"id":"i7ApCaZjDkvp"},"source":["### Positional min/max\n","\n","You can also find the index of a tensor where the max or minimum occurs with [`torch.argmax()`](https://pytorch.org/docs/stable/generated/torch.argmax.html) and [`torch.argmin()`](https://pytorch.org/docs/stable/generated/torch.argmin.html) respectively.\n","\n","This is helpful incase you just want the position where the highest (or lowest) value is and not the actual value itself (we'll see this in a later section when using the [softmax activation function](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html))."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FzNBl9JSGlHi","outputId":"01e0740e-c34f-469b-9c8f-9e6e5f0363af"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tensor: tensor([10, 20, 30, 40, 50, 60, 70, 80, 90])\n","Index where max value occurs: 8\n","Index where min value occurs: 0\n"]}],"source":["# Create a tensor\n","tensor = torch.arange(10, 100, 10)\n","print(f\"Tensor: {tensor}\")\n","\n","# Returns index of max and min values\n","print(f\"Index where max value occurs: {tensor.argmax()}\")\n","print(f\"Index where min value occurs: {tensor.argmin()}\")"]},{"cell_type":"markdown","metadata":{"id":"QBu33WihOXBk"},"source":["### Change tensor datatype\n","\n","As mentioned, a common issue with deep learning operations is having your tensors in different datatypes.\n","\n","If one tensor is in `torch.float64` and another is in `torch.float32`, you might run into some errors.\n","\n","But there's a fix.\n","\n","You can change the datatypes of tensors using [`torch.Tensor.type(dtype=None)`](https://pytorch.org/docs/stable/generated/torch.Tensor.type.html) where the `dtype` parameter is the datatype you'd like to use.\n","\n","First we'll create a tensor and check it's datatype (the default is `torch.float32`)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rY2FEsCAOaLu","outputId":"507f1ade-7c7a-4172-fa48-60c9ac4831c0"},"outputs":[{"data":{"text/plain":["torch.float32"]},"execution_count":47,"metadata":{},"output_type":"execute_result"}],"source":["# Create a tensor and check its datatype\n","tensor = torch.arange(10., 100., 10.)\n","tensor.dtype"]},{"cell_type":"markdown","metadata":{"id":"jR30FHEc92of"},"source":["Now we'll create another tensor the same as before but change its datatype to `torch.float16`.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Cac8gRYjOeab","outputId":"96e5ce12-bc29-4a2b-f81c-bfc89ea2d075"},"outputs":[{"data":{"text/plain":["tensor([10., 20., 30., 40., 50., 60., 70., 80., 90.], dtype=torch.float16)"]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["# Create a float16 tensor\n","tensor_float16 = tensor.type(torch.float16)\n","tensor_float16"]},{"cell_type":"markdown","metadata":{"id":"ndVlKJZ4-7_5"},"source":["And we can do something similar to make a `torch.int8` tensor."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8Yqovld2Oj6s","outputId":"667da17f-e38f-404a-bd2d-63683e45c99a"},"outputs":[{"data":{"text/plain":["tensor([10, 20, 30, 40, 50, 60, 70, 80, 90], dtype=torch.int8)"]},"execution_count":49,"metadata":{},"output_type":"execute_result"}],"source":["# Create a int8 tensor\n","tensor_int8 = tensor.type(torch.int8)\n","tensor_int8"]},{"cell_type":"markdown","metadata":{"id":"44GxVabar-xe"},"source":["> **Note:** Different datatypes can be confusing to begin with. But think of it like this, the lower the number (e.g. 32, 16, 8), the less precise a computer stores the value. And with a lower amount of storage, this generally results in faster computation and a smaller overall model. Mobile-based neural networks often operate with 8-bit integers, smaller and faster to run but less accurate than their float32 counterparts. For more on this, I'd read up about [precision in computing](https://en.wikipedia.org/wiki/Precision_(computer_science)).\n","\n","> **Exercise:** So far we've covered a fair few tensor methods but there's a bunch more in the [`torch.Tensor` documentation](https://pytorch.org/docs/stable/tensors.html), I'd recommend spending 10-minutes scrolling through and looking into any that catch your eye. Click on them and then write them out in code yourself to see what happens."]},{"cell_type":"markdown","metadata":{"id":"7CkCtAYmGsHY"},"source":["### Reshaping, stacking, squeezing and unsqueezing\n","\n","Often times you'll want to reshape or change the dimensions of your tensors without actually changing the values inside them.\n","\n","To do so, some popular methods are:\n","\n","| Method | One-line description |\n","| ----- | ----- |\n","| [`torch.reshape(input, shape)`](https://pytorch.org/docs/stable/generated/torch.reshape.html#torch.reshape) | Reshapes `input` to `shape` (if compatible), can also use `torch.Tensor.reshape()`. |\n","| [`Tensor.view(shape)`](https://pytorch.org/docs/stable/generated/torch.Tensor.view.html) | Returns a view of the original tensor in a different `shape` but shares the same data as the original tensor. |\n","| [`torch.stack(tensors, dim=0)`](https://pytorch.org/docs/1.9.1/generated/torch.stack.html) | Concatenates a sequence of `tensors` along a new dimension (`dim`), all `tensors` must be same size. |\n","| [`torch.squeeze(input)`](https://pytorch.org/docs/stable/generated/torch.squeeze.html) | Squeezes `input` to remove all the dimenions with value `1`. |\n","| [`torch.unsqueeze(input, dim)`](https://pytorch.org/docs/1.9.1/generated/torch.unsqueeze.html) | Returns `input` with a dimension value of `1` added at `dim`. |\n","| [`torch.permute(input, dims)`](https://pytorch.org/docs/stable/generated/torch.permute.html) | Returns a *view* of the original `input` with its dimensions permuted (rearranged) to `dims`. |\n","\n","Why do any of these?\n","\n","Because deep learning models (neural networks) are all about manipulating tensors in some way. And because of the rules of matrix multiplication, if you've got shape mismatches, you'll run into errors. These methods help you make sure the right elements of your tensors are mixing with the right elements of other tensors.\n","\n","Let's try them out.\n","\n","First, we'll create a tensor."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EYjRTLOzG4Ev","outputId":"f7f2719c-15ce-406b-dc8f-4477046cd5d9"},"outputs":[{"data":{"text/plain":["(tensor([1., 2., 3., 4., 5., 6., 7.]), torch.Size([7]))"]},"execution_count":50,"metadata":{},"output_type":"execute_result"}],"source":["# Create a tensor\n","import torch\n","x = torch.arange(1., 8.)\n","x, x.shape"]},{"cell_type":"markdown","metadata":{"id":"3_VarMO9CoT8"},"source":["Now let's add an extra dimension with `torch.reshape()`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"US4WjpQ3SG-8","outputId":"c519d59e-85f1-4a10-eaaa-acb487028e3a"},"outputs":[{"data":{"text/plain":["(tensor([[1., 2., 3., 4., 5., 6., 7.]]), torch.Size([1, 7]))"]},"execution_count":51,"metadata":{},"output_type":"execute_result"}],"source":["# Add an extra dimension\n","x_reshaped = x.reshape(1, 7)\n","x_reshaped, x_reshaped.shape"]},{"cell_type":"markdown","metadata":{"id":"tig5xm0jCxuU"},"source":["We can also change the view with `torch.view()`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WDN2BNe5TGfB","outputId":"3df1b0d6-2548-4ecc-ca25-0c4e28a6e536"},"outputs":[{"data":{"text/plain":["(tensor([[1., 2., 3., 4., 5., 6., 7.]]), torch.Size([1, 7]))"]},"execution_count":52,"metadata":{},"output_type":"execute_result"}],"source":["# Change view (keeps same data as original but changes view)\n","# See more: https://stackoverflow.com/a/54507446/7900723\n","z = x.view(1, 7)\n","z, z.shape"]},{"cell_type":"markdown","metadata":{"id":"m8joAaUEC2NX"},"source":["Remember though, changing the view of a tensor with `torch.view()` really only creates a new view of the *same* tensor.\n","\n","So changing the view changes the original tensor too."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2DxURVvXTGfC","outputId":"668d194d-dd0a-4db1-da00-9c3fd8849186"},"outputs":[{"data":{"text/plain":["(tensor([[5., 2., 3., 4., 5., 6., 7.]]), tensor([5., 2., 3., 4., 5., 6., 7.]))"]},"execution_count":53,"metadata":{},"output_type":"execute_result"}],"source":["# Changing z changes x\n","z[:, 0] = 5\n","z, x"]},{"cell_type":"markdown","metadata":{"id":"YxnqDBlpDDJ_"},"source":["If we wanted to stack our new tensor on top of itself five times, we could do so with `torch.stack()`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pX5Adf3ORiTK","outputId":"703e8568-61df-4ebd-f4d3-a6366dc265c0"},"outputs":[{"data":{"text/plain":["tensor([[5., 2., 3., 4., 5., 6., 7.],\n","        [5., 2., 3., 4., 5., 6., 7.],\n","        [5., 2., 3., 4., 5., 6., 7.],\n","        [5., 2., 3., 4., 5., 6., 7.]])"]},"execution_count":54,"metadata":{},"output_type":"execute_result"}],"source":["# Stack tensors on top of each other\n","x_stacked = torch.stack([x, x, x, x], dim=0) # try changing dim to dim=1 and see what happens\n","x_stacked"]},{"cell_type":"markdown","metadata":{"id":"ET56QzNHDuOI"},"source":["How about removing all single dimensions from a tensor?\n","\n","To do so you can use `torch.squeeze()` (I remember this as *squeezing* the tensor to only have dimensions over 1)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w2Y2HEoDRxJZ","outputId":"dd0645a6-1cdd-46bc-a3a2-433d9cd09336"},"outputs":[{"name":"stdout","output_type":"stream","text":["Previous tensor: tensor([[5., 2., 3., 4., 5., 6., 7.]])\n","Previous shape: torch.Size([1, 7])\n","\n","New tensor: tensor([5., 2., 3., 4., 5., 6., 7.])\n","New shape: torch.Size([7])\n"]}],"source":["print(f\"Previous tensor: {x_reshaped}\")\n","print(f\"Previous shape: {x_reshaped.shape}\")\n","\n","# Remove extra dimension from x_reshaped\n","x_squeezed = x_reshaped.squeeze()\n","print(f\"\\nNew tensor: {x_squeezed}\")\n","print(f\"New shape: {x_squeezed.shape}\")"]},{"cell_type":"markdown","metadata":{"id":"acjDLk8WD8NC"},"source":["And to do the reverse of `torch.squeeze()` you can use `torch.unsqueeze()` to add a dimension value of 1 at a specific index."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CUC-DEEwSYv7","outputId":"da60e019-3ea6-42f8-8e47-ba037ead737f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Previous tensor: tensor([5., 2., 3., 4., 5., 6., 7.])\n","Previous shape: torch.Size([7])\n","\n","New tensor: tensor([[5., 2., 3., 4., 5., 6., 7.]])\n","New shape: torch.Size([1, 7])\n"]}],"source":["print(f\"Previous tensor: {x_squeezed}\")\n","print(f\"Previous shape: {x_squeezed.shape}\")\n","\n","## Add an extra dimension with unsqueeze\n","x_unsqueezed = x_squeezed.unsqueeze(dim=0)\n","print(f\"\\nNew tensor: {x_unsqueezed}\")\n","print(f\"New shape: {x_unsqueezed.shape}\")"]},{"cell_type":"markdown","metadata":{"id":"R9DuJzXgFbM5"},"source":["You can also rearrange the order of axes values with `torch.permute(input, dims)`, where the `input` gets turned into a *view* with new `dims`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fCRGCX8DTGfC","outputId":"6853328b-a1cf-4470-f366-106a231a189c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Previous shape: torch.Size([224, 224, 3])\n","New shape: torch.Size([3, 224, 224])\n"]}],"source":["# Create tensor with specific shape\n","x_original = torch.rand(size=(224, 224, 3))\n","\n","# Permute the original tensor to rearrange the axis order\n","x_permuted = x_original.permute(2, 0, 1) # shifts axis 0->1, 1->2, 2->0\n","\n","print(f\"Previous shape: {x_original.shape}\")\n","print(f\"New shape: {x_permuted.shape}\")"]},{"cell_type":"markdown","metadata":{"id":"06LKaFemGBoE"},"source":["> **Note**: Because permuting returns a *view* (shares the same data as the original), the values in the permuted tensor will be the same as the original tensor and if you change the values in the view, it will change the values of the original."]},{"cell_type":"markdown","metadata":{"id":"nEPqVL7fTGfC"},"source":["## Indexing (selecting data from tensors)\n","\n","Sometimes you'll want to select specific data from tensors (for example, only the first column or second row).\n","\n","To do so, you can use indexing.\n","\n","If you've ever done indexing on Python lists or NumPy arrays, indexing in PyTorch with tensors is very similar."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oSXzdxCQTGfD","outputId":"05a72c08-5f8c-433a-cd31-46065686f825"},"outputs":[{"data":{"text/plain":["(tensor([[[1, 2, 3],\n","          [4, 5, 6],\n","          [7, 8, 9]]]),\n"," torch.Size([1, 3, 3]))"]},"execution_count":58,"metadata":{},"output_type":"execute_result"}],"source":["# Create a tensor\n","import torch\n","x = torch.arange(1, 10).reshape(1, 3, 3)\n","x, x.shape"]},{"cell_type":"markdown","metadata":{"id":"xQG5krnKG43B"},"source":["Indexing values goes outer dimension -> inner dimension (check out the square brackets)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zv_Z3IAzTGfD","outputId":"cf6c0936-7600-4af4-9b6f-f6b8ac9b4c05"},"outputs":[{"name":"stdout","output_type":"stream","text":["First square bracket:\n","tensor([[1, 2, 3],\n","        [4, 5, 6],\n","        [7, 8, 9]])\n","Second square bracket: tensor([1, 2, 3])\n","Third square bracket: 1\n"]}],"source":["# Let's index bracket by bracket\n","print(f\"First square bracket:\\n{x[0]}\")\n","print(f\"Second square bracket: {x[0][0]}\")\n","print(f\"Third square bracket: {x[0][0][0]}\")"]},{"cell_type":"markdown","metadata":{"id":"XaLjaIFxHe89"},"source":["You can also use `:` to specify \"all values in this dimension\" and then use a comma (`,`) to add another dimension."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gCT09pqeTGfD","outputId":"a91f9b73-f8f0-476a-9c69-fcd03b042f6b"},"outputs":[{"data":{"text/plain":["tensor([[1, 2, 3]])"]},"execution_count":60,"metadata":{},"output_type":"execute_result"}],"source":["# Get all values of 0th dimension and the 0 index of 1st dimension\n","x[:, 0]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dwDx_gMsTGfD","outputId":"8165cfd9-a88d-4212-8c45-1eb84ef5be83"},"outputs":[{"data":{"text/plain":["tensor([[2, 5, 8]])"]},"execution_count":61,"metadata":{},"output_type":"execute_result"}],"source":["# Get all values of 0th & 1st dimensions but only index 1 of 2nd dimension\n","x[:, :, 1]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xiw3_1E3TGfD","outputId":"12fa4749-cf52-4e88-c2c0-44d26aeb633c"},"outputs":[{"data":{"text/plain":["tensor([5])"]},"execution_count":62,"metadata":{},"output_type":"execute_result"}],"source":["# Get all values of the 0 dimension but only the 1 index value of the 1st and 2nd dimension\n","x[:, 1, 1]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XFVEgrKhTGfD","outputId":"69eadeb9-11b3-4b48-cb95-0b3305c1274c"},"outputs":[{"data":{"text/plain":["tensor([1, 2, 3])"]},"execution_count":63,"metadata":{},"output_type":"execute_result"}],"source":["# Get index 0 of 0th and 1st dimension and all values of 2nd dimension\n","x[0, 0, :] # same as x[0][0]"]},{"cell_type":"markdown","metadata":{"id":"6Ik0r11RIxtm"},"source":["Indexing can be quite confusing to begin with, especially with larger tensors (I still have to try indexing multiple times to get it right). But with a bit of practice and following the data explorer's motto (***visualize, visualize, visualize***), you'll start to get the hang of it."]},{"cell_type":"markdown","metadata":{"id":"h8ZaW0Bq7rCm"},"source":["## PyTorch tensors & NumPy\n","\n","Since NumPy is a popular Python numerical computing library, PyTorch has functionality to interact with it nicely.  \n","\n","The two main methods you'll want to use for NumPy to PyTorch (and back again) are:\n","* [`torch.from_numpy(ndarray)`](https://pytorch.org/docs/stable/generated/torch.from_numpy.html) - NumPy array -> PyTorch tensor.\n","* [`torch.Tensor.numpy()`](https://pytorch.org/docs/stable/generated/torch.Tensor.numpy.html) - PyTorch tensor -> NumPy array.\n","\n","Let's try them out."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yDrDCnvY7rKS","outputId":"86155a63-01f9-4372-e889-61a65ebf0fb1"},"outputs":[{"data":{"text/plain":["(array([1., 2., 3., 4., 5., 6., 7.]),\n"," tensor([1., 2., 3., 4., 5., 6., 7.], dtype=torch.float64))"]},"execution_count":64,"metadata":{},"output_type":"execute_result"}],"source":["# NumPy array to tensor\n","import torch\n","import numpy as np\n","array = np.arange(1.0, 8.0)\n","tensor = torch.from_numpy(array)\n","array, tensor"]},{"cell_type":"markdown","metadata":{"id":"16JG6cONLPnO"},"source":["> **Note:** By default, NumPy arrays are created with the datatype `float64` and if you convert it to a PyTorch tensor, it'll keep the same datatype (as above).\n",">\n","> However, many PyTorch calculations default to using `float32`.\n",">\n","> So if you want to convert your NumPy array (float64) -> PyTorch tensor (float64) -> PyTorch tensor (float32), you can use `tensor = torch.from_numpy(array).type(torch.float32)`.\n","\n","Because we reassigned `tensor` above, if you change the tensor, the array stays the same."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ovwl7VCREv8L","outputId":"efd21eb9-0010-436a-dc29-f851e3d7d77a"},"outputs":[{"data":{"text/plain":["(array([2., 3., 4., 5., 6., 7., 8.]),\n"," tensor([1., 2., 3., 4., 5., 6., 7.], dtype=torch.float64))"]},"execution_count":65,"metadata":{},"output_type":"execute_result"}],"source":["# Change the array, keep the tensor\n","array = array + 1\n","array, tensor"]},{"cell_type":"markdown","metadata":{"id":"geVvu1p0MTWc"},"source":["And if you want to go from PyTorch tensor to NumPy array, you can call `tensor.numpy()`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xw_7ZyVaTKxQ","outputId":"54d6f347-d3f6-44df-9155-83d980c31780"},"outputs":[{"data":{"text/plain":["(tensor([1., 1., 1., 1., 1., 1., 1.]),\n"," array([1., 1., 1., 1., 1., 1., 1.], dtype=float32))"]},"execution_count":66,"metadata":{},"output_type":"execute_result"}],"source":["# Tensor to NumPy array\n","tensor = torch.ones(7) # create a tensor of ones with dtype=float32\n","numpy_tensor = tensor.numpy() # will be dtype=float32 unless changed\n","tensor, numpy_tensor"]},{"cell_type":"markdown","metadata":{"id":"Dt8yEV1jMfi2"},"source":["And the same rule applies as above, if you change the original `tensor`, the new `numpy_tensor` stays the same."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mMp6ZSkET4_Y","outputId":"100678a4-c220-4a44-e4a5-0542359cb9de"},"outputs":[{"data":{"text/plain":["(tensor([2., 2., 2., 2., 2., 2., 2.]),\n"," array([1., 1., 1., 1., 1., 1., 1.], dtype=float32))"]},"execution_count":67,"metadata":{},"output_type":"execute_result"}],"source":["# Change the tensor, keep the array the same\n","tensor = tensor + 1\n","tensor, numpy_tensor"]},{"cell_type":"markdown","metadata":{"id":"7gU3ubCrUkI-"},"source":["## Reproducibility (trying to take the random out of random)\n","\n","As you learn more about neural networks and machine learning, you'll start to discover how much randomness plays a part.\n","\n","Well, pseudorandomness that is. Because after all, as they're designed, a computer is fundamentally deterministic (each step is predictable) so the randomness they create are simulated randomness (though there is debate on this too, but since I'm not a computer scientist, I'll let you find out more yourself).\n","\n","How does this relate to neural networks and deep learning then?\n","\n","We've discussed neural networks start with random numbers to describe patterns in data (these numbers are poor descriptions) and try to improve those random numbers using tensor operations (and a few other things we haven't discussed yet) to better describe patterns in data.\n","\n","In short:\n","\n","``start with random numbers -> tensor operations -> try to make better (again and again and again)``\n","\n","Although randomness is nice and powerful, sometimes you'd like there to be a little less randomness.\n","\n","Why?\n","\n","So you can perform repeatable experiments.\n","\n","For example, you create an algorithm capable of achieving X performance.\n","\n","And then your friend tries it out to verify you're not crazy.\n","\n","How could they do such a thing?\n","\n","That's where **reproducibility** comes in.\n","\n","In other words, can you get the same (or very similar) results on your computer running the same code as I get on mine?\n","\n","Let's see a brief example of reproducibility in PyTorch.\n","\n","We'll start by creating two random tensors, since they're random, you'd expect them to be different right?"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eSwxnwEbTGfF","outputId":"73b34154-734f-496f-9b55-b6aaa137e854"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tensor A:\n","tensor([[0.8016, 0.3649, 0.6286, 0.9663],\n","        [0.7687, 0.4566, 0.5745, 0.9200],\n","        [0.3230, 0.8613, 0.0919, 0.3102]])\n","\n","Tensor B:\n","tensor([[0.9536, 0.6002, 0.0351, 0.6826],\n","        [0.3743, 0.5220, 0.1336, 0.9666],\n","        [0.9754, 0.8474, 0.8988, 0.1105]])\n","\n","Does Tensor A equal Tensor B? (anywhere)\n"]},{"data":{"text/plain":["tensor([[False, False, False, False],\n","        [False, False, False, False],\n","        [False, False, False, False]])"]},"execution_count":68,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","\n","# Create two random tensors\n","random_tensor_A = torch.rand(3, 4)\n","random_tensor_B = torch.rand(3, 4)\n","\n","print(f\"Tensor A:\\n{random_tensor_A}\\n\")\n","print(f\"Tensor B:\\n{random_tensor_B}\\n\")\n","print(f\"Does Tensor A equal Tensor B? (anywhere)\")\n","random_tensor_A == random_tensor_B"]},{"cell_type":"markdown","metadata":{"id":"nPU6mDKJnr8M"},"source":["Just as you might've expected, the tensors come out with different values.\n","\n","But what if you wanted to created two random tensors with the *same* values.\n","\n","As in, the tensors would still contain random values but they would be of the same flavour.\n","\n","That's where [`torch.manual_seed(seed)`](https://pytorch.org/docs/stable/generated/torch.manual_seed.html) comes in, where `seed` is an integer (like `42` but it could be anything) that flavours the randomness.\n","\n","Let's try it out by creating some more *flavoured* random tensors."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sB6d1GfYTGfF","outputId":"4d11d38e-4406-4aff-9a81-cf13aa89ee5f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Tensor C:\n","tensor([[0.8823, 0.9150, 0.3829, 0.9593],\n","        [0.3904, 0.6009, 0.2566, 0.7936],\n","        [0.9408, 0.1332, 0.9346, 0.5936]])\n","\n","Tensor D:\n","tensor([[0.8823, 0.9150, 0.3829, 0.9593],\n","        [0.3904, 0.6009, 0.2566, 0.7936],\n","        [0.9408, 0.1332, 0.9346, 0.5936]])\n","\n","Does Tensor C equal Tensor D? (anywhere)\n"]},{"data":{"text/plain":["tensor([[True, True, True, True],\n","        [True, True, True, True],\n","        [True, True, True, True]])"]},"execution_count":69,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","import random\n","\n","# # Set the random seed\n","RANDOM_SEED=42 # try changing this to different values and see what happens to the numbers below\n","torch.manual_seed(seed=RANDOM_SEED)\n","random_tensor_C = torch.rand(3, 4)\n","\n","# Have to reset the seed every time a new rand() is called\n","# Without this, tensor_D would be different to tensor_C\n","torch.random.manual_seed(seed=RANDOM_SEED) # try commenting this line out and seeing what happens\n","random_tensor_D = torch.rand(3, 4)\n","\n","print(f\"Tensor C:\\n{random_tensor_C}\\n\")\n","print(f\"Tensor D:\\n{random_tensor_D}\\n\")\n","print(f\"Does Tensor C equal Tensor D? (anywhere)\")\n","random_tensor_C == random_tensor_D"]},{"cell_type":"markdown","metadata":{"id":"uct53Xr5QRC_"},"source":["Nice!\n","\n","It looks like setting the seed worked.\n","\n","> **Resource:** What we've just covered only scratches the surface of reproducibility in PyTorch. For more, on reproducbility in general and random seeds, I'd checkout:\n","> * [The PyTorch reproducibility documentation](https://pytorch.org/docs/stable/notes/randomness.html) (a good exericse would be to read through this for 10-minutes and even if you don't understand it now, being aware of it is important).\n","> * [The Wikipedia random seed page](https://en.wikipedia.org/wiki/Random_seed) (this'll give a good overview of random seeds and pseudorandomness in general)."]},{"cell_type":"markdown","metadata":{"id":"hxIIM7t27rQ-"},"source":["## Running tensors on GPUs (and making faster computations)\n","\n","Deep learning algorithms require a lot of numerical operations.\n","\n","And by default these operations are often done on a CPU (computer processing unit).\n","\n","However, there's another common piece of hardware called a GPU (graphics processing unit), which is often much faster at performing the specific types of operations neural networks need (matrix multiplications) than CPUs.\n","\n","Your computer might have one.\n","\n","If so, you should look to use it whenever you can to train neural networks because chances are it'll speed up the training time dramatically.\n","\n","There are a few ways to first get access to a GPU and secondly get PyTorch to use the GPU.\n","\n","> **Note:** When I reference \"GPU\" throughout this course, I'm referencing a [Nvidia GPU with CUDA](https://developer.nvidia.com/cuda-gpus) enabled (CUDA is a computing platform and API that helps allow GPUs be used for general purpose computing & not just graphics) unless otherwise specified.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"0UiR6QpoYQH_"},"source":["\n","### 1. Getting a GPU\n","\n","You may already know what's going on when I say GPU. But if not, there are a few ways to get access to one.\n","\n","| **Method** | **Difficulty to setup** | **Pros** | **Cons** | **How to setup** |\n","| ----- | ----- | ----- | ----- | ----- |\n","| Google Colab | Easy | Free to use, almost zero setup required, can share work with others as easy as a link | Doesn't save your data outputs, limited compute, subject to timeouts | [Follow the Google Colab Guide](https://colab.research.google.com/notebooks/gpu.ipynb) |\n","| Use your own | Medium | Run everything locally on your own machine | GPUs aren't free, require upfront cost | Follow the [PyTorch installation guidelines](https://pytorch.org/get-started/locally/) |\n","| Cloud computing (AWS, GCP, Azure) | Medium-Hard | Small upfront cost, access to almost infinite compute | Can get expensive if running continually, takes some time to setup right | Follow the [PyTorch installation guidelines](https://pytorch.org/get-started/cloud-partners/) |\n","\n","There are more options for using GPUs but the above three will suffice for now.\n","\n","Personally, I use a combination of Google Colab and my own personal computer for small scale experiments (and creating this course) and go to cloud resources when I need more compute power.\n","\n","> **Resource:** If you're looking to purchase a GPU of your own but not sure what to get, [Tim Dettmers has an excellent guide](https://timdettmers.com/2020/09/07/which-gpu-for-deep-learning/).\n","\n","To check if you've got access to a Nvidia GPU, you can run `!nvidia-smi` where the `!` (also called bang) means \"run this on the command line\".\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vEMcO-9zYc-w","outputId":"77405db7-3494-4add-cfc7-8415e52a0412"},"outputs":[{"name":"stdout","output_type":"stream","text":["Sat Jan 21 08:34:23 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 515.48.07    Driver Version: 515.48.07    CUDA Version: 11.7     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  NVIDIA TITAN RTX    On   | 00000000:01:00.0 Off |                  N/A |\n","| 40%   30C    P8     7W / 280W |    177MiB / 24576MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|    0   N/A  N/A      1061      G   /usr/lib/xorg/Xorg                 53MiB |\n","|    0   N/A  N/A   2671131      G   /usr/lib/xorg/Xorg                 97MiB |\n","|    0   N/A  N/A   2671256      G   /usr/bin/gnome-shell                9MiB |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"markdown","metadata":{"id":"HvkB9p5zYf8E"},"source":["If you don't have a Nvidia GPU accessible, the above will output something like:\n","\n","```\n","NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n","```\n","\n","In that case, go back up and follow the install steps.\n","\n","If you do have a GPU, the line above will output something like:\n","\n","```\n","Wed Jan 19 22:09:08 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 495.46       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   35C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n","```"]},{"cell_type":"markdown","metadata":{"id":"UvibZ6e0YcDk"},"source":["\n","\n","### 2. Getting PyTorch to run on the GPU\n","\n","Once you've got a GPU ready to access, the next step is getting PyTorch to use for storing data (tensors) and computing on data (performing operations on tensors).\n","\n","To do so, you can use the [`torch.cuda`](https://pytorch.org/docs/stable/cuda.html) package.\n","\n","Rather than talk about it, let's try it out.\n","\n","You can test if PyTorch has access to a GPU using [`torch.cuda.is_available()`](https://pytorch.org/docs/stable/generated/torch.cuda.is_available.html#torch.cuda.is_available).\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OweDLgwjEvZ2","outputId":"3a278a24-3ec3-4b1f-8f96-298086fa6ea6"},"outputs":[{"data":{"text/plain":["True"]},"execution_count":71,"metadata":{},"output_type":"execute_result"}],"source":["# Check for GPU\n","import torch\n","torch.cuda.is_available()"]},{"cell_type":"markdown","metadata":{"id":"jedZcx2PZFpL"},"source":["If the above outputs `True`, PyTorch can see and use the GPU, if it outputs `False`, it can't see the GPU and in that case, you'll have to go back through the installation steps.\n","\n","Now, let's say you wanted to setup your code so it ran on CPU *or* the GPU if it was available.\n","\n","That way, if you or someone decides to run your code, it'll work regardless of the computing device they're using.\n","\n","Let's create a `device` variable to store what kind of device is available."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"j92HBCKB7rYa","outputId":"8cca1643-645c-4b67-f1f5-37066f6b9549"},"outputs":[{"data":{"text/plain":["'cuda'"]},"execution_count":72,"metadata":{},"output_type":"execute_result"}],"source":["# Set device type\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","device"]},{"cell_type":"markdown","metadata":{"id":"FjFyPP2WaCch"},"source":["If the above output `\"cuda\"` it means we can set all of our PyTorch code to use the available CUDA device (a GPU) and if it output `\"cpu\"`, our PyTorch code will stick with the CPU.\n","\n","> **Note:** In PyTorch, it's best practice to write [**device agnostic code**](https://pytorch.org/docs/master/notes/cuda.html#device-agnostic-code). This means code that'll run on CPU (always available) or GPU (if available).\n","\n","If you want to do faster computing you can use a GPU but if you want to do *much* faster computing, you can use multiple GPUs.\n","\n","You can count the number of GPUs PyTorch has access to using [`torch.cuda.device_count()`](https://pytorch.org/docs/stable/generated/torch.cuda.device_count.html#torch.cuda.device_count)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MArsn0DFTGfG","outputId":"de717df5-bb67-4900-805e-a6f00ad0b409"},"outputs":[{"data":{"text/plain":["1"]},"execution_count":73,"metadata":{},"output_type":"execute_result"}],"source":["# Count number of devices\n","torch.cuda.device_count()"]},{"cell_type":"markdown","metadata":{"id":"xVNf1hiqa-gO"},"source":["Knowing the number of GPUs PyTorch has access to is helpful incase you wanted to run a specific process on one GPU and another process on another (PyTorch also has features to let you run a process across *all* GPUs)."]},{"cell_type":"markdown","metadata":{"id":"XqQLcuj68OA-"},"source":["### 3. Putting tensors (and models) on the GPU\n","\n","You can put tensors (and models, we'll see this later) on a specific device by calling [`to(device)`](https://pytorch.org/docs/stable/generated/torch.Tensor.to.html) on them. Where `device` is the target device you'd like the tensor (or model) to go to.\n","\n","Why do this?\n","\n","GPUs offer far faster numerical computing than CPUs do and if a GPU isn't available, because of our **device agnostic code** (see above), it'll run on the CPU.\n","\n","> **Note:** Putting a tensor on GPU using `to(device)` (e.g. `some_tensor.to(device)`) returns a copy of that tensor, e.g. the same tensor will be on CPU and GPU. To overwrite tensors, reassign them:\n",">\n","> `some_tensor = some_tensor.to(device)`\n","\n","Let's try creating a tensor and putting it on the GPU (if it's available)."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FhI3srFXEHfP","outputId":"2f4f6435-fdc4-4e99-e87c-9421c2100f36"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([1, 2, 3]) cpu\n"]},{"data":{"text/plain":["tensor([1, 2, 3], device='cuda:0')"]},"execution_count":74,"metadata":{},"output_type":"execute_result"}],"source":["# Create tensor (default on CPU)\n","tensor = torch.tensor([1, 2, 3])\n","\n","# Tensor not on GPU\n","print(tensor, tensor.device)\n","\n","# Move tensor to GPU (if available)\n","tensor_on_gpu = tensor.to(device)\n","tensor_on_gpu"]},{"cell_type":"markdown","metadata":{"id":"DxXeRKO0TGfG"},"source":["If you have a GPU available, the above code will output something like:\n","\n","```\n","tensor([1, 2, 3]) cpu\n","tensor([1, 2, 3], device='cuda:0')\n","```\n","\n","Notice the second tensor has `device='cuda:0'`, this means it's stored on the 0th GPU available (GPUs are 0 indexed, if two GPUs were available, they'd be `'cuda:0'` and `'cuda:1'` respectively, up to `'cuda:n'`).\n","\n"]},{"cell_type":"markdown","metadata":{"id":"4puyUX4Bci5D"},"source":["### 4. Moving tensors back to the CPU\n","\n","What if we wanted to move the tensor back to CPU?\n","\n","For example, you'll want to do this if you want to interact with your tensors with NumPy (NumPy does not leverage the GPU).\n","\n","Let's try using the [`torch.Tensor.numpy()`](https://pytorch.org/docs/stable/generated/torch.Tensor.numpy.html) method on our `tensor_on_gpu`."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":186},"id":"3ChSLJgPTGfG","outputId":"32e92f62-db28-4dc7-ce93-c2ab33229252"},"outputs":[{"ename":"TypeError","evalue":"can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m/home/daniel/code/pytorch/pytorch-course/pytorch-deep-learning/00_pytorch_fundamentals.ipynb Cell 157\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22544954414e2d525458227d/home/daniel/code/pytorch/pytorch-course/pytorch-deep-learning/00_pytorch_fundamentals.ipynb#Y312sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# If tensor is on GPU, can't transform it to NumPy (this will error)\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22544954414e2d525458227d/home/daniel/code/pytorch/pytorch-course/pytorch-deep-learning/00_pytorch_fundamentals.ipynb#Y312sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m tensor_on_gpu\u001b[39m.\u001b[39;49mnumpy()\n","\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."]}],"source":["# If tensor is on GPU, can't transform it to NumPy (this will error)\n","tensor_on_gpu.numpy()"]},{"cell_type":"markdown","metadata":{"id":"LhymtkRDTGfG"},"source":["Instead, to get a tensor back to CPU and usable with NumPy we can use [`Tensor.cpu()`](https://pytorch.org/docs/stable/generated/torch.Tensor.cpu.html).\n","\n","This copies the tensor to CPU memory so it's usable with CPUs."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gN15s-NdTGfG","outputId":"9fffb6f2-c200-4f9c-d987-d9ab5d9cba49"},"outputs":[{"data":{"text/plain":["array([1, 2, 3])"]},"execution_count":76,"metadata":{},"output_type":"execute_result"}],"source":["# Instead, copy the tensor back to cpu\n","tensor_back_on_cpu = tensor_on_gpu.cpu().numpy()\n","tensor_back_on_cpu"]},{"cell_type":"markdown","metadata":{"id":"qyzNH5lrTGfH"},"source":["The above returns a copy of the GPU tensor in CPU memory so the original tensor is still on GPU."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S5u83PCRTGfH","outputId":"4cb931e2-7c8d-49b9-a7de-db3d3c6589b5"},"outputs":[{"data":{"text/plain":["tensor([1, 2, 3], device='cuda:0')"]},"execution_count":77,"metadata":{},"output_type":"execute_result"}],"source":["tensor_on_gpu"]},{"cell_type":"markdown","metadata":{"id":"xlmBpnuPTGfH"},"source":["## Exercises\n","\n","All of the exercises are focused on practicing the code above.\n","\n","You should be able to complete them by referencing each section or by following the resource(s) linked.\n","\n","**Resources:**\n","\n","* [Exercise template notebook for 00](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/exercises/00_pytorch_fundamentals_exercises.ipynb).\n","* [Example solutions notebook for 00](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/solutions/00_pytorch_fundamentals_exercise_solutions.ipynb) (try the exercises *before* looking at this).\n","\n","1. Documentation reading - A big part of deep learning (and learning to code in general) is getting familiar with the documentation of a certain framework you're using. We'll be using the PyTorch documentation a lot throughout the rest of this course. So I'd recommend spending 10-minutes reading the following (it's okay if you don't get some things for now, the focus is not yet full understanding, it's awareness). See the documentation on [`torch.Tensor`](https://pytorch.org/docs/stable/tensors.html#torch-tensor) and for [`torch.cuda`](https://pytorch.org/docs/master/notes/cuda.html#cuda-semantics).\n","2. Create a random tensor with shape `(7, 7)`.\n","3. Perform a matrix multiplication on the tensor from 2 with another random tensor with shape `(1, 7)` (hint: you may have to transpose the second tensor).\n","4. Set the random seed to `0` and do exercises 2 & 3 over again.\n","5. Speaking of random seeds, we saw how to set it with `torch.manual_seed()` but is there a GPU equivalent? (hint: you'll need to look into the documentation for `torch.cuda` for this one). If there is, set the GPU random seed to `1234`.\n","6. Create two random tensors of shape `(2, 3)` and send them both to the GPU (you'll need access to a GPU for this). Set `torch.manual_seed(1234)` when creating the tensors (this doesn't have to be the GPU random seed).\n","7. Perform a matrix multiplication on the tensors you created in 6 (again, you may have to adjust the shapes of one of the tensors).\n","8. Find the maximum and minimum values of the output of 7.\n","9. Find the maximum and minimum index values of the output of 7.\n","10. Make a random tensor with shape `(1, 1, 1, 10)` and then create a new tensor with all the `1` dimensions removed to be left with a tensor of shape `(10)`. Set the seed to `7` when you create it and print out the first tensor and it's shape as well as the second tensor and it's shape."]},{"cell_type":"markdown","metadata":{"id":"c5jGUMdNbou3"},"source":["## Extra-curriculum\n","\n","* Spend 1-hour going through the [PyTorch basics tutorial](https://pytorch.org/tutorials/beginner/basics/intro.html) (I'd recommend the [Quickstart](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html) and [Tensors](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html) sections).\n","* To learn more on how a tensor can represent data, see this video: [What's a tensor?](https://youtu.be/f5liqUk0ZTw)"]},{"cell_type":"markdown","metadata":{"id":"u6RjOsOokieC"},"source":["# PyTorch 2.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CBdmqW9ZkieD","outputId":"87dc4ace-6a1e-49f8-8cb2-4b3a941ec69a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Notebook last updated: 2023-04-14 15:24:37.007274\n"]}],"source":["import datetime\n","print(f\"Notebook last updated: {datetime.datetime.now()}\")"]},{"cell_type":"markdown","metadata":{"id":"jDLhX1RekieE"},"source":["\n","## 30-second intro\n","\n","PyTorch 2.0 is out!\n","\n","With the main improvement being speed.\n","\n","This comes via a single backwards-compatible line.\n","\n","```python\n","torch.compile()\n","```\n","\n","In other words, after you create your model, you can pass it to `torch.compile()` and in turn expect speedups in training and inference on newer GPUs (e.g. NVIDIA RTX 40 series, A100, H100, the newer the GPU the more noticeable the speedups).\n","\n","> **Note:** There are plenty more upgrades within PyTorch 2.0 than just `torch.compile()` but since it's the main one, it's what we're going to focus on. For a full list of changes, see the [PyTorch 2.0 release notes](https://pytorch.org/blog/pytorch-2.0-release/).\n","\n","### Will my old PyTorch code still work?\n","\n","Yes, PyTorch 2.0 is backwards-compatible. The changes are mostly additive (new features).\n","\n","That means if you already know PyTorch, such as via the [learnpytorch.io](https://learnpytorch.io) course, you can start using PyTorch 2.0 straight away. And your old PyTorch code will still work."]},{"cell_type":"markdown","metadata":{"id":"C7_41T1akieE"},"source":["## Quick code examples"]},{"cell_type":"markdown","metadata":{"id":"GStIZd44kieE"},"source":["### Before PyTorch 2.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GYi9S7JpkieE"},"outputs":[],"source":["import torch\n","import torchvision\n","\n","model = torchvision.models.resnet50() # note: this could be any model\n","\n","### Train model ###\n","\n","### Test model ###"]},{"cell_type":"markdown","metadata":{"id":"XhdMzyMTkieF"},"source":["### After PyTorch 2.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qpY3F0RJkieF"},"outputs":[],"source":["import torch\n","import torchvision\n","\n","model = torchvision.models.resnet50() # note: this could be any model\n","compiled_model = torch.compile(model) # <- magic happens!\n","\n","### Train model ### <- faster!\n","\n","### Test model ### <- faster!"]},{"cell_type":"markdown","metadata":{"id":"V32YV2i-kieG"},"source":["## 3-minute overview\n","\n","> **Note:** The following is adapted from [*A Quick Introduction to PyTorch 2.0*](https://www.mrdbourke.com/pytorch-2/) on mrdbourke.com, there's also an accompanying [video explainer on YouTube](https://youtu.be/WqLKfta5Ijw).\n","\n","What's happening behind the scenes of `torch.compile()`?\n","\n","`torch.compile()` is designed to \"just work\" but there are a few technologies behind it:\n","* TorchDynamo\n","* AOTAutograd\n","* PrimTorch\n","* TorchInductor\n","\n","The [PyTorch 2.0 getting started notes](https://pytorch.org/get-started/pytorch-2.0/) explain these in more detail but from a high level the two main improvements `torch.compile()` offers are:\n","* Fusion (or operator fusion)\n","* Graph capture (or graph tracing)"]},{"cell_type":"markdown","metadata":{"id":"XZWXmH4-kieG"},"source":["### Fusion\n","\n","Fusion, also known as **operator fusion** is one of the best ways to make deep learning models go brrrrrr (brrrrrr is the sound your GPUs fan make when your models are training).\n","\n","Operator fusion condenses (like Dragon Ball Z) many operations into one (or many to less).\n","\n","Why?\n","\n","Modern GPUs have so much compute power they are often not compute limited, as in, the main bottleneck to training models is how fast can you get data from your CPU to your GPU.\n","This is known as bandwidth or memory bandwidth.\n","\n","You want to reduce your bandwidth costs as much as possible.\n","\n","And feed the data hungry GPUs with as much data as possible.\n","\n","<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/extras-memory-bandwidth-output-small.gif\" alt=\"example of memory bandwidth costs transferring data on and off the GPU\" width=950/>\n","\n","So instead of performing an operation on a piece of data and then saving the result to memory (increased bandwidth costs), you chain together as many operations as possible via fusion.\n","\n","A rough analogy would be using a blender to make a smoothie.\n","\n","Most blenders are good at blending things (like GPUs are good at performing matrix multiplications).\n","\n","Using a blender **without operator fusion** would be like adding each ingredient one by one and blending each time a new ingredient is added.\n","Not only is this insane, it increases your bandwidth cost.\n","\n","The actual blending is fast each time (like GPU computations generally are) but you lose a bunch of time adding each ingredient one by one.\n","\n","Using a blender **with operator fusion** is akin to using a blender by adding all the ingredients at the start (operator fusion) and then performing the blend once.\n","\n","You lose a little time adding at the start but you gain all of the lost memory bandwidth time back."]},{"cell_type":"markdown","metadata":{"id":"eYl2667LkieG"},"source":["### Graph capture\n","\n","Graph capture I’m less confident explaining.\n","\n","But the way I think about is that graph capture or graph tracing is:\n","\n","* Going through a series of operations that need to happen, such as the operations in a neural network.\n","* And capturing or tracing what needs to happen ahead of time.\n","\n","Computing **without graph capture** is like going to a new area and following GPS directions turn by turn.\n","\n","As a good human driver, you can follow the turns quite easily but you still have to think about each turn you take.\n","\n","This is the equivalent to PyTorch having to look up what each operation does as it does it.\n","\n","As in, to perform an addition, it has to look up what an addition does before it can perform it.\n","\n","It does this quickly but there’s still non-zero overhead.\n","\n","<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/extras-graph-capture.gif\" alt=\"Graph capture\" width=\"950\"/>\n","\n","*Example of graph capture, mapping out the steps in a neural network and then capturing every operation that needs to happen ahead of time.*\n","\n","Computing **with graph capture** is like driving through your own neighbourhood.\n","\n","You barely think about what turns to make.\n","\n","Sometimes you get out of the car and realise you can’t remember the last 5 minutes of the drive.\n","\n","Your brain was functioning on autopilot, minimal overhead.\n","\n","However, it took you some time upfront to remember how to drive to your house.\n","\n","This is a caveat of graph capture, it takes a little time upfront to memorize the operations that need to happen but subsequent computations should be faster.\n","\n","Of course, this is a quick high-level overview of what’s happening behind the scenes of torch.compile()but it's how I understand it.\n","\n","For more on fusion and graph tracing, I’d recommend Horace He’s [*Making Deep Learning Go Brrrr From First Principles*](https://horace.io/brrr_intro.html) blog post."]},{"cell_type":"markdown","metadata":{"id":"_ctepRUHkieG"},"source":["## Things to note\n","\n","Since PyTorch 2.0 was just released, there are a few limitations with some of the features.\n","\n","One of the main ones being with exporting models.\n","\n","<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/extras-pytorch-2-limitations.png\" alt=\"PyTorch 2 limitations\" width=650/>\n","\n","*There are a few caveats when using the PyTorch 2.0 features, such as not being about to export to mobile devices when using the `torch.compile()` default options. However, there are work arounds to this and improved exporting is on the PyTorch 2.x roadmap. *Source:* [PyTorch 2.0 announcement post](https://pytorch.org/get-started/pytorch-2.0/).*\n","\n","However, these will likely be fixed in future releases.\n","\n","Another main limitation is that because the features of PyTorch 2.0 are designed for newer hardware, old GPUs and desktop-class GPUs (e.g. NVIDIA RTX 30 series) will likely see less speedups than newer hardware."]},{"cell_type":"markdown","metadata":{"id":"1PY7AVhHkieG"},"source":["## What we're going to cover\n","\n","Since many of the upgrades in PyTorch 2.0 are speed focused and happen behind the scenes (e.g. PyTorch takes care of them for you), in this notebook we're going to run a compartive speed test.\n","\n","Namely we'll make two of the same models, one using the default PyTorch setup and the other using the new `torch.compile()` setup and we'll train them on the same dataset.\n","\n","1. Model 1 - no `torch.compile()`.\n","2. Model 2 - `torch.compile()`.\n","\n","We'll then compare the training/testing times of both models for single run and multiple runs.\n","\n","| **Experiment** | **Model** | **Data** | **Epochs** | **Batch size** | **Image size** | **`torch.compile()`** |  \n","|----- |-----| -----| -----| -----| -----| -----|\n","| 1 (single run) | [ResNet50](https://pytorch.org/vision/master/models/generated/torchvision.models.resnet50.html) | [CIFAR10](https://pytorch.org/vision/stable/generated/torchvision.datasets.CIFAR10.html#torchvision.datasets.CIFAR10) | 5 | 128 | 224 | No |\n","| 2 (single run) | ResNet50 | CIFAR10 | 5 | 128 | 224 | Yes |\n","| 3 (multi-run) | ResNet50 | CIFAR10 | 3x5 | 128 | 224 | No |\n","| 4 (multi-run) | ResNet50 | CIFAR10 | 3x5 | 128 | 224 | Yes |\n","\n","We've chosen ResNet50 and CIFAR10 here for ease of access or use, however, you could substitute any model/dataset you like.\n","\n","The biggest speedups I've noticed with PyTorch 2.0 are when the GPU computes on as much data as possible (e.g. larger batch size/image size/data size/model size).\n","\n","> **Note:** Depending on the size of your GPU, you may have to lower the batch size (or image size) to fit the model on your GPU. For example, if you're using a GPU with 8GB of memory or less, you may have to lower the batch size to 64 or 32."]},{"cell_type":"markdown","metadata":{"id":"dnjc1x0SkieG"},"source":["## 0. Getting setup\n","\n","To get setup we'll first check for PyTorch 2.x+ and install it if it's not available.\n","\n","You can see how to install PyTorch 2.x on your own system in the [PyTorch documentation](https://pytorch.org/get-started/locally/).\n","\n","> **Note:** If you're running on Google Colab, you'll need to setup a GPU: runtime -> change runtime type -> hardware accelerator. The best speedups are on newer NVIDIA/AMD GPUs (this is because PyTorch 2.0 leverages newer GPU hardware) such as the NVIDIA A100 and above. This tutorial focuses on NVIDIA GPUs."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VseRfzQzkieG","outputId":"db29a32a-4896-4db2-fed1-6200e1b3a5dc"},"outputs":[{"name":"stdout","output_type":"stream","text":["[INFO] Current PyTorch version: 2.0.0+cu118 (should be 2.x+)\n","[INFO] PyTorch 2.x installed, you'll be able to use the new features.\n"]}],"source":["import torch\n","\n","# Check PyTorch version\n","pt_version = torch.__version__\n","print(f\"[INFO] Current PyTorch version: {pt_version} (should be 2.x+)\")\n","\n","# Install PyTorch 2.0 if necessary\n","if pt_version.split(\".\")[0] == \"1\": # Check if PyTorch version begins with 1\n","    !pip3 install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n","    print(\"[INFO] PyTorch 2.x installed, if you're on Google Colab, you may need to restart your runtime.\\\n","          Though as of April 2023, Google Colab comes with PyTorch 2.0 pre-installed.\")\n","    import torch\n","    pt_version = torch.__version__\n","    print(f\"[INFO] Current PyTorch version: {pt_version} (should be 2.x+)\")\n","else:\n","    print(\"[INFO] PyTorch 2.x installed, you'll be able to use the new features.\")"]},{"cell_type":"markdown","metadata":{"id":"mlshmYeTkieH"},"source":["Wonderful!\n","\n","Now PyTorch 2.x is installed, let's try out the new features!"]},{"cell_type":"markdown","metadata":{"id":"f6OuMXFhkief"},"source":["## 1. Get GPU info\n","\n","Time to get GPU info.\n","\n","Why?\n","\n","Many of the speedups PyTorch 2.0 offers are best experienced on newer NVIDIA GPUs (we're focused on NVIDIA GPUs for now).\n","\n","This is because PyTorch 2.0 takes advantage of the new hardware on newer GPUs.\n","\n","How do you tell what's a newer GPU?\n","\n","Generally, a *newer* GPU will have a compute capability score of 8.0 or higher.\n","\n","You can see a list of [NVIDIA GPU compute capability scores](https://developer.nvidia.com/cuda-gpus) on NVIDIA's developer page.\n","\n","Here are some scores of NVIDIA GPUs released in 2020 or later:\n","\n","| **NVIDIA GPU** | **Compute capability score** | **GPU Type** | **Release year** | **Architecture** |\n","|----- |-----| -----| -----| -----|\n","| RTX 4090 | 8.9 | Desktop-class | 2022 | [Ada Lovelace](https://www.nvidia.com/en-au/geforce/ada-lovelace-architecture/) |\n","| RTX 4080 | 8.9 | Desktop-class | 2022 | Ada Lovelace |\n","| RTX 4070 Ti | 8.9 | Desktop-class | 2022 | Ada Lovelace |\n","| RTX 3090 | 8.6 | Desktop-class | 2020 | [Ampere](https://en.wikipedia.org/wiki/Ampere_(microarchitecture)) |\n","| RTX 3080 | 8.6 | Desktop-class | 2020 | Ampere|\n","| RTX 3070 | 8.6 | Desktop-class | 2020 | Ampere |  \n","| RTX 3060 Ti | 8.6 | Desktop-class | 2020 | Ampere |\n","| H100 | 9.0 | Datacenter-class | 2022 | [Hopper](https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/) |\n","| A100 | 8.0 | Datacenter-class | 2020 | Ampere |\n","| A10 | 8.6 | Datacenter-class | 2021 | Ampere |\n","\n","GPUs with a compute capability score of 8.0 or above are likely to see the biggest speedups.\n","\n","And GPUs which are datacenter-class (e.g. A100, A10, H100) are likely to see more significant speedups than desktop-class GPUs (e.g. RTX 3090, RTX 3080, RTX 3070, RTX 3060 Ti).\n","\n","We can check the compute capbility score of our GPU using [`torch.cuda.get_device_capability()`](https://pytorch.org/docs/stable/generated/torch.cuda.get_device_capability.html).\n","\n","This will output a tuple of `(major, minor)` compute capability scores, for example, `(8, 0)` for the A100.\n","\n","We'll also get some other details about our GPU such as the name and other info using [`nvidia-smi`](https://developer.nvidia.com/nvidia-system-management-interface).  \n","\n","> **Resource:** For an in-depth comparison of many different NVIDIA GPUs and their speeds, costs and tradeoffs, I'd recommend reading Tim Dettmers' [*Which GPU for deep learning?*](https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/) blog post."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VSoANsw5kieg","outputId":"91b96c7c-1e3c-4347-ad9d-688162956fff"},"outputs":[{"name":"stdout","output_type":"stream","text":["GPU name: NVIDIA_TITAN_RTX\n","GPU capability score: (7, 5)\n","GPU score lower than (8, 0), PyTorch 2.x speedup features will be limited (PyTorch 2.x speedups happen most on newer GPUs).\n","GPU information:\n","Fri Apr 14 15:24:38 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.89.02    Driver Version: 525.89.02    CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  NVIDIA TITAN RTX    Off  | 00000000:01:00.0 Off |                  N/A |\n","| 40%   50C    P8     9W / 280W |    260MiB / 24576MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|    0   N/A  N/A      1020      G   /usr/lib/xorg/Xorg                 53MiB |\n","|    0   N/A  N/A   1415245      G   /usr/lib/xorg/Xorg                162MiB |\n","|    0   N/A  N/A   1415374      G   /usr/bin/gnome-shell                8MiB |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["# Make sure we're using a NVIDIA GPU\n","if torch.cuda.is_available():\n","  gpu_info = !nvidia-smi\n","  gpu_info = '\\n'.join(gpu_info)\n","  if gpu_info.find(\"failed\") >= 0:\n","    print(\"Not connected to a GPU, to leverage the best of PyTorch 2.0, you should connect to a GPU.\")\n","\n","  # Get GPU name\n","  gpu_name = !nvidia-smi --query-gpu=gpu_name --format=csv\n","  gpu_name = gpu_name[1]\n","  GPU_NAME = gpu_name.replace(\" \", \"_\") # remove underscores for easier saving\n","  print(f'GPU name: {GPU_NAME}')\n","\n","  # Get GPU capability score\n","  GPU_SCORE = torch.cuda.get_device_capability()\n","  print(f\"GPU capability score: {GPU_SCORE}\")\n","  if GPU_SCORE >= (8, 0):\n","    print(f\"GPU score higher than or equal to (8, 0), PyTorch 2.x speedup features available.\")\n","  else:\n","    print(f\"GPU score lower than (8, 0), PyTorch 2.x speedup features will be limited (PyTorch 2.x speedups happen most on newer GPUs).\")\n","\n","  # Print GPU info\n","  print(f\"GPU information:\\n{gpu_info}\")\n","\n","else:\n","  print(\"PyTorch couldn't find a GPU, to leverage the best of PyTorch 2.0, you should connect to a GPU.\")"]},{"cell_type":"markdown","metadata":{"id":"Tj1Isvgnkieh"},"source":["### 1.1 Globally set devices\n","\n","One of my favourite new features in PyTorch 2.x is being able to set the [default device type](https://pytorch.org/tutorials/recipes/recipes/changing_default_device.html ) via:\n","* Context manager\n","* Globally\n","\n","Previously, you could only set the default device type via:\n","* `tensor.to(device)`\n","\n","Let's see these two new device settings in action."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sPwPOriqkien","outputId":"eadfc574-bcf8-41f3-e972-3dd8263dceed"},"outputs":[{"name":"stdout","output_type":"stream","text":["Layer weights are on device: cuda:0\n","Layer creating data on device: cuda:0\n"]}],"source":["import torch\n","\n","# Set the device\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# Set the device with context manager (requires PyTorch 2.x+)\n","with torch.device(device):\n","    # All tensors created in this block will be on device\n","    layer = torch.nn.Linear(20, 30)\n","    print(f\"Layer weights are on device: {layer.weight.device}\")\n","    print(f\"Layer creating data on device: {layer(torch.randn(128, 20)).device}\")"]},{"cell_type":"markdown","metadata":{"id":"SyEa2uSZkieo"},"source":["Now how about setting the global device?\n","\n","This will mean that any tensors created without an explicit device will be created on the device you set by default."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0ZIPYXnAkiep","outputId":"476ea358-ac0b-472a-8558-78938876aecd"},"outputs":[{"name":"stdout","output_type":"stream","text":["Layer weights are on device: cuda:0\n","Layer creating data on device: cuda:0\n"]}],"source":["import torch\n","\n","# Set the device\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# Set the device globally\n","torch.set_default_device(device)\n","\n","# All tensors created will be on the global device by default\n","layer = torch.nn.Linear(20, 30)\n","print(f\"Layer weights are on device: {layer.weight.device}\")\n","print(f\"Layer creating data on device: {layer(torch.randn(128, 20)).device}\")"]},{"cell_type":"markdown","metadata":{"id":"Ji4OhlK7kieq"},"source":["And now back to CPU."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6N0MXaGRkiew","outputId":"ec84b47f-4cc3-4fd6-ae63-1a1982124cc6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Layer weights are on device: cpu\n","Layer creating data on device: cpu\n"]}],"source":["import torch\n","\n","# Set the device globally\n","torch.set_default_device(\"cpu\")\n","\n","# All tensors created will be on \"cpu\"\n","layer = torch.nn.Linear(20, 30)\n","print(f\"Layer weights are on device: {layer.weight.device}\")\n","print(f\"Layer creating data on device: {layer(torch.randn(128, 20)).device}\")"]},{"cell_type":"markdown","metadata":{"id":"nBAar5tgkiex"},"source":["## 2. Setting up the experiments\n","\n","Okay, time to measure speed!\n","\n","To keep things simple, as we discussed we're going to run a series of four experiments, all with:\n","\n","* **Model:** ResNet50 (from [TorchVision](https://pytorch.org/vision/main/models/generated/torchvision.models.resnet50.html))\n","* **Data:** CIFAR10 (from [TorchVision](https://pytorch.org/vision/main/generated/torchvision.datasets.CIFAR10.html))\n","* **Epochs:** 5 (single run) and 3x5 (multiple runs)\n","* **Batch size:** 128\n","* **Image size:** 224\n","\n","Each experiment will be run with and without `torch.compile()`.\n","\n","Why the single and multiple runs?\n","\n","Because we can measure speedups via a single run, however, we'll also want to run the tests multiple times to get an average (just to make sure the results from a single run weren't a fluke or something went wrong).\n","\n","> **Note:** Depending on the amount of memory your GPU has, you may have to lower the batch size or the image size. This tutorial is focused on using an NVIDIA A100 GPU with 40GB of memory, the amount of memory on this GPU means it can handle a larger batch size. As of April 2023, NVIDIA A100 GPUs are available via Google Colab Pro.\n","\n","Let's start by importing `torch` and `torchvision` and setting the target device."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GUXk8yZbkiex","outputId":"4e4236fb-c268-4a5e-9729-976d0296ac67"},"outputs":[{"name":"stdout","output_type":"stream","text":["PyTorch version: 2.0.0+cu118\n","TorchVision version: 0.15.1+cu118\n","Using device: cuda\n"]}],"source":["import torch\n","import torchvision\n","\n","print(f\"PyTorch version: {torch.__version__}\")\n","print(f\"TorchVision version: {torchvision.__version__}\")\n","\n","# Set the target device\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","print(f\"Using device: {device}\")"]},{"cell_type":"markdown","metadata":{"id":"nsgPxHm4kiex"},"source":["### 2.1 Create model and transforms\n","\n","Let's now create our model and transforms.\n","\n","We'll use the same setup to create the model and transforms we covered in [06. PyTorch Transfer Learning section 2.2](https://www.learnpytorch.io/06_pytorch_transfer_learning/).\n","\n","In essence, we'll create the model and transforms for the model using the [`torchvision.models`](https://pytorch.org/vision/stable/models.html) API.\n","\n","We can get the weights and transforms for ResNet50 using the following:\n","* `model_weights = torchvision.models.ResNet50_Weights.IMAGENET1K_V2` (this requires `torchvision` 0.14 or later).\n","* `transforms = model_weights.transforms()` (once we have the weights, we can get the appropriate transforms for the model).\n","\n","> **Note:** We'll count the model's parameters to see how big of a model we're working with. The more parameters in a model, the larger GPU memory you'll need to train it. However, the more parameters your model has, the more GPU memory it uses, the larger *relative* speedup you'll often see. Meaning, a larger model may take longer to train in total, however, on a relative basis because it's using more GPU power, it could be faster than a smaller model. As in, a model with 10M parameters may take only 5x longer to train than a model with 1M parameters (10x the size but only 5x the training time)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cIxtrXX1kiey","outputId":"73970462-1d7b-4f12-8900-7db3955a91bf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total parameters of model: 25557032 (the more parameters, the more GPU memory the model will use, the more *relative* of a speedup you'll get)\n","Model transforms:\n","ImageClassification(\n","    crop_size=[224]\n","    resize_size=[232]\n","    mean=[0.485, 0.456, 0.406]\n","    std=[0.229, 0.224, 0.225]\n","    interpolation=InterpolationMode.BILINEAR\n",")\n"]}],"source":["# Create model weights and transforms\n","model_weights = torchvision.models.ResNet50_Weights.IMAGENET1K_V2 # <- use the latest weights (could also use .DEFAULT)\n","transforms = model_weights.transforms()\n","\n","# Setup model\n","model = torchvision.models.resnet50(weights=model_weights)\n","\n","# Count the number of parameters in the model\n","total_params = sum(\n","    param.numel() for param in model.parameters() # <- all params\n","\t# param.numel() for param in model.parameters() if param.requires_grad # <- only trainable params\n",")\n","\n","print(f\"Total parameters of model: {total_params} (the more parameters, the more GPU memory the model will use, the more *relative* of a speedup you'll get)\")\n","print(f\"Model transforms:\\n{transforms}\")"]},{"cell_type":"markdown","metadata":{"id":"v_x_f3F7kiey"},"source":["Now let's turn the above code into a function so we can replicate it later, we'll also adjust the last layer's (`model.fc`) output features to match the number of classes in CIFAR10 (10)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PwC7TrBSkiey"},"outputs":[],"source":["def create_model(num_classes=10):\n","  \"\"\"\n","  Creates a ResNet50 model with the latest weights and transforms via torchvision.\n","  \"\"\"\n","  model_weights = torchvision.models.ResNet50_Weights.IMAGENET1K_V2\n","  transforms = model_weights.transforms()\n","  model = torchvision.models.resnet50(weights=model_weights)\n","\n","  # Adjust the number of output features in model to match the number of classes in the dataset\n","  model.fc = torch.nn.Linear(in_features=2048,\n","                             out_features=num_classes)\n","  return model, transforms\n","\n","model, transforms = create_model()"]},{"cell_type":"markdown","metadata":{"id":"EvVMgzlSkiey"},"source":["### 2.2 Speedups are most noticeable when a large portion of the GPU is being used\n","\n","Since modern GPUs are so fast at performing operations, you will often notice the majority of *relative* speedups when as much data as possible is on the GPU.\n","\n","This can be achieved by:\n","* **Increasing the batch size** - More samples per batch means more samples on the GPU, for example, using a batch size of 256 instead of 32.\n","* **Increasing data size** - For example, using larger image size, 224x224 instead of 32x32. A larger data size means that more tensor operations will be happening on the GPU.\n","* **Increasing model size** - For example, using a larger model such as ResNet101 instead of ResNet50. A larger model means that more tensor operations will be happening on the GPU.\n","* **Decreasing data transfer** - For example, setting up all your tensors to be on GPU memory, this minizes the amount of data transfer between the CPU and GPU.\n","\n","All of these result in *more* data being on the GPU.\n","\n","<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/extras-speedups-are-biggest-when-more-gpu-is-used.png\" width=950 alt=\"speedups are biggest when more of the GPU is used\"/>\n","\n","You may be thinking, \"but doesn't this mean that the GPU will be slower because it has to do more work?\"\n","\n","This is correct, operations may take longer when using *more* data on the GPU, however, they benefit from [parallelism](https://en.wikipedia.org/wiki/Parallel_computing) (many operations happening at once).\n","\n","This means that although *more* operations are happening, the GPU is performing as many of them as possible simultaneously.\n","\n","So while you may see speedups with smaller datasets, models, batch sizes and data sizes, however, you will tend to see the *biggest relative* speedups with increasing scale.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"K_v2me5Hkie5"},"source":["### 2.3 Checking the memory limits of our GPU\n","\n","To take advantage of speedups at scale, let's check how much memory our GPU has.\n","\n","If your GPU has less memory, you may need to decrease the batch size or image size (less potential for speedups).\n","\n","We can check the memory available on our GPU using [`torch.cuda.mem_get_info()`](https://pytorch.org/docs/stable/generated/torch.cuda.mem_get_info.html#torch.cuda.mem_get_info).\n","\n","This will return a tuple of `(total_free_gpu_memory, total_gpu_memory)`.\n","\n","Where:\n","* `total_free_gpu_memory` is the amount of memory currently *not being used* on the GPU in bytes.\n","* `total_gpu_memory` is the total amount of memory available on the GPU in bytes.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4pkSgzZ4kie5","outputId":"caa367c1-4a0c-4868-db03-6e9f7343eaa5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total free GPU memory: 24.187 GB\n","Total GPU memory: 25.386 GB\n"]}],"source":["# Check available GPU memory and total GPU memory\n","total_free_gpu_memory, total_gpu_memory = torch.cuda.mem_get_info()\n","print(f\"Total free GPU memory: {round(total_free_gpu_memory * 1e-9, 3)} GB\")\n","print(f\"Total GPU memory: {round(total_gpu_memory * 1e-9, 3)} GB\")"]},{"cell_type":"markdown","metadata":{"id":"bCNuiwzPkie6"},"source":["Wonderful!\n","\n","The takeaways here are:\n","1. The higher the memory available on your GPU, **the bigger your batch size can be, the bigger your model can be, the bigger your data samples can be**.\n","2. For speedups, you should always be trying to use **as much of the GPU(s) as possible**.\n","\n","Let's write some code to use a larger batch size if more GPU memory is available.\n","\n","> **Note:** The ideal batch size you use will depend on the specific GPU and dataset and model you're working with. The code below is specifically targeted for the A100 GPU available on Google Colab Pro. However, you may to adjust it for your own GPU. As if you set the batch size too high, you may run into CUDA out of memory errors.\n","\n","If the total memory on the GPU available is **above 16GB**, let's use a batch size of 128 and an image size of 224 (both of these values can be increased on GPUs with more memory).\n","\n","If the total memory on the GPU available is **below 16GB**, let's use a batch size of 32 and an image size of 64 (both of these values can be altered on GPUs with less memory)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HSDGMPn5kie8","outputId":"829c2b01-4182-420b-9a19-5cc595381ec2"},"outputs":[{"name":"stdout","output_type":"stream","text":["GPU memory available is 24.187 GB, using batch size of 128 and image size 224\n"]}],"source":["# Set batch size depending on amount of GPU memory\n","total_free_gpu_memory_gb = round(total_free_gpu_memory * 1e-9, 3)\n","if total_free_gpu_memory_gb >= 16:\n","  BATCH_SIZE = 128 # Note: you could experiment with higher values here if you like.\n","  IMAGE_SIZE = 224\n","  print(f\"GPU memory available is {total_free_gpu_memory_gb} GB, using batch size of {BATCH_SIZE} and image size {IMAGE_SIZE}\")\n","else:\n","  BATCH_SIZE = 32\n","  IMAGE_SIZE = 128\n","  print(f\"GPU memory available is {total_free_gpu_memory_gb} GB, using batch size of {BATCH_SIZE} and image size {IMAGE_SIZE}\")"]},{"cell_type":"markdown","metadata":{"id":"ta6-6eMfkie9"},"source":["Now let's adjust the `transforms` to use the respective `IMAGE_SIZE`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1gOwxUOokie9","outputId":"a3cd9b3a-031e-4b9d-a1a7-7dba8b67ce4b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Updated data transforms:\n","ImageClassification(\n","    crop_size=224\n","    resize_size=224\n","    mean=[0.485, 0.456, 0.406]\n","    std=[0.229, 0.224, 0.225]\n","    interpolation=InterpolationMode.BILINEAR\n",")\n"]}],"source":["transforms.crop_size = IMAGE_SIZE\n","transforms.resize_size = IMAGE_SIZE\n","print(f\"Updated data transforms:\\n{transforms}\")"]},{"cell_type":"markdown","metadata":{"id":"woQYyoMekifD"},"source":["### 2.4  More potential speedups with TF32\n","\n","TF32 stands for TensorFloat-32, a data format which is a combination of 16-bit and 32-bit floating point numbers.\n","\n","You can read more about how it works on [NVIDIA's blog](https://blogs.nvidia.com/blog/2020/05/14/tensorfloat-32-precision-format/).\n","\n","The main thing you should know is that it allows you to **perform faster matrix multiplications** on GPUs with the Ampere architecture and above (a compute capability score of 8.0+).\n","\n","Although it's not specific to PyTorch 2.0, since we're talking about newer GPUs, it's worth mentioning.\n","\n","If you're using a GPU with a compute capability score of 8.0 or above, you can enable TF32 by setting [`torch.backends.cuda.matmul.allow_tf32 = True`](https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices) (this defaults to `False`).\n","\n","Let's write a check that sets it automatically for us based on our GPUs compute capability score.\n","\n","> **Note:** TensorFloat32 is disabled by default (set to `False`) in PyTorch versions 1.12 onwards. This is because it [may cause inconsistent results across different devices](https://dev-discuss.pytorch.org/t/pytorch-and-tensorfloat32/504). Although this issue is not noticed for all use cases, it's worth knowing."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m26-Zi7xkifJ","outputId":"12a20bdf-c7e7-442e-ab16-d2c0f9eee1d2"},"outputs":[{"name":"stdout","output_type":"stream","text":["[INFO] Using GPU with score: (7, 5), TensorFloat32 (TF32) not available, to use it you need a GPU with score >= (8, 0)\n"]}],"source":["if GPU_SCORE >= (8, 0):\n","  print(f\"[INFO] Using GPU with score: {GPU_SCORE}, enabling TensorFloat32 (TF32) computing (faster on new GPUs)\")\n","  torch.backends.cuda.matmul.allow_tf32 = True\n","else:\n","  print(f\"[INFO] Using GPU with score: {GPU_SCORE}, TensorFloat32 (TF32) not available, to use it you need a GPU with score >= (8, 0)\")\n","  torch.backends.cuda.matmul.allow_tf32 = False"]},{"cell_type":"markdown","metadata":{"id":"XlbET8rvkifK"},"source":["### 2.5 Preparing datasets\n","\n","Computing setup done!\n","\n","Let's now create our datasets.\n","\n","To keep things simple, we'll use [CIFAR10](https://pytorch.org/vision/main/generated/torchvision.datasets.CIFAR10.html) since it's readily available in `torchvision`.\n","\n","Some info about CIFAR10 the [CIFAR10 website](https://www.cs.toronto.edu/~kriz/cifar.html):\n","\n","* CIFAR10 is a dataset of 60,000 32x32 color images in 10 classes, with 6,000 images per class.\n","* There are 50,000 training images and 10,000 test images.\n","* The dataset contains 10 classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck.\n","\n","Although the original dataset consists of 32x32 images, we'll use the `transforms` we created earlier to resize them to 224x224 (larger images provide more information and will take up more memory on the GPU)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bzT8-1MCkifK","outputId":"024d6073-8313-4603-cffe-b0590daa853e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Files already downloaded and verified\n","Files already downloaded and verified\n","[INFO] Train dataset length: 50000\n","[INFO] Test dataset length: 10000\n"]}],"source":["# Create train and test datasets\n","train_dataset = torchvision.datasets.CIFAR10(root='.',\n","                                             train=True,\n","                                             download=True,\n","                                             transform=transforms)\n","\n","test_dataset = torchvision.datasets.CIFAR10(root='.',\n","                                            train=False, # want the test split\n","                                            download=True,\n","                                            transform=transforms)\n","\n","# Get the lengths of the datasets\n","train_len = len(train_dataset)\n","test_len = len(test_dataset)\n","\n","print(f\"[INFO] Train dataset length: {train_len}\")\n","print(f\"[INFO] Test dataset length: {test_len}\")"]},{"cell_type":"markdown","metadata":{"id":"dPEvoWPckifR"},"source":["### 2.6 Create DataLoaders\n","\n","Generally GPUs aren't the bottleneck of machine learning code.\n","\n","Data loading is the main bottleneck.\n","\n","As in, the transfer speed from CPU to GPU.\n","\n","As we're discussed before you want to get your data to the GPU as fast as possible.\n","\n","Let's create our `DataLoaders` using `torch.utils.data.DataLoader`.\n","\n","We'll set their `batch_size` to the `BATCH_SIZE` we created earlier.\n","\n","And the `num_workers` parameter to be the number of CPU cores we have available with `os.cpu_count()`.\n","\n","> **Note:** You may want to experiment with different values for `num_workers` to see what works best for your specific GPU and CPU setup. In my experience, more is better but some people have found this [generally caps out](https://discuss.pytorch.org/t/guidelines-for-assigning-num-workers-to-dataloader/813/3) at `4 * number_of_gpus_you_have`, for example, `num_workers = 4 * 1` for 1 GPU."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ygsiSGVBkifT","outputId":"f5bb4f1d-630b-42f1-8d98-8a0049f65a18"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train dataloader length: 391 batches of size 128\n","Test dataloader length: 79 batches of size 128\n","Using number of workers: 16 (generally more workers means faster dataloading from CPU to GPU)\n"]}],"source":["from torch.utils.data import DataLoader\n","\n","# Create DataLoaders\n","import os\n","NUM_WORKERS = os.cpu_count() # <- use all available CPU cores (this number can be tweaked through experimentation but generally more workers means faster dataloading from CPU to GPU)\n","\n","train_dataloader = DataLoader(dataset=train_dataset,\n","                              batch_size=BATCH_SIZE,\n","                              shuffle=True,\n","                              num_workers=NUM_WORKERS)\n","\n","test_dataloader = DataLoader(dataset=test_dataset,\n","                              batch_size=BATCH_SIZE,\n","                              shuffle=False,\n","                              num_workers=NUM_WORKERS)\n","\n","# Print details\n","print(f\"Train dataloader length: {len(train_dataloader)} batches of size {BATCH_SIZE}\")\n","print(f\"Test dataloader length: {len(test_dataloader)} batches of size {BATCH_SIZE}\")\n","print(f\"Using number of workers: {NUM_WORKERS} (generally more workers means faster dataloading from CPU to GPU)\")"]},{"cell_type":"markdown","metadata":{"id":"-L3QmhERkifV"},"source":["### 2.7 Create training and testing loops\n","\n","Dataloaders ready!\n","\n","Let's now create some training and testing loops.\n","\n","These will be the same training and testing loops we created in [05. PyTorch Going Modular](https://www.learnpytorch.io/05_pytorch_going_modular/) with some slight modifications.\n","\n","Since we're focused on measuring speed, we're going to add a timing component to each loop to measure how long each takes to complete.\n","\n","We'll do this by measuring the start and end time of each training and testing epoch with Python's [`time.time()`](https://docs.python.org/3/library/time.html#time.time) and tracking it in a dictionary.\n","\n","> **Note:** One thing I found when experimenting with PyTorch 2.0 is that [`torch.inference_mode()`](https://pytorch.org/docs/stable/generated/torch.inference_mode.html) produced errors in the testing loop. So I've changed it to be [`torch.no_grad()`](https://pytorch.org/docs/stable/generated/torch.no_grad.html) which offers similar functionality but is an older method than `torch.inference_mode()`. If you find that `torch.inference_mode()` works for you, please [let me know on GitHub](https://github.com/mrdbourke/pytorch-deep-learning/discussions) and I'll update this notebook."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YpD3XCsEkifc"},"outputs":[],"source":["import time\n","from tqdm.auto import tqdm\n","from typing import Dict, List, Tuple\n","\n","def train_step(epoch: int,\n","               model: torch.nn.Module,\n","               dataloader: torch.utils.data.DataLoader,\n","               loss_fn: torch.nn.Module,\n","               optimizer: torch.optim.Optimizer,\n","               device: torch.device,\n","               disable_progress_bar: bool = False) -> Tuple[float, float]:\n","  \"\"\"Trains a PyTorch model for a single epoch.\n","\n","  Turns a target PyTorch model to training mode and then\n","  runs through all of the required training steps (forward\n","  pass, loss calculation, optimizer step).\n","\n","  Args:\n","    model: A PyTorch model to be trained.\n","    dataloader: A DataLoader instance for the model to be trained on.\n","    loss_fn: A PyTorch loss function to minimize.\n","    optimizer: A PyTorch optimizer to help minimize the loss function.\n","    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n","\n","  Returns:\n","    A tuple of training loss and training accuracy metrics.\n","    In the form (train_loss, train_accuracy). For example:\n","\n","    (0.1112, 0.8743)\n","  \"\"\"\n","  # Put model in train mode\n","  model.train()\n","\n","  # Setup train loss and train accuracy values\n","  train_loss, train_acc = 0, 0\n","\n","  # Loop through data loader data batches\n","  progress_bar = tqdm(\n","        enumerate(dataloader),\n","        desc=f\"Training Epoch {epoch}\",\n","        total=len(dataloader),\n","        disable=disable_progress_bar\n","    )\n","\n","  for batch, (X, y) in progress_bar:\n","      # Send data to target device\n","      X, y = X.to(device), y.to(device)\n","\n","      # 1. Forward pass\n","      y_pred = model(X)\n","\n","      # 2. Calculate  and accumulate loss\n","      loss = loss_fn(y_pred, y)\n","      train_loss += loss.item()\n","\n","      # 3. Optimizer zero grad\n","      optimizer.zero_grad()\n","\n","      # 4. Loss backward\n","      loss.backward()\n","\n","      # 5. Optimizer step\n","      optimizer.step()\n","\n","      # Calculate and accumulate accuracy metric across all batches\n","      y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n","      train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n","\n","      # Update progress bar\n","      progress_bar.set_postfix(\n","            {\n","                \"train_loss\": train_loss / (batch + 1),\n","                \"train_acc\": train_acc / (batch + 1),\n","            }\n","        )\n","\n","\n","  # Adjust metrics to get average loss and accuracy per batch\n","  train_loss = train_loss / len(dataloader)\n","  train_acc = train_acc / len(dataloader)\n","  return train_loss, train_acc\n","\n","def test_step(epoch: int,\n","              model: torch.nn.Module,\n","              dataloader: torch.utils.data.DataLoader,\n","              loss_fn: torch.nn.Module,\n","              device: torch.device,\n","              disable_progress_bar: bool = False) -> Tuple[float, float]:\n","  \"\"\"Tests a PyTorch model for a single epoch.\n","\n","  Turns a target PyTorch model to \"eval\" mode and then performs\n","  a forward pass on a testing dataset.\n","\n","  Args:\n","    model: A PyTorch model to be tested.\n","    dataloader: A DataLoader instance for the model to be tested on.\n","    loss_fn: A PyTorch loss function to calculate loss on the test data.\n","    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n","\n","  Returns:\n","    A tuple of testing loss and testing accuracy metrics.\n","    In the form (test_loss, test_accuracy). For example:\n","\n","    (0.0223, 0.8985)\n","  \"\"\"\n","  # Put model in eval mode\n","  model.eval()\n","\n","  # Setup test loss and test accuracy values\n","  test_loss, test_acc = 0, 0\n","\n","  # Loop through data loader data batches\n","  progress_bar = tqdm(\n","      enumerate(dataloader),\n","      desc=f\"Testing Epoch {epoch}\",\n","      total=len(dataloader),\n","      disable=disable_progress_bar\n","  )\n","\n","  # Turn on inference context manager\n","  with torch.no_grad(): # no_grad() required for PyTorch 2.0, I found some errors with `torch.inference_mode()`, please let me know if this is not the case\n","      # Loop through DataLoader batches\n","      for batch, (X, y) in progress_bar:\n","          # Send data to target device\n","          X, y = X.to(device), y.to(device)\n","\n","          # 1. Forward pass\n","          test_pred_logits = model(X)\n","\n","          # 2. Calculate and accumulate loss\n","          loss = loss_fn(test_pred_logits, y)\n","          test_loss += loss.item()\n","\n","          # Calculate and accumulate accuracy\n","          test_pred_labels = test_pred_logits.argmax(dim=1)\n","          test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n","\n","          # Update progress bar\n","          progress_bar.set_postfix(\n","              {\n","                  \"test_loss\": test_loss / (batch + 1),\n","                  \"test_acc\": test_acc / (batch + 1),\n","              }\n","          )\n","\n","  # Adjust metrics to get average loss and accuracy per batch\n","  test_loss = test_loss / len(dataloader)\n","  test_acc = test_acc / len(dataloader)\n","  return test_loss, test_acc\n","\n","def train(model: torch.nn.Module,\n","          train_dataloader: torch.utils.data.DataLoader,\n","          test_dataloader: torch.utils.data.DataLoader,\n","          optimizer: torch.optim.Optimizer,\n","          loss_fn: torch.nn.Module,\n","          epochs: int,\n","          device: torch.device,\n","          disable_progress_bar: bool = False) -> Dict[str, List]:\n","  \"\"\"Trains and tests a PyTorch model.\n","\n","  Passes a target PyTorch models through train_step() and test_step()\n","  functions for a number of epochs, training and testing the model\n","  in the same epoch loop.\n","\n","  Calculates, prints and stores evaluation metrics throughout.\n","\n","  Args:\n","    model: A PyTorch model to be trained and tested.\n","    train_dataloader: A DataLoader instance for the model to be trained on.\n","    test_dataloader: A DataLoader instance for the model to be tested on.\n","    optimizer: A PyTorch optimizer to help minimize the loss function.\n","    loss_fn: A PyTorch loss function to calculate loss on both datasets.\n","    epochs: An integer indicating how many epochs to train for.\n","    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n","\n","  Returns:\n","    A dictionary of training and testing loss as well as training and\n","    testing accuracy metrics. Each metric has a value in a list for\n","    each epoch.\n","    In the form: {train_loss: [...],\n","                  train_acc: [...],\n","                  test_loss: [...],\n","                  test_acc: [...]}\n","    For example if training for epochs=2:\n","                 {train_loss: [2.0616, 1.0537],\n","                  train_acc: [0.3945, 0.3945],\n","                  test_loss: [1.2641, 1.5706],\n","                  test_acc: [0.3400, 0.2973]}\n","  \"\"\"\n","  # Create empty results dictionary\n","  results = {\"train_loss\": [],\n","      \"train_acc\": [],\n","      \"test_loss\": [],\n","      \"test_acc\": [],\n","      \"train_epoch_time\": [],\n","      \"test_epoch_time\": []\n","  }\n","\n","  # Loop through training and testing steps for a number of epochs\n","  for epoch in tqdm(range(epochs), disable=disable_progress_bar):\n","\n","      # Perform training step and time it\n","      train_epoch_start_time = time.time()\n","      train_loss, train_acc = train_step(epoch=epoch,\n","                                        model=model,\n","                                        dataloader=train_dataloader,\n","                                        loss_fn=loss_fn,\n","                                        optimizer=optimizer,\n","                                        device=device,\n","                                        disable_progress_bar=disable_progress_bar)\n","      train_epoch_end_time = time.time()\n","      train_epoch_time = train_epoch_end_time - train_epoch_start_time\n","\n","      # Perform testing step and time it\n","      test_epoch_start_time = time.time()\n","      test_loss, test_acc = test_step(epoch=epoch,\n","                                      model=model,\n","                                      dataloader=test_dataloader,\n","                                      loss_fn=loss_fn,\n","                                      device=device,\n","                                      disable_progress_bar=disable_progress_bar)\n","      test_epoch_end_time = time.time()\n","      test_epoch_time = test_epoch_end_time - test_epoch_start_time\n","\n","      # Print out what's happening\n","      print(\n","          f\"Epoch: {epoch+1} | \"\n","          f\"train_loss: {train_loss:.4f} | \"\n","          f\"train_acc: {train_acc:.4f} | \"\n","          f\"test_loss: {test_loss:.4f} | \"\n","          f\"test_acc: {test_acc:.4f} | \"\n","          f\"train_epoch_time: {train_epoch_time:.4f} | \"\n","          f\"test_epoch_time: {test_epoch_time:.4f}\"\n","      )\n","\n","      # Update results dictionary\n","      results[\"train_loss\"].append(train_loss)\n","      results[\"train_acc\"].append(train_acc)\n","      results[\"test_loss\"].append(test_loss)\n","      results[\"test_acc\"].append(test_acc)\n","      results[\"train_epoch_time\"].append(train_epoch_time)\n","      results[\"test_epoch_time\"].append(test_epoch_time)\n","\n","  # Return the filled results at the end of the epochs\n","  return results"]},{"cell_type":"markdown","metadata":{"id":"j6mabd9_kifd"},"source":["## 3. Time models across single run\n","\n","Training and testing functions ready!\n","\n","Time to start training/evaluating and timing our model.\n","\n","We'll start with the first experiment."]},{"cell_type":"markdown","metadata":{"id":"x5P2FYCvkifj"},"source":["### 3.1 Experiment 1 - Single run, no compile\n","\n","For experiment 1, we'll use the following parameters:\n","\n","| **Experiment** | **Model** | **Data** | **Epochs** | **Batch size** | **Image size** | **`torch.compile()`** |  \n","|----- |-----| -----| -----| -----| -----| -----|\n","| 1 (single run) | ResNet50 | CIFAR10 | 5 | 128 | 224 | No |\n","\n","We'll set the number of epochs to `5` and use a learning rate of `0.003` throughout (you can experiment with different learning rates for better results but we're focused on speed)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OUOBD8BLkifk"},"outputs":[],"source":["# Set the number of epochs as a constant\n","NUM_EPOCHS = 5\n","\n","# Set the learning rate as a constant (this can be changed to get better results but for now we're just focused on time)\n","LEARNING_RATE = 0.003"]},{"cell_type":"markdown","metadata":{"id":"EyYmR8Y-kifk"},"source":["> **Note:** Depending on the speed of your GPU, the following code can take a little while to run. For example, it took around 16 minutes on my local NVIDIA TITAN RTX and around 7 minutes on a NVIDIA A100 GPU on Google Colab Pro."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZbnXzNf5kifk","outputId":"2a222539-bce6-4154-d3c5-4bc56aff0604","colab":{"referenced_widgets":["2d3932b6a6fa4d659b87ba3d26f412e8","2f544154a5b048d98df936ad18774e38","a48a76ce7d684b5e9efc8b005fd9c05d","5bc6d82283564dd199bd441333f120f9","892d15bbe3944008be8572592b994128","ed2bb3f592094bbc81098e44f3fbc335","a74a7d81cdf6474185b4f599d2275c50","fd4ab8292b1d491a8024b100f0e9235d","4f6dd56062f746f985630fe2eae64435","801b1f9317664fa2a1128b69b0180cb4","ed406b4b41e54292b23cfef46bac7fdb"]}},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2d3932b6a6fa4d659b87ba3d26f412e8","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/5 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2f544154a5b048d98df936ad18774e38","version_major":2,"version_minor":0},"text/plain":["Training Epoch 0:   0%|          | 0/391 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a48a76ce7d684b5e9efc8b005fd9c05d","version_major":2,"version_minor":0},"text/plain":["Testing Epoch 0:   0%|          | 0/79 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: 1 | train_loss: 0.7734 | train_acc: 0.7333 | test_loss: 0.8021 | test_acc: 0.7477 | train_epoch_time: 184.9701 | test_epoch_time: 12.9893\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5bc6d82283564dd199bd441333f120f9","version_major":2,"version_minor":0},"text/plain":["Training Epoch 1:   0%|          | 0/391 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"892d15bbe3944008be8572592b994128","version_major":2,"version_minor":0},"text/plain":["Testing Epoch 1:   0%|          | 0/79 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: 2 | train_loss: 0.4337 | train_acc: 0.8501 | test_loss: 0.4794 | test_acc: 0.8338 | train_epoch_time: 185.3404 | test_epoch_time: 12.9515\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ed2bb3f592094bbc81098e44f3fbc335","version_major":2,"version_minor":0},"text/plain":["Training Epoch 2:   0%|          | 0/391 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a74a7d81cdf6474185b4f599d2275c50","version_major":2,"version_minor":0},"text/plain":["Testing Epoch 2:   0%|          | 0/79 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: 3 | train_loss: 0.3055 | train_acc: 0.8944 | test_loss: 0.4282 | test_acc: 0.8533 | train_epoch_time: 185.3870 | test_epoch_time: 13.0559\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fd4ab8292b1d491a8024b100f0e9235d","version_major":2,"version_minor":0},"text/plain":["Training Epoch 3:   0%|          | 0/391 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4f6dd56062f746f985630fe2eae64435","version_major":2,"version_minor":0},"text/plain":["Testing Epoch 3:   0%|          | 0/79 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: 4 | train_loss: 0.2268 | train_acc: 0.9198 | test_loss: 0.4387 | test_acc: 0.8580 | train_epoch_time: 185.5914 | test_epoch_time: 13.0495\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"801b1f9317664fa2a1128b69b0180cb4","version_major":2,"version_minor":0},"text/plain":["Training Epoch 4:   0%|          | 0/391 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ed406b4b41e54292b23cfef46bac7fdb","version_major":2,"version_minor":0},"text/plain":["Testing Epoch 4:   0%|          | 0/79 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: 5 | train_loss: 0.1723 | train_acc: 0.9395 | test_loss: 0.3901 | test_acc: 0.8754 | train_epoch_time: 185.5304 | test_epoch_time: 13.0517\n"]}],"source":["# Create model\n","model, transforms = create_model()\n","model.to(device)\n","\n","# Create loss function and optimizer\n","loss_fn = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(),\n","                             lr=LEARNING_RATE)\n","\n","# Train model and track results\n","single_run_no_compile_results = train(model=model,\n","                                      train_dataloader=train_dataloader,\n","                                      test_dataloader=test_dataloader,\n","                                      loss_fn=loss_fn,\n","                                      optimizer=optimizer,\n","                                      epochs=NUM_EPOCHS,\n","                                      device=device)"]},{"cell_type":"markdown","metadata":{"id":"iQEpHZUEkifr"},"source":["### 3.2 Experiment 2 - Single run, with compile\n","\n","Now we'll do the same experiment but this time we'll use `torch.compile()`.\n","\n","| **Experiment** | **Model** | **Data** | **Epochs** | **Batch size** | **Image size** | **`torch.compile()`** |  \n","|----- |-----| -----| -----| -----| -----| -----|\n","| 2 (single run) | ResNet50 | CIFAR10 | 5 | 128 | 224 | Yes |\n","\n","> **Note:** Depending on the speed of your GPU, the following code can take a little while to run. For example, it took around 16 minutes on my local NVIDIA TITAN RTX and around 7 minutes on a NVIDIA A100 GPU on Google Colab Pro."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YvDVqV8qkifr","outputId":"c2d11c6c-e982-4ed8-8fc4-683aa1272860","colab":{"referenced_widgets":["3dffe08523224e548af0d418592dd488","56e51390cff74ffaaaae2c81fa44c734","ccd33c0eb0c749ba84e213c439cae923","e771faaff76e4aa3a02c50f9a44e4e49","5a449904b51f4196b15c406e94dd4afb","107e83a661944b5399375e0b9865fffe","a951984d0d8a40f99f65d46ef2991fa9","c8b835b479934323b714e6c0e9619f44","0e75958332e24018af652c6d52d2f98a","e41cd029f0184a0c8f635a824f15922f","b9aeca32a54e49e997424afa99626e77"]}},"outputs":[{"name":"stdout","output_type":"stream","text":["Time to compile: 0.00491642951965332 | Note: The first time you compile your model, the first few epochs will be slower than subsequent runs.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3dffe08523224e548af0d418592dd488","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/5 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"56e51390cff74ffaaaae2c81fa44c734","version_major":2,"version_minor":0},"text/plain":["Training Epoch 0:   0%|          | 0/391 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ccd33c0eb0c749ba84e213c439cae923","version_major":2,"version_minor":0},"text/plain":["Testing Epoch 0:   0%|          | 0/79 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: 1 | train_loss: 0.7585 | train_acc: 0.7364 | test_loss: 0.5852 | test_acc: 0.8004 | train_epoch_time: 196.4621 | test_epoch_time: 21.0730\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e771faaff76e4aa3a02c50f9a44e4e49","version_major":2,"version_minor":0},"text/plain":["Training Epoch 1:   0%|          | 0/391 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5a449904b51f4196b15c406e94dd4afb","version_major":2,"version_minor":0},"text/plain":["Testing Epoch 1:   0%|          | 0/79 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: 2 | train_loss: 0.4288 | train_acc: 0.8521 | test_loss: 0.5468 | test_acc: 0.8108 | train_epoch_time: 169.9891 | test_epoch_time: 11.0555\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"107e83a661944b5399375e0b9865fffe","version_major":2,"version_minor":0},"text/plain":["Training Epoch 2:   0%|          | 0/391 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a951984d0d8a40f99f65d46ef2991fa9","version_major":2,"version_minor":0},"text/plain":["Testing Epoch 2:   0%|          | 0/79 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: 3 | train_loss: 0.3080 | train_acc: 0.8928 | test_loss: 0.4791 | test_acc: 0.8377 | train_epoch_time: 170.4004 | test_epoch_time: 10.9841\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c8b835b479934323b714e6c0e9619f44","version_major":2,"version_minor":0},"text/plain":["Training Epoch 3:   0%|          | 0/391 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0e75958332e24018af652c6d52d2f98a","version_major":2,"version_minor":0},"text/plain":["Testing Epoch 3:   0%|          | 0/79 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: 4 | train_loss: 0.2322 | train_acc: 0.9184 | test_loss: 0.5551 | test_acc: 0.8306 | train_epoch_time: 170.1974 | test_epoch_time: 11.0482\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e41cd029f0184a0c8f635a824f15922f","version_major":2,"version_minor":0},"text/plain":["Training Epoch 4:   0%|          | 0/391 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b9aeca32a54e49e997424afa99626e77","version_major":2,"version_minor":0},"text/plain":["Testing Epoch 4:   0%|          | 0/79 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch: 5 | train_loss: 0.1766 | train_acc: 0.9376 | test_loss: 0.3410 | test_acc: 0.8874 | train_epoch_time: 170.0547 | test_epoch_time: 10.9239\n"]}],"source":["# Create model and transforms\n","model, transforms = create_model()\n","model.to(device)\n","\n","# Create loss function and optimizer\n","loss_fn = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(),\n","                             lr=LEARNING_RATE)\n","\n","# Compile the model and time how long it takes\n","compile_start_time = time.time()\n","\n","### New in PyTorch 2.x ###\n","compiled_model = torch.compile(model)\n","##########################\n","\n","compile_end_time = time.time()\n","compile_time = compile_end_time - compile_start_time\n","print(f\"Time to compile: {compile_time} | Note: The first time you compile your model, the first few epochs will be slower than subsequent runs.\")\n","\n","# Train the compiled model\n","single_run_compile_results = train(model=compiled_model,\n","                                   train_dataloader=train_dataloader,\n","                                   test_dataloader=test_dataloader,\n","                                   loss_fn=loss_fn,\n","                                   optimizer=optimizer,\n","                                   epochs=NUM_EPOCHS,\n","                                   device=device)"]},{"cell_type":"markdown","metadata":{"id":"6A5_ti57kifs"},"source":["### 3.3 Compare the results of experiment 1 and 2\n","\n","Nice!\n","\n","We've got two trained models:\n","\n","1. One without `torch.compile()`.\n","2. One with `torch.compile()`.\n","\n","Let's compare the results of each experiment.\n","\n","To do so, we'll first create dataframes of the results of each.\n","\n","Then we'll plot the results of each experiment on a bar chart."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J0oimV7Zkifs"},"outputs":[],"source":["# Turn experiment results into dataframes\n","import pandas as pd\n","single_run_no_compile_results_df = pd.DataFrame(single_run_no_compile_results)\n","single_run_compile_results_df = pd.DataFrame(single_run_compile_results)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YqCr2QlYkify","outputId":"54561ddf-14fe-4e3a-ee00-39668a1b8371"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>train_loss</th>\n","      <th>train_acc</th>\n","      <th>test_loss</th>\n","      <th>test_acc</th>\n","      <th>train_epoch_time</th>\n","      <th>test_epoch_time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.773435</td>\n","      <td>0.733272</td>\n","      <td>0.802100</td>\n","      <td>0.747725</td>\n","      <td>184.970135</td>\n","      <td>12.989331</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.433699</td>\n","      <td>0.850052</td>\n","      <td>0.479412</td>\n","      <td>0.833762</td>\n","      <td>185.340373</td>\n","      <td>12.951483</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.305494</td>\n","      <td>0.894429</td>\n","      <td>0.428212</td>\n","      <td>0.853343</td>\n","      <td>185.386973</td>\n","      <td>13.055891</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.226751</td>\n","      <td>0.919829</td>\n","      <td>0.438668</td>\n","      <td>0.857991</td>\n","      <td>185.591368</td>\n","      <td>13.049541</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.172269</td>\n","      <td>0.939482</td>\n","      <td>0.390148</td>\n","      <td>0.875396</td>\n","      <td>185.530370</td>\n","      <td>13.051713</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   train_loss  train_acc  test_loss  test_acc  train_epoch_time  \\\n","0    0.773435   0.733272   0.802100  0.747725        184.970135   \n","1    0.433699   0.850052   0.479412  0.833762        185.340373   \n","2    0.305494   0.894429   0.428212  0.853343        185.386973   \n","3    0.226751   0.919829   0.438668  0.857991        185.591368   \n","4    0.172269   0.939482   0.390148  0.875396        185.530370   \n","\n","   test_epoch_time  \n","0        12.989331  \n","1        12.951483  \n","2        13.055891  \n","3        13.049541  \n","4        13.051713  "]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["# Check out the head of one of the results dataframes\n","single_run_no_compile_results_df.head()"]},{"cell_type":"markdown","metadata":{"id":"Pf8jIS7Rkifz"},"source":["Got the results for experiments 1 and 2!\n","\n","Now let's write a function to take in the results and compare them with a bar chart.\n","\n","We'll add some metadata to the function so it can display some information about the experiments.\n","\n","Namely all of the parameters in our experiment setup:\n","* The dataset name.\n","* The model name.\n","* The number of epochs.\n","* The batch size.\n","* The image size."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CU6rCoMWkifz"},"outputs":[],"source":["# Create filename to save the results\n","DATASET_NAME = \"CIFAR10\"\n","MODEL_NAME = \"ResNet50\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RY_J9Ziokifz"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","def plot_mean_epoch_times(non_compiled_results: pd.DataFrame,\n","                          compiled_results: pd.DataFrame,\n","                          multi_runs: bool=False,\n","                          num_runs: int=0,\n","                          save: bool=False,\n","                          save_path: str=\"\",\n","                          dataset_name: str=DATASET_NAME,\n","                          model_name: str=MODEL_NAME,\n","                          num_epochs: int=NUM_EPOCHS,\n","                          image_size: int=IMAGE_SIZE,\n","                          batch_size: int=BATCH_SIZE) -> plt.figure:\n","\n","    # Get the mean epoch times from the non-compiled models\n","    mean_train_epoch_time = non_compiled_results.train_epoch_time.mean()\n","    mean_test_epoch_time = non_compiled_results.test_epoch_time.mean()\n","    mean_results = [mean_train_epoch_time, mean_test_epoch_time]\n","\n","    # Get the mean epoch times from the compiled models\n","    mean_compile_train_epoch_time = compiled_results.train_epoch_time.mean()\n","    mean_compile_test_epoch_time = compiled_results.test_epoch_time.mean()\n","    mean_compile_results = [mean_compile_train_epoch_time, mean_compile_test_epoch_time]\n","\n","    # Calculate the percentage difference between the mean compile and non-compile train epoch times\n","    train_epoch_time_diff = mean_compile_train_epoch_time - mean_train_epoch_time\n","    train_epoch_time_diff_percent = (train_epoch_time_diff / mean_train_epoch_time) * 100\n","\n","    # Calculate the percentage difference between the mean compile and non-compile test epoch times\n","    test_epoch_time_diff = mean_compile_test_epoch_time - mean_test_epoch_time\n","    test_epoch_time_diff_percent = (test_epoch_time_diff / mean_test_epoch_time) * 100\n","\n","    # Print the mean difference percentages\n","    print(f\"Mean train epoch time difference: {round(train_epoch_time_diff_percent, 3)}% (negative means faster)\")\n","    print(f\"Mean test epoch time difference: {round(test_epoch_time_diff_percent, 3)}% (negative means faster)\")\n","\n","    # Create a bar plot of the mean train and test epoch time for both compiled and non-compiled models\n","    plt.figure(figsize=(10, 7))\n","    width = 0.3\n","    x_indicies = np.arange(len(mean_results))\n","\n","    plt.bar(x=x_indicies, height=mean_results, width=width, label=\"non_compiled_results\")\n","    plt.bar(x=x_indicies + width, height=mean_compile_results, width=width, label=\"compiled_results\")\n","    plt.xticks(x_indicies + width / 2, (\"Train Epoch\", \"Test Epoch\"))\n","    plt.ylabel(\"Mean epoch time (seconds, lower is better)\")\n","\n","    # Create the title based on the parameters passed to the function\n","    if multi_runs:\n","        plt.suptitle(\"Multiple run results\")\n","        plt.title(f\"GPU: {gpu_name} | Epochs: {num_epochs} ({num_runs} runs) | Data: {dataset_name} | Model: {model_name} | Image size: {image_size} | Batch size: {batch_size}\")\n","    else:\n","        plt.suptitle(\"Single run results\")\n","        plt.title(f\"GPU: {gpu_name} | Epochs: {num_epochs} | Data: {dataset_name} | Model: {model_name} | Image size: {image_size} | Batch size: {batch_size}\")\n","    plt.legend();\n","\n","    # Save the figure\n","    if save:\n","        assert save_path != \"\", \"Please specify a save path to save the model figure to via the save_path parameter.\"\n","        plt.savefig(save_path)\n","        print(f\"[INFO] Plot saved to {save_path}\")"]},{"cell_type":"markdown","metadata":{"id":"LjHOqO10kif5"},"source":["Plot function ready!\n","\n","Let's create a directory to store our figures in and then plot the results of our first two experiments."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aT1qX3cpkif5","outputId":"4cefe31e-1ae1-49df-92f0-c3217f22644b"},"outputs":[{"name":"stdout","output_type":"stream","text":["[INFO] Save path for single run results: pytorch_2_results/figures/single_run_NVIDIA_TITAN_RTX_ResNet50_CIFAR10_224_train_epoch_time.png\n","Mean train epoch time difference: -5.364% (negative means faster)\n","Mean test epoch time difference: -0.02% (negative means faster)\n","[INFO] Plot saved to pytorch_2_results/figures/single_run_NVIDIA_TITAN_RTX_ResNet50_CIFAR10_224_train_epoch_time.png\n"]},{"data":{"image/png":"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","text/plain":["<Figure size 720x504 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["# Create directory for saving figures\n","import os\n","dir_to_save_figures_in = \"pytorch_2_results/figures/\"\n","os.makedirs(dir_to_save_figures_in, exist_ok=True)\n","\n","# Create a save path for the single run results\n","save_path_multi_run = f\"{dir_to_save_figures_in}single_run_{GPU_NAME}_{MODEL_NAME}_{DATASET_NAME}_{IMAGE_SIZE}_train_epoch_time.png\"\n","print(f\"[INFO] Save path for single run results: {save_path_multi_run}\")\n","\n","# Plot the results and save the figures\n","plot_mean_epoch_times(non_compiled_results=single_run_no_compile_results_df,\n","                      compiled_results=single_run_compile_results_df,\n","                      multi_runs=False,\n","                      save_path=save_path_multi_run,\n","                      save=True)"]},{"cell_type":"markdown","metadata":{"id":"-eSfGUMSkif7"},"source":["Hmm... what's happening here?\n","\n","It looks like the model with `torch.compile()` took *longer* than the model without it (on an A100, this is the case, but on my local NVIDIA TITAN RTX, the compiled model is slightly faster).\n","\n","Why might this be the case?\n","\n","Well on a per epoch time we can see that although experiment 2 (with `torch.compile()`) was far slower for the first epoch, it started being faster than experiment 1 (without `torch.compile()`) for subsequent epochs.\n","\n","This is because behind the scenes `torch.compile()` spends the first steps of a training run \"warming up\" the model and performing optimization steps behind the scenes.\n","\n","These **optimization steps take time up front** but mean **subsequent steps should be faster**.\n","\n","To test if this is true, you could try training the model above for longer (say 50 epochs rather than 5) and see what the average training times come out to be."]},{"cell_type":"markdown","metadata":{"id":"VLUSnBWCkif7"},"source":["### 3.4 Save single run results to file with GPU details\n","\n","We can save the raw data of our results to file too by exporting the dataframes as CSVs.\n","\n","We'll first create a directory for storing results.\n","\n","Then we'll create filepaths to save each of the target dataframes to before exporting them."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_S2okHR7kigB","outputId":"e75dae5e-9872-4852-affb-cf55313183e0"},"outputs":[{"name":"stdout","output_type":"stream","text":["[INFO] Saving non-compiled experiment 1 results to: pytorch_2_results/single_run_results/single_run_non_compiled_results_CIFAR10_ResNet50_NVIDIA_TITAN_RTX.csv\n","[INFO] Saving compiled experiment 2 results to: pytorch_2_results/single_run_results/single_run_compiled_results_CIFAR10_ResNet50_NVIDIA_TITAN_RTX.csv\n"]}],"source":["# Make a directory for single_run results\n","import os\n","pytorch_2_results_dir = \"pytorch_2_results\"\n","pytorch_2_single_run_results_dir = f\"{pytorch_2_results_dir}/single_run_results\"\n","os.makedirs(pytorch_2_single_run_results_dir, exist_ok=True)\n","\n","# Create filenames for each of the dataframes\n","save_name_for_non_compiled_results = f\"single_run_non_compiled_results_{DATASET_NAME}_{MODEL_NAME}_{GPU_NAME}.csv\"\n","save_name_for_compiled_results = f\"single_run_compiled_results_{DATASET_NAME}_{MODEL_NAME}_{GPU_NAME}.csv\"\n","\n","# Create filepaths to save the results to\n","single_run_no_compile_save_path = f\"{pytorch_2_single_run_results_dir}/{save_name_for_non_compiled_results}\"\n","single_run_compile_save_path = f\"{pytorch_2_single_run_results_dir}/{save_name_for_compiled_results}\"\n","print(f\"[INFO] Saving non-compiled experiment 1 results to: {single_run_no_compile_save_path}\")\n","print(f\"[INFO] Saving compiled experiment 2 results to: {single_run_compile_save_path}\")\n","\n","# Save the results\n","single_run_no_compile_results_df.to_csv(single_run_no_compile_save_path)\n","single_run_compile_results_df.to_csv(single_run_compile_save_path)"]},{"cell_type":"markdown","metadata":{"id":"dogikz0ikigC"},"source":["## 4. Time models across multiple runs\n","\n","Now we've tested our model with a single run with `torch.compile()` on and off, let's do the same for multiple runs.\n","\n","We're going to start by creating three functions for experiments 3 and 4.\n","\n","1. **Experiment 3:** `create_and_train_non_compiled_model()` - this function will be similar to the workflow we've used for the single runs. We'll put the model creation (via `create_model()`) and training in a single function so we can call it multiple times (for multiple runs) and measure the time of each run.\n","2. **Experiment 4:** `create_compiled_model()` - this function will be similar to the `create_model()` function above, however, it will create a normal PyTorch model and then call `torch.compile()` on it and return it.\n","3. **Experiment 4:** `train_compiled_model()` - this function will take in a compiled model and train it in the same way we've been training our models for single runs.\n","\n","Why separate functions 2 and 3 (`create_compiled_model()` and `train_compiled_model()`) for experiment 4?\n","\n","Because calling `torch.compile()` on model means that for the first few runs, the model will be \"warming up\" as PyTorch calculates a bunch of optimization steps behind the scenes.\n","\n","So in practice, you'll generally want to compile up front *once* and then train/perform inference with an already compiled model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JbbxaeMjkigC"},"outputs":[],"source":["def create_and_train_non_compiled_model(epochs=NUM_EPOCHS,\n","                                        learning_rate=LEARNING_RATE,\n","                                        disable_progress_bar=False):\n","    \"\"\"\n","    Create and train a non-compiled PyTorch model.\n","    \"\"\"\n","    model, _ = create_model()\n","    model.to(device)\n","\n","    loss_fn = torch.nn.CrossEntropyLoss()\n","    optimizer = torch.optim.Adam(model.parameters(),\n","                                 lr=learning_rate)\n","\n","    results = train(model=model,\n","                    train_dataloader=train_dataloader,\n","                    test_dataloader=test_dataloader,\n","                    loss_fn=loss_fn,\n","                    optimizer=optimizer,\n","                    epochs=epochs,\n","                    device=device,\n","                    disable_progress_bar=disable_progress_bar)\n","    return results\n","\n","def create_compiled_model():\n","    \"\"\"\n","    Create a compiled PyTorch model and return it.\n","    \"\"\"\n","    model, _ = create_model()\n","    model.to(device)\n","\n","    compile_start_time = time.time()\n","    ### New in PyTorch 2.x ###\n","    compiled_model = torch.compile(model)\n","    ##########################\n","    compile_end_time = time.time()\n","\n","    compile_time = compile_end_time - compile_start_time\n","\n","    print(f\"Time to compile: {compile_time} | Note: The first time you compile your model, the first few epochs will be slower than subsequent runs.\")\n","    return compiled_model\n","\n","def train_compiled_model(model=compiled_model,\n","                         epochs=NUM_EPOCHS,\n","                         learning_rate=LEARNING_RATE,\n","                         disable_progress_bar=False):\n","    \"\"\"\n","    Train a compiled model and return the results.\n","    \"\"\"\n","    loss_fn = torch.nn.CrossEntropyLoss()\n","    optimizer = torch.optim.Adam(compiled_model.parameters(),\n","                                 lr=learning_rate)\n","\n","    compile_results = train(model=model,\n","                            train_dataloader=train_dataloader,\n","                            test_dataloader=test_dataloader,\n","                            loss_fn=loss_fn,\n","                            optimizer=optimizer,\n","                            epochs=epochs,\n","                            device=device,\n","                            disable_progress_bar=disable_progress_bar)\n","\n","    return compile_results"]},{"cell_type":"markdown","metadata":{"id":"yImOxTUbkigI"},"source":["### 4.1 Experiment 3 - Multiple runs, no compile\n","\n","Functions ready for experiment 3 and 4!\n","\n","Let's start with experiment 3.\n","\n","| **Experiment** | **Model** | **Data** | **Epochs** | **Batch size** | **Image size** | **`torch.compile()`** |  \n","|----- |-----| -----| -----| -----| -----| -----|\n","| 3 (multi-run) | ResNet50 | CIFAR10 | 3x5 | 128 | 224 | No |\n","\n","We'll set the number of runs to 3 and the number of epochs to 5.\n","\n","We'll create an empty list to store the results and append the results of each run to it after each run.\n","\n","> **Note:** Running the following code can take quite a while depending on the speed of your GPU, for me, it took 20 minutes on a NVIDIA A100 on Google Colab Pro and around 49 minutes on a NVIDIA TITAN RTX."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LUpLbgBtkigI","outputId":"3a8bc41b-17a8-40d3-ed2b-7711b2e22f3a","colab":{"referenced_widgets":["4108ca563be14d4a8ed57c3de1697ebd"]}},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4108ca563be14d4a8ed57c3de1697ebd","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["[INFO] Run 1 of 3 for non-compiled model\n","Epoch: 1 | train_loss: 0.8242 | train_acc: 0.7136 | test_loss: 0.5486 | test_acc: 0.8124 | train_epoch_time: 185.1112 | test_epoch_time: 12.9925\n","Epoch: 2 | train_loss: 0.4415 | train_acc: 0.8479 | test_loss: 0.6415 | test_acc: 0.7829 | train_epoch_time: 185.0138 | test_epoch_time: 12.9690\n","Epoch: 3 | train_loss: 0.3229 | train_acc: 0.8882 | test_loss: 0.4486 | test_acc: 0.8488 | train_epoch_time: 185.0366 | test_epoch_time: 12.9433\n","Epoch: 4 | train_loss: 0.2433 | train_acc: 0.9151 | test_loss: 0.4376 | test_acc: 0.8596 | train_epoch_time: 185.0900 | test_epoch_time: 12.9465\n","Epoch: 5 | train_loss: 0.1785 | train_acc: 0.9379 | test_loss: 0.4305 | test_acc: 0.8641 | train_epoch_time: 185.0405 | test_epoch_time: 13.0102\n","[INFO] Run 2 of 3 for non-compiled model\n","Epoch: 1 | train_loss: 0.8304 | train_acc: 0.7101 | test_loss: 0.6132 | test_acc: 0.7884 | train_epoch_time: 185.0911 | test_epoch_time: 13.0429\n","Epoch: 2 | train_loss: 0.4602 | train_acc: 0.8411 | test_loss: 0.6183 | test_acc: 0.7907 | train_epoch_time: 185.0738 | test_epoch_time: 12.9596\n","Epoch: 3 | train_loss: 0.3283 | train_acc: 0.8869 | test_loss: 0.4309 | test_acc: 0.8534 | train_epoch_time: 185.0462 | test_epoch_time: 12.9877\n","Epoch: 4 | train_loss: 0.2474 | train_acc: 0.9140 | test_loss: 0.4525 | test_acc: 0.8565 | train_epoch_time: 184.9521 | test_epoch_time: 12.9942\n","Epoch: 5 | train_loss: 0.1860 | train_acc: 0.9360 | test_loss: 0.6284 | test_acc: 0.8195 | train_epoch_time: 184.9911 | test_epoch_time: 12.9369\n","[INFO] Run 3 of 3 for non-compiled model\n","Epoch: 1 | train_loss: 0.7915 | train_acc: 0.7246 | test_loss: 0.6102 | test_acc: 0.7894 | train_epoch_time: 184.9795 | test_epoch_time: 13.0175\n","Epoch: 2 | train_loss: 0.4394 | train_acc: 0.8477 | test_loss: 0.5958 | test_acc: 0.7968 | train_epoch_time: 184.9266 | test_epoch_time: 12.9909\n","Epoch: 3 | train_loss: 0.3156 | train_acc: 0.8893 | test_loss: 0.4299 | test_acc: 0.8547 | train_epoch_time: 185.1226 | test_epoch_time: 12.9396\n","Epoch: 4 | train_loss: 0.2371 | train_acc: 0.9163 | test_loss: 0.4185 | test_acc: 0.8608 | train_epoch_time: 184.9447 | test_epoch_time: 12.9673\n","Epoch: 5 | train_loss: 0.1739 | train_acc: 0.9389 | test_loss: 0.3797 | test_acc: 0.8805 | train_epoch_time: 184.9552 | test_epoch_time: 13.0328\n"]}],"source":["# Run non-compiled model for multiple runs\n","NUM_RUNS = 3\n","NUM_EPOCHS = 5\n","\n","# Create an empty list to store multiple run results\n","non_compile_results_multiple_runs = []\n","\n","# Run non-compiled model for multiple runs\n","for i in tqdm(range(NUM_RUNS)):\n","    print(f\"[INFO] Run {i+1} of {NUM_RUNS} for non-compiled model\")\n","    results = create_and_train_non_compiled_model(epochs=NUM_EPOCHS, disable_progress_bar=True)\n","    non_compile_results_multiple_runs.append(results)"]},{"cell_type":"markdown","metadata":{"id":"liqZmS-wkigJ"},"source":["Now we've got a list of results from experiment 3, let's iterate through them and create a dataframe containing all of the results.\n","\n","We'll then average the results across the 3 runs by grouping by the epoch number (the index of the dataframe) and taking the mean of the results."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MuVa8X0okigO","outputId":"1d4425d8-981f-4f35-b01d-e7f4971f2116"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>train_loss</th>\n","      <th>train_acc</th>\n","      <th>test_loss</th>\n","      <th>test_acc</th>\n","      <th>train_epoch_time</th>\n","      <th>test_epoch_time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.815352</td>\n","      <td>0.716103</td>\n","      <td>0.590690</td>\n","      <td>0.796710</td>\n","      <td>185.060622</td>\n","      <td>13.017663</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.447013</td>\n","      <td>0.845567</td>\n","      <td>0.618526</td>\n","      <td>0.790150</td>\n","      <td>185.004740</td>\n","      <td>12.973144</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.322255</td>\n","      <td>0.888117</td>\n","      <td>0.436471</td>\n","      <td>0.852321</td>\n","      <td>185.068499</td>\n","      <td>12.956863</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.242587</td>\n","      <td>0.915120</td>\n","      <td>0.436207</td>\n","      <td>0.858946</td>\n","      <td>184.995601</td>\n","      <td>12.969341</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.179439</td>\n","      <td>0.937612</td>\n","      <td>0.479547</td>\n","      <td>0.854727</td>\n","      <td>184.995575</td>\n","      <td>12.993280</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   train_loss  train_acc  test_loss  test_acc  train_epoch_time  \\\n","0    0.815352   0.716103   0.590690  0.796710        185.060622   \n","1    0.447013   0.845567   0.618526  0.790150        185.004740   \n","2    0.322255   0.888117   0.436471  0.852321        185.068499   \n","3    0.242587   0.915120   0.436207  0.858946        184.995601   \n","4    0.179439   0.937612   0.479547  0.854727        184.995575   \n","\n","   test_epoch_time  \n","0        13.017663  \n","1        12.973144  \n","2        12.956863  \n","3        12.969341  \n","4        12.993280  "]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["# Go through non_compile_results_multiple_runs and create a dataframe for each run then concatenate them together\n","non_compile_results_dfs = []\n","for result in non_compile_results_multiple_runs:\n","    result_df = pd.DataFrame(result)\n","    non_compile_results_dfs.append(result_df)\n","non_compile_results_multiple_runs_df = pd.concat(non_compile_results_dfs)\n","\n","# Get the averages across the multiple runs\n","non_compile_results_multiple_runs_df = non_compile_results_multiple_runs_df.groupby(non_compile_results_multiple_runs_df.index).mean()\n","non_compile_results_multiple_runs_df"]},{"cell_type":"markdown","metadata":{"id":"YkFEh1KGkigQ"},"source":["Wonderful!\n","\n","We can inspect these later, let's move onto experiment 4."]},{"cell_type":"markdown","metadata":{"id":"q5yXenGzkigQ"},"source":["### 4.2 Experiment 4 - Multiple runs, with compile\n","\n","Time for experiment 4.\n","\n","Running a compiled model for multiple runs.\n","\n","| **Experiment** | **Model** | **Data** | **Epochs** | **Batch size** | **Image size** | **`torch.compile()`** |  \n","|----- |-----| -----| -----| -----| -----| -----|\n","| 4 (multi-run) | ResNet50 | CIFAR10 | 3x5 | 128 | 224 | Yes |\n","\n","We can do this by using the `create_compiled_model()` and `train_compiled_model()` functions we created earlier.\n","\n","We'll start by creating the compiled model *first* and then training it for 3 runs.\n","\n","We're not worried about the results of the model (loss and accuracy) as much as how long it takes.\n","\n","The reason why we compile it once at the start is that PyTorch only needs to run the optimization steps once (this can take some time) and then it can reuse them for the rest of the runs.\n","\n","We'll also create an empty list just like before to store our model's results over a series of runs.\n","\n","> **Note:** Running the following code can take quite a while depending on the speed of your GPU, for me, it took 18 minutes on a NVIDIA A100 on Google Colab Pro and around 45 minutes on a NVIDIA TITAN RTX."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"afc1SE4MkigQ","outputId":"6a804b1d-a910-4fc3-82b2-13487bb4d7e9","colab":{"referenced_widgets":["f3338920367645c1b013ba2882dfc2d8"]}},"outputs":[{"name":"stdout","output_type":"stream","text":["Time to compile: 0.001275777816772461 | Note: The first time you compile your model, the first few epochs will be slower than subsequent runs.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f3338920367645c1b013ba2882dfc2d8","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["[INFO] Run 1 of 3 for compiled model\n","Epoch: 1 | train_loss: 0.8026 | train_acc: 0.7192 | test_loss: 0.6995 | test_acc: 0.7650 | train_epoch_time: 194.3336 | test_epoch_time: 20.6106\n","Epoch: 2 | train_loss: 0.4440 | train_acc: 0.8483 | test_loss: 0.5565 | test_acc: 0.8089 | train_epoch_time: 169.3882 | test_epoch_time: 10.8076\n","Epoch: 3 | train_loss: 0.3208 | train_acc: 0.8896 | test_loss: 0.4164 | test_acc: 0.8620 | train_epoch_time: 169.9283 | test_epoch_time: 10.8361\n","Epoch: 4 | train_loss: 0.2329 | train_acc: 0.9197 | test_loss: 0.3635 | test_acc: 0.8792 | train_epoch_time: 169.8744 | test_epoch_time: 10.9050\n","Epoch: 5 | train_loss: 0.1803 | train_acc: 0.9369 | test_loss: 0.4387 | test_acc: 0.8587 | train_epoch_time: 169.6391 | test_epoch_time: 10.8240\n","[INFO] Run 2 of 3 for compiled model\n","Epoch: 1 | train_loss: 0.1875 | train_acc: 0.9347 | test_loss: 0.4187 | test_acc: 0.8714 | train_epoch_time: 169.4814 | test_epoch_time: 10.8180\n","Epoch: 2 | train_loss: 0.1288 | train_acc: 0.9550 | test_loss: 0.4333 | test_acc: 0.8698 | train_epoch_time: 169.4503 | test_epoch_time: 10.8263\n","Epoch: 3 | train_loss: 0.0950 | train_acc: 0.9672 | test_loss: 0.4867 | test_acc: 0.8650 | train_epoch_time: 169.6038 | test_epoch_time: 10.8199\n","Epoch: 4 | train_loss: 0.0943 | train_acc: 0.9675 | test_loss: 0.3714 | test_acc: 0.8966 | train_epoch_time: 169.5757 | test_epoch_time: 10.8221\n","Epoch: 5 | train_loss: 0.0537 | train_acc: 0.9821 | test_loss: 0.5002 | test_acc: 0.8701 | train_epoch_time: 169.5253 | test_epoch_time: 10.8426\n","[INFO] Run 3 of 3 for compiled model\n","Epoch: 1 | train_loss: 0.0705 | train_acc: 0.9751 | test_loss: 0.4333 | test_acc: 0.8839 | train_epoch_time: 169.4846 | test_epoch_time: 10.9057\n","Epoch: 2 | train_loss: 0.0595 | train_acc: 0.9802 | test_loss: 0.4341 | test_acc: 0.8904 | train_epoch_time: 169.6055 | test_epoch_time: 10.8804\n","Epoch: 3 | train_loss: 0.0405 | train_acc: 0.9859 | test_loss: 0.4478 | test_acc: 0.8901 | train_epoch_time: 169.5788 | test_epoch_time: 10.8449\n","Epoch: 4 | train_loss: 0.0365 | train_acc: 0.9873 | test_loss: 0.5382 | test_acc: 0.8765 | train_epoch_time: 169.6732 | test_epoch_time: 10.9873\n","Epoch: 5 | train_loss: 0.0422 | train_acc: 0.9854 | test_loss: 0.5057 | test_acc: 0.8832 | train_epoch_time: 169.6618 | test_epoch_time: 10.8969\n"]}],"source":["# Create compiled model\n","compiled_model = create_compiled_model()\n","\n","# Create an empty list to store compiled model results\n","compiled_results_multiple_runs = []\n","\n","# Run compiled model for multiple runs\n","for i in tqdm(range(NUM_RUNS)):\n","    print(f\"[INFO] Run {i+1} of {NUM_RUNS} for compiled model\")\n","    # Train the compiled model (note: the model will only be compiled once and then re-used for subsequent runs)\n","    results = train_compiled_model(model=compiled_model, epochs=NUM_EPOCHS, disable_progress_bar=True)\n","    compiled_results_multiple_runs.append(results)"]},{"cell_type":"markdown","metadata":{"id":"7zcnKB7okigX"},"source":["Experiment 4 done!\n","\n","Now let's put the results together into a dataframe and take the mean across each of the runs (we'll do this by grouping by the epoch number, which is the index number of the dataframe)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"993MDgDqkigX","outputId":"cb358e5e-e14a-4e0d-8b32-eb9c8f934a75"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>train_loss</th>\n","      <th>train_acc</th>\n","      <th>test_loss</th>\n","      <th>test_acc</th>\n","      <th>train_epoch_time</th>\n","      <th>test_epoch_time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.353548</td>\n","      <td>0.876332</td>\n","      <td>0.517181</td>\n","      <td>0.840124</td>\n","      <td>177.766548</td>\n","      <td>14.111428</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.210781</td>\n","      <td>0.927845</td>\n","      <td>0.474630</td>\n","      <td>0.856375</td>\n","      <td>169.481367</td>\n","      <td>10.838063</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.152098</td>\n","      <td>0.947577</td>\n","      <td>0.450293</td>\n","      <td>0.872396</td>\n","      <td>169.703638</td>\n","      <td>10.833619</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.121230</td>\n","      <td>0.958177</td>\n","      <td>0.424376</td>\n","      <td>0.884065</td>\n","      <td>169.707751</td>\n","      <td>10.904810</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0.092080</td>\n","      <td>0.968116</td>\n","      <td>0.481520</td>\n","      <td>0.870649</td>\n","      <td>169.608708</td>\n","      <td>10.854486</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   train_loss  train_acc  test_loss  test_acc  train_epoch_time  \\\n","0    0.353548   0.876332   0.517181  0.840124        177.766548   \n","1    0.210781   0.927845   0.474630  0.856375        169.481367   \n","2    0.152098   0.947577   0.450293  0.872396        169.703638   \n","3    0.121230   0.958177   0.424376  0.884065        169.707751   \n","4    0.092080   0.968116   0.481520  0.870649        169.608708   \n","\n","   test_epoch_time  \n","0        14.111428  \n","1        10.838063  \n","2        10.833619  \n","3        10.904810  \n","4        10.854486  "]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":["# Go through compile_results_multiple_runs and create a dataframe for each run then concatenate them together\n","compile_results_dfs = []\n","for result in compiled_results_multiple_runs:\n","    result_df = pd.DataFrame(result)\n","    compile_results_dfs.append(result_df)\n","compile_results_multiple_runs_df = pd.concat(compile_results_dfs)\n","\n","# Get the averages across the multiple runs\n","compile_results_multiple_runs_df = compile_results_multiple_runs_df.groupby(compile_results_multiple_runs_df.index).mean() # .index = groupby the epoch number\n","compile_results_multiple_runs_df"]},{"cell_type":"markdown","metadata":{"id":"i3ebt9tPkigX"},"source":["### 4.3 Compare results of experiment 3 and 4\n","\n","Multi-run experiments done!\n","\n","Let's inspect the results.\n","\n","We can do so with our `plot_mean_epoch_times()` function we created before.\n","\n","This time we'll set the `multi_runs` parameter to `True` so that our plots reflect the fact we're plotting the results of multiple runs.\n","\n","We'll make sure we've got a directory to save the figure to as well."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1b_96OhCkigX","outputId":"61230a5a-d29d-41b1-d272-4ad159ca06e9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mean train epoch time difference: -7.443% (negative means faster)\n","Mean test epoch time difference: -11.351% (negative means faster)\n","[INFO] Plot saved to pytorch_2_results/figures/multi_run_NVIDIA_TITAN_RTX_ResNet50_CIFAR10_224_train_epoch_time.png\n"]},{"data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAroAAAHOCAYAAAB3pqpTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABLNklEQVR4nO3dd7gdZdWw8XsBgVAiNRZqAhJKIAQISCdKE2nSpIQSkKYgxfJSVAy+oqgogqJ54ZOmNKUoCCJSQlEQEwglFGlBQg2BQCgJJKzvj5kTdk5OzTnJJJP7d13nOntPeWbN7Jln1n7mmdmRmUiSJEl1s0DVAUiSJEmzg4muJEmSaslEV5IkSbVkoitJkqRaMtGVJElSLZnoSpIkqZZMdCXNUyIiI+LTbYwfExGDO1jW2IjYtrtimxe1tz0laV5moitpjiiTyvcjYrlmw0eXyVafWSjzooj4QeOwzOyfmSO6Fu38qaXtKUnzMhNdSXPSs8B+TW8iYl1g0erC6R4RsdC8XL4k1ZWJrqQ56XfAQQ3vDwYuaZwgIkZExGEN74dGxN3NC4qII4AhwP9ExNsRcX05fHp3hIgYFhFXRcSVETEpIu6PiPVaCiwiFoiIkyLi6YiYEBF/iIhlWpl2cESMi4gTI+Jl4MKW4mzsFlC2lp4bETeUsfwrIlZrpfw+5bxfjoj/AreVww+NiMci4o2I+FtErFIOj4g4KyJejYg3I+KhiFinm7bniRHxQhnzExGxTUsxS9LcyERX0px0L/CxiFgrIhYE9gF+PysFZeZ5wKXATzJziczcpZVJdwP+CCwDXAb8KSJ6tDDdscAXga2B5YE3gHPbCOGTZZmrAEd0MOz9gNOApYGngNPbmX5rYC1gh4j4InAKsAfQG7gLuLycbntgK6AfsBTFdp3QwZiAlrdnRKwBHANslJm9gB2AsZ0pV5KqZKIraU5ratXdDngceGE2L29UZl6VmR8APwd6Apu0MN2RwLczc1xmTgGGAXu10W3gQ+B7mTklM9/rYCzXZOZ9mTmVIqkc2M70wzLznbL8I4EfZeZj5fw/BAaWrbofAL2ANYEop3mpgzG1ZRqwCLB2RPTIzLGZ+XQ3lCtJc4SJrqQ57XfA/sBQmnVbmE2eb3qRmR8C4yhabJtbBbg2IiZGxETgMYpE7xOtlDs+Myd3MpaXG16/CyzRzvTPN7xeBTi7Ib7XgQBWyMzbgF9RtEC/EhHnRcTHOhnbTDLzKeB4iqT/1Yi4IiJa2naSNFcy0ZU0R2XmcxQ3pX0BuKaFSd4BFmt4/8m2iuvAIldqehERCwArAi+2MN3zwI6ZuVTDX8/MbK3FufmyZ4g7ItqKu6Mal/E8cGSz+BbNzH8CZOY5mbkh0J+iC8O3WoqLTm7PzLwsM7egSLQT+PGsr44kzVkmupKq8GXgc5n5TgvjRgN7RMRi5Y1cX26jnFeAVdtZ1oYRsUfZBeF4YApFX+HmhgOnN9zg1Tsidmun7EYPAv0jYmBE9KRoBe1Ow4GTI6J/Gd+SEbF3+XqjiPhM2ff4HWAyRWs0dGF7RsQaEfG5iFikLPO9hnIlaa5noitpjsvMpzNzZCujzwLep0i6Lqboy9qa31L0H50YEX9qZZo/U9yc9QZwILBH2V+3ubOB64CbI2ISRTL8mfbWpUlm/gf4PnAL8CQw05MNuiIzr6VoTb0iIt4CHgF2LEd/DDifYh2fo7gR7cxyXFe25yLAGcBrFN0uPk5xQ5wkzRMisyNX/iRp3hMRw4BPZ+YBVcciSZrzbNGVJElSLZnoSpIkqZbsuiBJkqRaskVXkiRJtWSiK0mSpFoy0ZUkSVItmehKkiSplkx0JUmSVEsmupIkSaolE11JkiTVkomuJEmSaslEV5IkSbVkoitJkqRaMtGVJElSLZnoSpIkqZZMdCVJklRLJrqSJEmqJRNdSZIk1ZKJriRJkmrJRFeSJEm1ZKIrSZKkWjLRlSRJUi2Z6EqSJKmWTHQlSZJUSya6mq0iYmxE9Kk6jiYRMSwifl91HAAR8Y+IWL/qONoSEbtGxBXtTDMsIobNoZDUioi4KCKGzuFlDo2Iuzs47UUR8YPZHdO8rNyeF1Udx+wSEWMiYnDVcXSXueX81pnjsJ1yTomI/9cdMc1NOpToRsS+EfGviHgnIl4tX381IqIcf1FEvB8Rb0fE6xHx94hYs2HcD5qV1yciMiIW6uDyMyIejogFGob9oCy7Z0RMjIjPtTDfWRFxVfl6bERsW74eGhHTynjfjohnI+LCiOjXXozlST0jYuM24h3eUPb7EfFBw/u/NpZdvm8a90HDdnw7IoaX5S1evr+xhWWNjYhXImLxhmGHRcSIVmJrWnbTMsZGxEnluDENw6dFxOSG96dExNci4pGIWLihvOMj4oGOfpbNYmn+OTT9Ld/ZsqpW7hcfNFuPVduYfhdgUmY+UL7fNyKeiIg3y2Ps4oj42BxbgVZk5nXAOhExYFbLKPe3d8ptMiEibo2IfTox/+CIGDery2+lzIXLz+zJMraxEXFBlCetiBgREYc1LP/DZp/t9Q1ltVgnNNu/34qIByNi52bTnFd+7h9GC0lqRJwQES+X+8UFEbHILK5v03F/f7Phy5V1zthZKXd26ci2m8VyO1VftjB/S+ezEc3qyieajd8mIh6PiHcj4vaIWKWL8W87q/NXLTP7Z+aI2VV+RHw8Ii6PiBfLY+YfEfGZhvE7RcTdUeQML0fE+RHRq4VylomI8dGF5LGF89szEfGVTsw/x78YZuYPM/Ow2bmMiPhseRy82bzeae/zK6f5WhQ521sRMTIitmhvme0muhHxDeBs4KfAJ4FPAEcBmwMLN0z6k8xcAlgReBW4qL2yO2l5YN/mAzNzMnAlcFCzuBcE9gMubqW8e8p4lwS2Bd4DRkXEOq0FEBEBHAi8Dhzc2nSZeVRmLlGW/0Pgyqb3mbljs2l3bJj2UsrtWP4dVU62FzAF2D4iPtXCIhcCjmstnlYsVS5zL+C7EbFdWQk1xXIXcExDLD8EzgUmAt8ut8eqwGnAlzNzaieX3+SehmU0/b04i2VV7cpm6/FMG9MeBfyu4f0/gM0zc0lgVYrPtEOVXMzCl4xOuhw4ootlrFfuV2tQ1A2/iojvdTWwLrgK2BXYn6IOWA8YBWzTyvQvNvtsd4EO1QlN9cxSwK+BKyJiqYbxDwJfBe5vPmNE7ACcVMbUh2K/OK1TazmzxZvVcfsDz3axzNmlvW03q2alvmxPY125RtPAiFgOuAb4LrAMMJLifKXZYwng38CGFNv7YuCGiFiiHL8kRb26PLAWRb7y0xbK+THwWDfEc0/DOXUv4Ccxl1/FmwPeAS4AvtXCuDY/vzLpPYNiWy4J/Ba4tsz3WtVmohsRSwLfB76amVdl5qQsPJCZQzJzSvN5MvNd4DKg1YRxFv0EOK2Vk/rFwJ4RsVjDsB0o1u+vbRWamdMy8+nM/CpwBzCsjcm3pDhAjgP2jYaWzdnsYGA48BAwpIXxPwW+OSsngcwcCYwBBnZg2g+BLwMnRNHCdz7w68yc6STdHcrWi5Mj4tGIeCOKVveeDeMPj4inoriKcF00tARHRP8oriy8XrbgnNJQ9MIRcUlETIqiFXtQw3wnRsQL5bgnIqK1xKcr67Uw8DmK/Q2AzHw+M19rmGwa8Ok2ysiIODoingSejBauQMSMrZJDy5aMM8tt+WxE7Ngw7dCyxWFSOa5xPxsB7NTV9S7X87XM/B3wFeDkiFi2XP4hEfFYufxnIuLIcvjiFMfw8g0tI8tHxMYRcU/ZMvNSRPyqo8djFC1i2wG7Zea/M3NqZr6Zmedm5m87uUodqhPKY+d3wOLA6g3Dz83MW4HJLcx2MPDbzByTmW8A/wsM7WR8zf2OGRPyg4BLGieIiLXKfWdieXzs2jBu2fJYeysi7gNWazbvmg3H3RMR8aUuxtvitouIRcp9+b/l8T08IhYtxy0XEX8p4389Iu6KhquBtFNftrYOEXEERf37P9GsVb8NewBjMvOPZaPMMGC9KK94dkV5zP4jiiuXE8vjZrNy+PNRXBk6uGH6naK4+vZWOX5Ys/IOiojnorjq8t2Y8SroAhFxUkQ8XY7/Q0Qs00pcrW7/ZmVObDim3ynrrz7luJ0jYnQ5zT+jg1eUMvOZzPx5Zr5UntvPo2iQW6Mcf1lm3pSZ75bH1PkUjXaN8W9Kkb9c2JFldlR5nnyMIsFuWtYf46MrNndGRP9yeIv7WkSsFBHXRNHaPCEiftUs9hbr9+ailfNcNHTtK+vUxqtYU5v2mbIOvrqM49mIOLYT2+G+8hwwU0NQe58fxRf+MZk5KjOTou5aDvh4W8tsr0V3U2AR4M8dXYkoMu8hwAOdmOfXEfHrdia7BniLFir6zPwn8BJFpdLkQOCyTrY0XkNx4mrNwcD1fPSNvMuX0toTESsDgylaey+lWct1aSRFMvLNWSh/E4qD+qmOTJ+ZTwA/Am6j+Dbc1Ram9gyh+NKyGtAP+A5AFF1VfgR8CfgU8BxwRTmuF3ALcBNFEvJp4NaGMnctp10KuA74VTnfGsAxwEaZ2atc7thy3BYRMbGdWHcpK/Yx0fYlqtWBDzNzhsvx5TLeBCYBewK/aGd5XwQ+A6zdznRNPgM8QVEx/AT4bRQWB84BdizXezNgdMN8jwF9onu7UvyZomWt6XL/qxTH08eAQ4CzImKDzHwH2JEZW1RfpPgicEK5LptStHp+tanw8kR7UivL3ha4LzOf74b16FCdEEWLwyHABxT7akf0p2jxbfIg8IkovxzMot9TJOQLRsRaQC/gXw1x9qBYn5spTh5fAy4tjw0orupMpjjmDi3/muZdHPg7RUPHxymuqP266eTdXJnEtHvZsZVt92OK+mAgxfG9AnBqOe4bwDigN8UVyFOAbCiy1fqyrXUoT7qNV912aZj1RxHxWpl4Dm4YPsNnWO7PT5fDu8NnKBpAli1jvgLYiGKbHEBx5aSpNfMdivPHUhRfXL8SEV8s13ttilbzIRSf7ZIU27TJsRT1zdYUdeobFPtCS9rb/gBk5lINrZ1nU1xFfCEiNqBo8TuyXK//A66LsttOB/MFymkHUiRKrZ3ftqJo6GmafsFyvY5pKeauiIiNKPbZkQ2D/0pxPvg4xVWdSwFa2tfK2P5CcQz0ofh8Gu+faLF+byGOVs9zjTLzmIbPZwuKz/zP5ZeW6yn26xUo6t7jo7gC1dFzZYe08Pn9FVgwIj5Tbo9DKc5VL7dVTnuJ7nLAa43JYvntamJEvBcRWzVM+81y5Z6iaH4e2tGVycyvli2qbU5Gcfnn1Gi5n9ollElgeULejda7LbTmRYrm8plE0Vq8N0Xy/AHFpc9Wuy90o4OAhzLzUYpLyP2j5UsfpwJfi4jeHSz3tYh4D7iHooL7UydiuouiArqqbKXoik3K/anp7+lm439Vtna+DpxOceKBokK+IDPvL68snAxsWrYI7Ay8nJk/y8zJ5ZWIfzWUeXdm3piZ0yhaitYrh0+j+GK3dkT0yMyxmfk0QGbenZlLtbEef6D4pt4bOJxiP92vlWmXokhmZ1AuY0k+upw2to3lAfwoM1/PzPfama7Jc5l5frneF1Oc0D5RjvuQoi/uouW36TEN8zXFulQHl9Ou8hh6jfJ4y8wbyisrmZl3UCRarX7pLL/R31u2xo6lOBlu3TB+58w8o5XZl6X4YtwZyzfbT7/UwTphk7JenAycCRyQma92cJlLAG82vG96PVOfwk4YR3Ey3JYi1kuajd+kXO4Zmfl+Zt5GcXLdrzyx7AmcmpnvZOYjzFjH7gyMzcwLy8/lfuBqisuMMykTnbb6QLa47cqT9+HACeX+P4mii1hT17YPKPbtVTLzg8y8q2z9adRafdmpdSidSNGtZAXgPOD6iGhq6W7+GVK+78pn2OjZMtZpFF+2VgK+n5lTMvNm4H3KK0OZOSIzH87MDzPzIYrzSdMxsxdwfVkHvU+xfRq32ZHAtzNzXFnfDgP2ipavsHZk+08XRX/9/YE9y+PocOD/MvNfZavexRRd9zYp16Mj+UJTHvA74LTMbP4ZEBHbURwDpzYMPhb4V2aOaq/8Dmo6v70N3FfG82TTyMy8oDw/NW3T9aK4kt6SjSm+ZHyrPP4mNzt+2qrfG7V6nmtJeYz8CfhaFveUbAT0zszvl3XEMxQt4/uW69TeubJDWvn8JlEcj3dT7BPfA45oa/+C9hPdCcByjTtzZm5WrsSEZvOfWVZcn8zMXRs23FSgR7Nye1CcWD9sZ/kzyMwbgf/Scn/BS4DPRsQKFAftU+WH0hkrUPS1a8nuFOvSdEPYpcCOnUgsZ9VBfPQt70WKy90zJdjlSecvFH36OmI5ikr4mxQtxs0/oxZFcWn2/4BfAsdEGzdcddC95X7T9Ldas/GNrW7PURzolP+nt4xl5tsU++QKFJV9qwcuM377exfoGRELZeZTwPEUFc6rEXFFdPDGuMx8NDNfLCvmf1K0ULR2cnyDNk50mfkCRWt0m087YMZt0xHT1zuLLkYAS2TRyrQPRb/hlyLihpjx0mpTrBM7ubxWlS2HvSmPt4jYMSLuLVvEJwJfoNhHW5u/X9lq+3JEvEWR6LQ6fTMTKE4CnfFis/30D3SsTri3rC+Xprh60NYVo+bepmjhbtL0eqYvSZ10CUVDxH4ULbyNlgeez6K7QJPnKI6r3hSt8M2PySarAJ9p/EJA8YX0k7MYZ2vbrjewGMU9FU3LuakcDsWXxKeAm6O4nD9TndhGfdnpdSgTskllcnkxRX/7L5Sjm3+GlO+7+hk2eaXh9XtlPM2HTe/fGMVNQOPLK0dH8dExszwNn2tZP0xoKGcVir6QTdvkMYqEqaVEqt3t36RstPkVsHtmjm9Y1jeafQYr8VHd364ourFcT7EP/aiF8ZtQtIDvlZn/KYctT5Hofrujy+mApvPbEhT7UH+KuoryqsoZUXQHeYuPGjZaq8dWokhmW7tK3WL93nyizpznynr6Koov803no1Vo9sWfotW+pX1hlrTx+R1G0Yrbn6Kl9wDgL+2dp9tLdO+hyJp3m+WIi8S0T7NhfZm5Mu2o71DsiI39ccnM/1K0NA6h6LbQvKWiI3Yvy2jJwRQ7zX8j4mXgjxTJYWutdl0WEZtRXNY4uTyhv0xxeWK/Vr5Jf4/i2/AKLYybSZmU/YyixaTdb8il71JcZj6Oot/w/3Vwvlm1UsPrlSla3Sn/T797OYpLjssCL1BU2M0T5g7Jog/XFmXZSXGJdJaKAma6bFR6kuI+prY+p4Vofx0av8W+U/5vPC46nGBk5t8yczuKBPBxim/oTdaiaOV6q6PldcBuFEnifeUVmqspWu0+USY3N/LR9mvp2/pvyjhXz8yPUVS0rW3v5m4BNo6IFWc9fKATdUL5ReyrwIGtXJFpyRg+utpA+fqVzJzQyvQddTXFpetnMrN5N4oXgZVixj6tK1McV+MpPrPmx2ST54E7mn0hWCIzO3yneUta2HavUSRw/RuWs2SZTFAmnd/IzFWBXYCvR8t97VuqL9tbh45czm489mf4DMt6ajUaLpfPQZdRfGFYKYsrR8P5KM6XKK4kAdMTjcYuMs9TdG1q3C49yy/lM+jo9i+/EF5LcSNfY6PU88DpzZa1WGZe3pGVLOuTP1Hss0e2MH79cjscmkX/+CYbU9R/j5bH89kU9cTL0c7NTh1RfgG5mmKbQNGKvRvF1ZUl+ShPaq3eex5YuZVzf2dj6eh57pcUX8q+0yyOZ5t9Pr0y8wstF9E57Xx+61FcefhPFlcmbqLYdzdrq8w2E93MnEjRB/PXEbFXRCwRRaf0gRQ3BnTE1cBOEbF9+Q1meYqN1l5rVWsxjQAepuVuAxdT9D3ZnLIVtD1lTH0j4pcULZsz9TktE5JtKC5rDSz/1qPYOWZn94WDKfqLrd2w3HUokpmZOpqX39SupPhW2hlnUHR679nWRBGxXln24eWlgmEUfTcP6eTyOuPoiFgxihsfTuGjvpCXAYdExMDywPghxSWnsRQtNZ+M4tFni0REr2j2iJKWRMQaEfG5srzJFCfTaR0JMiJ2i4ilo7AxxXZqsW97FpfnbqHhUntEDImIlcv5V6HopnFrS/O3UuZ4iorhgHKfPpQOJvsR8Ykonpe7OMUX27eZcb23pp2bOjsqisf2DKHoB/fjMmlbmOJS2nhgahQ3UWzfMNsrwLIx4yW9XhR99t8uW587nExl5i0Ux9W1EbFhFI/56xURR5XbrSPr0ek6oVzX/0fDpdIoHnPWk+Lk1iOKxyU21cuXAF+OiLUjYmmKevOijq5na8oW/M9RtI409y+KL03/ExE9ouhvugtwRRaXRK8BhkXEYlH062xc178A/SLiwHLeHhGxURR9gbsa8/RtVzaQnE/Rj/vjUHwe8VEfwZ0j4tMRERT7yDRaOI5bqS/bW4dXKLopUC5rqYjYofzcFir37a2Av5WTXEvRJWjP8nM+laIr2uNd3SazoBfwemZOLuuo/RvGXUVxj8FmUVy1O40ZvzgOB04v6yYiondEtNgA1pHtXyZrVwOXZmbzp1CcDxwVRQt0RPF4zZ2ihceAtbDsphbI94CDmjemRfHEkZsoLsM3v5nwrxTJ5sDy71SKe40Glvt+l0TRt353PvqS04uivp1AcU7/YbNZZtjXKLo+vAScUW6TnhGxOZ3U0fNcFDcEbw3s32w73ge8FcUNbYuW55t1ouiD3JHlL1AeCz2Kt9Gz3Ofa/fwonsiwU0SsWu4b21H0e36kzYVmZrt/FK2k91Fc5h1PURkeASxcjr8I+EEb8+9C8eieNykudf0UWLRh/HBgeBvzJ/DphvefKYdd1Gy6xSm+ffy1hTLGAtuWr4dSfLBvU1Tqz1EkyWs1TN+nXMZCFJe3RrVQ5vIU/ZHWaSP2YcDvmw2bXnaz4dO3I9CT4hL3Li2U+WuK/rEzrFf5fiWKnXdEK/HMtGyKCm0MxcHfNGwEcFjD+wUpOtH/T7PyBlO0sHyileWNBfq0Mq7xc2j826hh3pOBRykum18MLNYw/1EUXRRepzhBrdgwbh2KRPENiks6J7X0eTT7nAdQ7OeTGspcvpxuS+DtNj7nyykqrLcpWhqPbeeY2omG/ZQisR1HsT+Oo+jrt2xHj4ly2I4Uj4qaCPyMopvLYQ3b+u6WyqBoxbiD4vicWH72azdM9zDF48Ha2seHtRPrO+W2eR24naLybJzmaIqKfSJFv6wraKhTKG5OmVCOX54imXi8LPMuiqfD3N0w/V+BU9qIqelk/hQf1QH/D1i5+f5PsY+PazZ/u3VCK9t8RYqT24CG5WSzv8EN03+93C5vUdwFvkgb63QRMLSjx33DuG0pWuyb3vdv2B8epbis3DSuN8Vx8RbFsfK/zbb7GsANFOeJCRQ3rQ5siK/xM30b2LKNuqHVbUdRP/6Q4s7ttygupR9bTncCRd3RdCx9t1l91GZ92c46rE5x88tEilan3hQn30nlsHuB7VrYvo9TnLxH0Ep92LDeF7Uxfnr8zbcRxbGczaYfB2xRvt6LYj+fVH6Gv2LGunAoxRXYCRRX7l5o+nwoGsW+TtG/exJFvfvDVmJsd/vz0f7YVC80/TUdf58vt+tEiuTuj0Cvclyr+QJFYpYUuUpjuU3rcSFFl8nGcWM6ug+28nl09Pz2KsV54uPl+CUoGkMmlZ/LQTTU6zTb18phK1PsdxMozrvntHG8zHSOKIe3dZ4b1rRPUOyrTQ0fTX+nNNRzl1OcW9+g2O+b9sv2zpWDmbnOG9HBzy8o6vr/lvE/BhzY1meUmUQ5szRbRPFA6MFZtLTOyryHZdECVztRPIy8qYP/XCmKH7Y4MDNbfUxUlI+cycxhcygstSCKX9QakZkXVRyKZlEUPxoyODOHVhzHEhQJ1uqZ+WyVsczNunJ+05wzux80L6kVWfSRmqtlcXmvI88LlTQPK7/U3krRanYmxZWcsVXGJHUHE13Nbr+gG+/W11xpRNUBCCguaY6tOAZ1zWiqqy93o+g2FBTd1PZNL/m25xd4fpvr2XVBkiRJtdTe48UkSZKkeZJdF1qx3HLLZZ8+faoOQ5IkqV2jRo16LTNn949YzXNMdFvRp08fRo4c2f6EkiRJFYuI5j8AI+y6IEmSpJoy0ZUkSVItmehKkiSpluyjK0nSHPbBBx8wbtw4Jk+eXHUomsf07NmTFVdckR49elQdyjzBRFeSpDls3Lhx9OrViz59+hARVYejeURmMmHCBMaNG0ffvn2rDmeeYNcFSZLmsMmTJ7Psssua5KpTIoJll13WKwGdYKIrSVIFTHI1K9xvOsdEV5IkSbVkH11JkirW56QburW8sWfs1K3lSfMqW3QlSdI87Qtf+AITJ04EYIkllujUvMOGDePMM8+cDVG1rSnOsWPHctlll83x5c8vTHQlSdI87cYbb2SppZaa7cuZOnVqt5dpojt7mehKkjQfGjt2LGuttRaHH344/fv3Z/vtt+e9995j9OjRbLLJJgwYMIDdd9+dN954A4DBgwdz4oknsvHGG9OvXz/uuuuuVsueNm0a3/zmN1l33XUZMGAAv/zlLwG49dZbWX/99Vl33XU59NBDmTJlCgB9+vThlFNOYdNNN2XQoEHcf//97LDDDqy22moMHz4cgBEjRrDVVlux++67s/baa3PUUUfx4YcfTp//tddemymOn/70p2y00UYMGDCA733ve9OHn3766ayxxhpsu+22PPHEE21up8GDB3PKKaew9dZbc/bZZzNq1Ci23nprNtxwQ3bYYQdeeuklAM455xzWXnttBgwYwL777gvM3Fq8zjrrMHbs2BnKP+mkk7jrrrsYOHAgZ511FmPGjGHjjTdm4MCBDBgwgCeffLLN+NQ2E11JkuZTTz75JEcffTRjxoxhqaWW4uqrr+aggw7ixz/+MQ899BDrrrsup5122vTpp06dyn333ccvfvGLGYY3d9555/Hss8/ywAMP8NBDDzFkyBAmT57M0KFDufLKK3n44YeZOnUqv/nNb6bPs9JKK3HPPfew5ZZbMnToUK666iruvfdeTj311OnT3HffffzsZz/j4Ycf5umnn+aaa65pNYabb76ZJ598kvvuu4/Ro0czatQo7rzzTkaNGsUVV1zBAw88wDXXXMO///3vdrfTxIkTueOOOzj22GP52te+xlVXXcWoUaM49NBD+fa3vw3AGWecMX19m5LzjjjjjDPYcsstGT16NCeccALDhw/nuOOOY/To0YwcOZIVV1yxw2VpZt6MJknSfKpv374MHDgQgA033JCnn36aiRMnsvXWWwNw8MEHs/fee0+ffo899pg+bfOWyUa33HILRx11FAstVKQZyyyzDA8++CB9+/alX79+08s+99xzOf744wHYddddAVh33XV5++236dWrF7169aJnz57T+99uvPHGrLrqqgDst99+3H333ey1114txnDzzTdz8803s/766wPw9ttv8+STTzJp0iR23313FltssRmW25Z99tkHgCeeeIJHHnmE7bbbDiharj/1qU8BMGDAAIYMGcIXv/hFvvjFL7ZbZms23XRTTj/9dMaNG8cee+zB6quvPstlyRZdSZLmW4ssssj01wsuuOD0hLK96RdccME2+6tm5kzPe83MDpW9wAILzBDXAgssMH1Zzcts65mymcnJJ5/M6NGjGT16NE899RRf/vKX252vJYsvvvj0Mvv37z+9zIcffpibb74ZgBtuuIGjjz6aUaNGseGGGzJ16lQWWmih6d0rgA790MP+++/Pddddx6KLLsoOO+zAbbfd1qlYNSNbdCVJqtjc8jiwJZdckqWXXpq77rqLLbfckt/97nfTW3c7Y/vtt2f48OEMHjyYhRZaiNdff50111yTsWPH8tRTT/HpT396lsq+7777ePbZZ1lllVW48sorOeKII1qddocdduC73/0uQ4YMYYklluCFF16gR48ebLXVVgwdOpSTTjqJqVOncv3113PkkUd2aPlrrLEG48eP55577mHTTTflgw8+4D//+Q9rrbUWzz//PJ/97GfZYostuOyyy3j77bfp06cPf/nLXwC4//77efbZZ2cqs1evXkyaNGn6+2eeeYZVV12VY489lmeeeYaHHnqIz33uc53aTvqIiW7FuvvZieq4ueXEIklzk4svvpijjjqKd999l1VXXZULL7yw02Ucdthh/Oc//2HAgAH06NGDww8/nGOOOYYLL7yQvffem6lTp7LRRhtx1FFHdarcTTfdlJNOOomHH354+o1prdl+++157LHH2HTTTYHicV6///3v2WCDDdhnn30YOHAgq6yyCltuuWWHl7/wwgtz1VVXceyxx/Lmm28ydepUjj/+ePr168cBBxzAm2++SWZywgknsNRSS7HnnntyySWXMHDgQDbaaKPp3TYaDRgwgIUWWoj11luPoUOHMnnyZH7/+9/To0cPPvnJT87QR1mdF+1dSphfDRo0KEeOHDnbl2OiWx0TXUlVeeyxx1hrrbWqDmOeMmLECM4888zpLaTzs5b2n4gYlZmDKgpprmUfXUmSJNWSXRckSdIs+dvf/saJJ544w7C+ffty7bXXdvuyBg8ezODBg7u93CZHH300//jHP2YYdtxxx3HIIYfMtmVq9jPRlSRJs2SHHXZghx12qDqMbnHuuedWHYJmA7suSJIkqZZMdCVJklRLJrqSJEmqJfvoSpJUtWFLdnN5b3ZveV106qmnstVWW7HtttsyePBgzjzzTAYN6tiTsKp6rFhjnD/84Q855ZRT5ujy1T1s0ZUkSbPV97//fbbddtvZvpy2fpa4K374wx/OlnI1+5noSpI0n7rkkksYMGAA6623HgceeCDPPfcc22yzDQMGDGCbbbbhv//9LwBDhw7lK1/5Cp/97GdZddVVueOOOzj00ENZa621GDp06PTyllhiCb7xjW+wwQYbsM022zB+/Pjp81911VUzLf/mm29m0003ZYMNNmDvvffm7bffBuCmm25izTXXZIsttuCaa65pcx2GDRvGEUccwfbbb89BBx3E+PHj2XPPPdloo43YaKONpj8y7I477mDgwIEMHDiQ9ddfn0mTJjFixAh23nnn6WUdc8wxXHTRRTOUf9JJJ/Hee+8xcOBAhgwZwjvvvMNOO+3EeuutxzrrrMOVV17Z6e2uOcdEV5Kk+dCYMWM4/fTTue2223jwwQc5++yzOeaYYzjooIN46KGHGDJkCMcee+z06d944w1uu+02zjrrLHbZZRdOOOEExowZw8MPP8zo0aMBeOedd9hggw24//772XrrrTnttNNaXf5rr73GD37wA2655Rbuv/9+Bg0axM9//nMmT57M4YcfzvXXX89dd93Fyy+/3O66jBo1ij//+c9cdtllHHfccZxwwgn8+9//5uqrr+awww4D4Mwzz+Tcc89l9OjR3HXXXSy66KId2k5nnHEGiy66KKNHj+bSSy/lpptuYvnll+fBBx/kkUce4fOf/3yHylE1THQlSZoP3Xbbbey1114st9xyACyzzDLcc8897L///gAceOCB3H333dOn32WXXYgI1l13XT7xiU+w7rrrssACC9C/f3/Gjh0LwAILLMA+++wDwAEHHDDD/M3de++9PProo2y++eYMHDiQiy++mOeee47HH3+cvn37svrqqxMRHHDAAe2uy6677jo9cb3llls45phjGDhwILvuuitvvfUWkyZNYvPNN+frX/8655xzDhMnTmShhWbtNqV1112XW265hRNPPJG77rqLJZfs5v7V6lbejCZJ0nwoM4mINqdpHL/IIosARTLb9LrpfWt9Y9sqPzPZbrvtuPzyy2cYPnr06Hbjam7xxRef/vrDDz/knnvumanF9qSTTmKnnXbixhtvZJNNNuGWW25hoYUW4sMPP5w+zeTJk9tdVr9+/Rg1ahQ33ngjJ598Mttvvz2nnnpqp+LVnGOLriRJ86FtttmGP/zhD0yYMAGA119/nc0224wrrrgCgEsvvZQtttiiU2V++OGH0/viXnbZZW3Ov8kmm/CPf/yDp556CoB3332X//znP6y55po8++yzPP300wAzJcLt2X777fnVr341/X1Tt4qnn36addddlxNPPJFBgwbx+OOPs8oqq/Doo48yZcoU3nzzTW699dYWy+zRowcffPABAC+++CKLLbYYBxxwAN/85je5//77OxWf5ixbdCVJqloFjwPr378/3/72t9l6661ZcMEFWX/99TnnnHM49NBD+elPf0rv3r258MILO1Xm4osvzpgxY9hwww1Zcskl27xRq3fv3lx00UXst99+TJkyBYAf/OAH9OvXj/POO4+ddtqJ5ZZbji222IJHHnmkwzGcc845HH300QwYMICpU6ey1VZbMXz4cH7xi19w++23s+CCC7L22muz4447ssgii/ClL32JAQMGsPrqq7P++uu3WOYRRxzBgAED2GCDDTjooIP41re+xQILLECPHj34zW9+06ltpDkrMrPqGDotIi4AdgZezcx1ymFXAmuUkywFTMzMgRHRB3gMeKIcd29mHtXeMgYNGpQjR47s7tBn0uekG2b7MtSysWfsVHUIkuZTjz32GGuttVbVYXS7JZZYYvqTEzT7tLT/RMSozOzYw4nnI/Nqi+5FwK+AS5oGZOY+Ta8j4mdA49fjpzNz4JwKTpIkSdWbJxPdzLyzbKmdSRQ92L8EfG6OBiVJ0nxudrbmXnjhhZx99tkzDNt8880599xzZ9syNe+bJxPddmwJvJKZTzYM6xsRDwBvAd/JzLuqCU2SJM2KQw45hEMOOaTqMDSPqWOiux/QeIvmS8DKmTkhIjYE/hQR/TPzreYzRsQRwBEAK6+88hwJVpI0f+rI472k5ubFe6uqVKvHi0XEQsAewPTbPDNzSmZOKF+PAp4G+rU0f2ael5mDMnNQ796950TIkqT5UM+ePZkwYYJJizolM5kwYQI9e/asOpR5Rt1adLcFHs/McU0DIqI38HpmTouIVYHVgWeqClCSpBVXXJFx48Yxfvz4qkPRPKZnz56suOKKVYcxz5gnE92IuBwYDCwXEeOA72Xmb4F9mbHbAsBWwPcjYiowDTgqM1+fk/FKktSoR48e9O3bt+owpNqbJxPdzNyvleFDWxh2NXD17I5JkiRJc5da9dGVJEmSmpjoSpIkqZbmya4LUrcYtmTVEcy/hr3Z/jSSJHWRLbqSJEmqJRNdSZIk1ZKJriRJkmrJRFeSJEm1ZKIrSZKkWjLRlSRJUi2Z6EqSJKmWTHQlSZJUSya6kiRJqiUTXUmSJNWSia4kSZJqyURXkiRJtWSiK0mSpFoy0ZUkSVItmehKkiSplkx0JUmSVEsmupIkSaolE11JkiTVkomuJEmSaslEV5IkSbVkoitJkqRaMtGVJElSLZnoSpIkqZZMdCVJklRLJrqSJEmqJRNdSZIk1ZKJriRJkmrJRFeSJEm1ZKIrSZKkWjLRlSRJUi2Z6EqSJKmWTHQlSZJUSya6kiRJqiUTXUmSJNWSia4kSZJqyURXkiRJtWSiK0mSpFqaJxPdiLggIl6NiEcahg2LiBciYnT594WGcSdHxFMR8URE7FBN1JIkSZqTFqpy4RHxcWBzYHngPeARYGRmftjOrBcBvwIuaTb8rMw8s9ky1gb2BfqXy7klIvpl5rSur4EkSZLmVpW06EbEZyPib8ANwI7Ap4C1ge8AD0fEaRHxsdbmz8w7gdc7uLjdgCsyc0pmPgs8BWzcpRWQJEnSXK+qFt0vAIdn5n+bj4iIhYCdge2AqztZ7jERcRAwEvhGZr4BrADc2zDNuHKYJEmSaqySFt3M/BYwLiK+1MK4qZn5p8zsbJL7G2A1YCDwEvCzcni0FEJLBUTEERExMiJGjh8/vpOLlyRJ0tykspvRyn64X+vG8l7JzGlluefzUfeEccBKDZOuCLzYShnnZeagzBzUu3fv7gpNkiRJFaj6qQs3R8Q3I2KliFim6W9WCoqITzW83Z3ixjaA64B9I2KRiOgLrA7c17WwJUmSNLer9KkLwKHl/6MbhiWwalszRcTlwGBguYgYB3wPGBwRA8v5xwJHAmTmmIj4A/AoMBU42icuSJIk1V+liW5m9p3F+fZrYfBv25j+dOD0WVmWJEmS5k2Vdl2IiMUi4jsRcV75fvWI2LnKmCRJklQPVffRvRB4H9isfD8O+EF14UiSJKkuqk50V8vMnwAfAGTme7T8ODBJkiSpU6pOdN+PiEUpn2sbEasBU6oNSZIkSXVQ9VMXhgE3AStFxKXA5sAhlUYkSZKkWqj6qQs3R8QoYBOKLgvHZeZrVcYkSZKkeqj6qQu3ZuaEzLwhM/+Sma9FxK1VxiRJkqR6qKRFNyJ6AotR/ODD0nx0A9rHgOWriEmSJEn1UlXXhSOB4ymS2lF8lOi+BZxbUUySJEmqkUoS3cw8Gzg7Io7NzHMax0XEIlXEJEmSpHqp+vFiQ1sYds+cDkKSJEn1U1Uf3U8CKwCLRsT6zNhHd7EqYpIkSVK9VNVHdweK1twVgZ83DH8LOKWKgCRJklQvVfXRvRi4OCL2zMyrq4hBkiRJ9VZ1H91/RMRvI+KvABGxdkR8ueKYJEmSVANVJ7oXAn/jo2fn/ofisWOSJElSl1Sd6C6XmX8APgTIzKnAtGpDkiRJUh1Unei+ExHLAgkQEZsAb1YbkiRJkuqgqqcuNPk6cB2wWkT8A+gN7FVtSJIkSaqDShPdzLw/IrYG1qB4lu4TmflBlTFJkiSpHipNdCOiJ/BVYAuK7gt3RcTwzJxcZVySJEma91XddeESYBLwy/L9fsDvgL0ri0iSJEm1UHWiu0Zmrtfw/vaIeLCyaCRJklQbVT914YHySQsARMRngH9UGI8kSZJqopIW3Yh4mKJPbg/goIj4b/l+FeDRKmKSJElSvVTVdWHnipYrSZKk+UQliW5mPlfFciVJkjT/qLqPriRJkjRbmOhKkiSplipNdCNi8YhYoHzdLyJ2jYgeVcYkSZKkeqi6RfdOoGdErADcChwCXFRpRJIkSaqFqhPdyMx3gT2AX2bm7sDaFcckSZKkGqg80Y2ITYEhwA3lsKp/rU2SJEk1UHWiezxwMnBtZo6JiFWB26sNSZIkSXVQaetpZt4B3NHw/hng2OoikiRJUl1U9RPAv8jM4yPieoqf/p1BZu5aQViSJEmqkapadH9X/j+zouVLkiSp5qr6CeBR5f872ptWkiRJmhVV34wmSZIkzRYmupIkSaqlyhLdiFgwIn46i/NeEBGvRsQjDcN+GhGPR8RDEXFtRCxVDu8TEe9FxOjyb3g3rYIkSZLmYpUlupk5DdgwImIWZr8I+HyzYX8H1snMAcB/KJ7P2+TpzBxY/h01SwFLkiRpnlL1r5A9APw5Iv4IvNM0MDOvaWumzLwzIvo0G3Zzw9t7gb26MU5JkiTNY6pOdJcBJgCfaxiWQJuJbgccClzZ8L5vRDwAvAV8JzPv6mL5kiRJmstV/ctoh3R3mRHxbWAqcGk56CVg5cycEBEbAn+KiP6Z+VYL8x4BHAGw8sord3dokiRJmoMqfepCRPSLiFubbiqLiAER8Z0ulHcwsDMwJDMTIDOnZOaE8vUo4GmgX0vzZ+Z5mTkoMwf17t17VsOQJEnSXKDqx4udT3HT2AcAmfkQsO+sFBQRnwdOBHbNzHcbhveOiAXL16sCqwPPdDFuSZIkzeWq7qO7WGbe1+zBC1PbmykiLgcGA8tFxDjgexQJ8yLA38vy7i2fsLAV8P2ImApMA47KzNe7dS0kSZI016k60X0tIlajuAGNiNiLok9tmzJzvxYG/7aVaa8Gru5KkJIkSZr3VJ3oHg2cB6wZES8AzwJDqg1JkiRJdVD1UxeeAbaNiMWBBTJzUpXxSJIkqT6qfurC0xFxKXAgsFKVsUiSJKleqn7qwtrA/wHLAmdGxDMRcW3FMUmSJKkGqk50p1E8Wmwa8CHwCvBqpRFJkiSpFqq+Ge0t4GHg58D5TT/sIEmSJHVV1S26+wF3Al8FroiI0yJim4pjkiRJUg1U/dSFPwN/jog1gR2B44H/ARatMi5JkiTN+6p+6sLVEfE0cDawBHAQsHSVMUmSJKkequ6jewZwf2ZOqzgOSZIk1UzVie5o4OiI2Kp8fwcwPDM/qC4kSZIk1UHVie5vgB7Ar8v3B5bDDqssIkmSJNVC1YnuRpm5XsP72yLiwcqikSRJUm1U/XixaRGxWtObiFiV4scjJEmSpC6pukX3W8DtEfEMEMAqwCHVhiRJkqQ6qPo5urdGxOrAGhSJ7uOZOaXKmCRJklQPlSS6EbFHK6NWiwgy85o5GpAkSZJqp6oW3V3aGJeAia4kSZK6pJJENzPthytJkqTZquqnLkiSJEmzhYmuJEmSaslEV5IkSbU0VyW6ETEoIlaoOg5JkiTN++aqRBf4GvCXiLiy6kAkSZI0b6v6l9FmkJkHA0REr6pjkSRJ0ryt0hbdiNg8IhYvXx8QET+PiFUyc1KVcUmSJGneV3XXhd8A70bEesD/AM8Bl1QbkiRJkuqg6kR3amYmsBtwdmaeDdhtQZIkSV1WdR/dSRFxMnAAsFVELAj0qDgmSZIk1UDVLbr7AFOAL2fmy8AKwE+rDUmSJEl1UGmLbpnc/rzh/X+xj64kSZK6QSWJbkRMArK18Zn5sTkYjiRJkmqokkQ3M3sBRMT3gZeB3wEBDMGb0SRJktQNqu6ju0Nm/jozJ2XmW5n5G2DPimOSJElSDVSd6E6LiCERsWBELBARQ4BpFcckSZKkGqg60d0f+BLwSvm3dzlMkiRJ6pKqn7owluLHIiRJkqRuVWmiGxG9gcOBPo2xZOahVcUkSZKkeqj6l9H+DNwF3IJ9cyVJktSNqk50F8vMEyuOQZIkSTVU9c1of4mIL3R2poi4ICJejYhHGoYtExF/j4gny/9LN4w7OSKeiognImKH7gpekiRJc6+qE93jKJLdyRExqfx7qwPzXQR8vtmwk4BbM3N14NbyPRGxNrAv0L+c59cRsWB3rYAkSZLmTpUmupnZKzMXyMye5eteHfn538y8E3i92eDdgIvL1xcDX2wYfkVmTsnMZ4GngI27Zw0kSZI0t6q6jy4RsSuwVfl2RGb+ZRaL+kRmvgSQmS9FxMfL4SsA9zZMN64cJkmSpBqrtEU3Is6g6L7waPl3XDmsWxfTwrBsJZ4jImJkRIwcP358N4chSZKkOanqPrpfALbLzAsy8wKKPrSdvjmt9EpEfAqg/P9qOXwcsFLDdCsCL7ZUQGael5mDMnNQ7969ZzEMSZIkzQ2qTnQBlmp4vWQXyrkOOLh8fTDFM3qbhu8bEYtERF9gdeC+LixHkiRJ84Cq++j+CHggIm6n6GKwFXByezNFxOXAYGC5iBgHfA84A/hDRHwZ+C+wN0BmjomIP1B0jZgKHJ2Z/jiFJElSzVWa6Gbm5RExAtiIItE9MTNf7sB8+7UyaptWpj8dOH1W45QkSdK8p+qb0XYH3s3M6zLzz8DkiPhilTFJkiSpHqruo/u9zHyz6U1mTqTohiBJkiR1SdWJbkvLr7rfsCRJkmqg6kR3ZET8PCJWi4hVI+IsYFTFMUmSJKkGqk50vwa8D1wJ/AF4Dzi60ogkSZJUC1U/deEd4KSIWCIz364yFkmSJNVL1U9d2Cwimn7+l4hYLyJ+XWVMkiRJqoequy6cBewATADIzAcpfjRCkiRJ6pKqE10y8/lmg/zVMkmSJHVZ1Y/yej4iNgMyIhYGjgUeqzgmSZIk1UDVLbpHUTxlYQVgHDAQn7ogSZKkblD1UxdeA4ZUGYMkSZLqqeqnLvwkIj4WET0i4taIeC0iDqgyJkmSJNVD1V0Xts/Mt4CdKbou9AO+VW1IkiRJqoOqE90e5f8vAJdn5utVBiNJkqT6qPqpC9dHxOMUP/371YjoDUyuOCZJkiTVQKUtupl5ErApMCgzPwDeBXarMiZJkiTVQyWJbkRs0fQ6M9/IzGnl63cy8+XyBrV1qohNkiRJ9VBV14U9I+InwE3AKGA80BP4NPBZYBXgGxXFJkmSpBqoJNHNzBMiYmlgL2Bv4FMU/XQfA/4vM++uIi5JkiTVR2U3o2XmG8D55Z8kSZLUrap+vJgkSZI0W5joSpIkqZZMdCVJklRLlSa6EbFYRHw3Is4v368eETtXGZMkSZLqoeoW3QuBKRQ/GgEwDvhBdeFIkiSpLqpOdFfLzJ8AHwBk5ntAVBuSJEmS6qDqRPf9iFgUSICIWI2ihVeSJEnqksqeo1v6HsWvo60UEZcCmwNDK41IkiRJtVBpopuZf4+I+4FNKLosHJeZr1UZkyRJkuqh6q4LACsACwILA1tFxB4VxyNJkqQaqLRFNyIuAAYAY4APy8EJXFNZUJIkSaqFqvvobpKZa1ccgyRJkmqo6q4L90SEia4kSZK6XdUtuhdTJLsvUzxWLIDMzAHVhiVJkqR5XdWJ7gXAgcDDfNRHV5IkSeqyqhPd/2bmdRXHIEmSpBqqOtF9PCIuA66n4RfRMtOnLkiSJKlLqk50F6VIcLdvGObjxSRJktRlVf8y2iFVLl+SJEn1VUmiGxH/k5k/iYhfUrTgziAzj53FctcArmwYtCpwKrAUcDgwvhx+SmbeOCvLkCRJ0ryhqhbdx8r/I7uz0Mx8AhgIEBELAi8A1wKHAGdl5pnduTxJkiTNvSpJdDPz+vLlu5n5x8ZxEbF3Ny1mG+DpzHwuIrqpSEmSJM0rqv5ltJM7OGxW7Atc3vD+mIh4KCIuiIilu2kZkiRJmktV1Ud3R+ALwAoRcU7DqI8BU7uh/IWBXfkoaf4N8L8U/YH/F/gZcGgL8x0BHAGw8sordzUMSZIkVaiqFt0XKfrnTgZGNfxdB+zQDeXvCNyfma8AZOYrmTktMz8Ezgc2bmmmzDwvMwdl5qDevXt3QxiSJEmqSlV9dB8EHoyIyzLzg9mwiP1o6LYQEZ/KzJfKt7sDj8yGZUqSJGkuUvVzdLs9yY2IxYDtgCMbBv8kIgZSdF0Y22ycJEmSaqjqX0brdpn5LrBss2EHVhSOJEmSKlL1UxckSZKk2aLSFt2I6Ad8C1ilMZbM/FxlQUmSJKkWqu668EdgOMWTEKZVHIskSZJqpOpEd2pm/qbiGCRJklRDVf1gxDLly+sj4qvAtcCUpvGZ+XoVcUmSJKk+qmrRHUXxqK8o33+rYVwCq87xiCRJklQrVf1gRN8qlitJkqT5R6WPF4uIoyNiqYb3S5ddGSRJkqQuqfo5uodn5sSmN5n5BnB4deFIkiSpLqpOdBeIiKZ+ukTEgsDCFcYjSZKkmqj68WJ/A/4QEcMpbkI7Crip2pAkSZJUB1UnuicCRwJfoXgCw83A/6s0IkmSJNVCpYluZn4YEb8F7qZo0X0iM/2FNEmSJHVZpYluRAwGLgbGUrTorhQRB2fmnRWGJUmSpBqouuvCz4DtM/MJgIjoB1wObFhpVJIkSZrnVf3UhR5NSS5AZv4H6FFhPJIkSaqJqlt0R5Z9dH9Xvh9C8fPAkiRJUpdUneh+BTgaOJaij+6dwK8rjUiSJEm1UPVTF6ZExK+AW4EPKZ668H6VMUmSJKkeqn7qwk7AcOBpihbdvhFxZGb+tcq4JEmSNO+ruuvCz4DPZuZTABGxGnADYKIrSZKkLqn6qQuvNiW5pWeAV6sKRpIkSfVRdYvumIi4EfgDxS+j7Q38OyL2AMjMa6oMTpIkSfOuqhPdnsArwNbl+/HAMsAuFImvia4kSZJmSdVPXTikyuVLkiSpvirtoxsR/SLi1oh4pHw/ICK+U2VMkiRJqoeqb0Y7HzgZ+AAgMx8C9q00IkmSJNVC1YnuYpl5X7NhUyuJRJIkSbVSdaL7Wvns3ASIiL2Al6oNSZIkSXVQ9VMXjgbOA9aMiBeAZ4Eh1YYkSZKkOqj6qQvPANtGxOLAApk5qcp4JEmSVB9Vt+gCkJnvVB2DJEmS6qXqPrqSJEnSbGGiK0mSpFqqvOtCRGwG9KEhlsy8pLKAJEmSVAuVJroR8TtgNWA0MK0cnICJriRJkrqk6hbdQcDamZkVxyFJkqSaqbqP7iPAJyuOQZIkSTVUdYvucsCjEXEfMKVpYGbuWl1IkiRJqoOqE91hFS9fkiRJNVX1L6Pd0d1lRsRYYBLFzW1TM3NQRCwDXEnxdIexwJcy843uXrYkSZLmHpX20Y2ITSLi3xHxdkS8HxHTIuKtbij6s5k5MDMHle9PAm7NzNWBW8v3kiRJqrGqb0b7FbAf8CSwKHBYOay77QZcXL6+GPjibFiGJEmS5iJVJ7pk5lPAgpk5LTMvBAZ3tUjg5ogYFRFHlMM+kZkvlct7Cfh4F5chSZKkuVzVN6O9GxELA6Mj4ifAS8DiXSxz88x8MSI+Dvw9Ih7v6IxlYnwEwMorr9zFMCRJklSlqlt0DyxjOAZ4B1gJ2LMrBWbmi+X/V4FrgY2BVyLiUwDl/1dbmfe8zByUmYN69+7dlTAkSZJUsUoT3cx8DgjgU5l5WmZ+vezKMEsiYvGI6NX0Gtie4kcprgMOLic7GPhz1yKXJEnS3K7qpy7sAowGbirfD4yI67pQ5CeAuyPiQeA+4IbMvAk4A9guIp4EtivfS5Ikqcaq7qM7jKJrwQiAzBwdEX1mtbDMfAZYr4XhE4BtZrVcSZIkzXuq7qM7NTPfrDgGSZIk1VDVLbqPRMT+wIIRsTpwLPDPimOSJElSDVTdovs1oD8wBbgceAs4vsqAJEmSVA+Vtuhm5rvAt8s/SZIkqdtUkui292SFzNx1TsUiSZKkeqqqRXdT4HmK7gr/oniWriRJktRtqkp0P0nxPNv9gP2BG4DLM3NMRfFIkiSpZiq5GS0zp2XmTZl5MLAJ8BQwIiK+VkU8kiRJqp/KbkaLiEWAnShadfsA5wDXVBWPJEmS6qWqm9EuBtYB/gqclpmPVBGHJEmS6quqFt0DgXeAfsCxEdPvRQsgM/NjFcUlSZKkmqgk0c3Mqn+oQpIkSTVnwilJkqRaMtGVJElSLZnoSpIkqZZMdCVJklRLJrqSJEmqJRNdSZIk1ZKJriRJkmrJRFeSJEm1ZKIrSZKkWjLRlSRJUi2Z6EqSJKmWTHQlSZJUSya6kiRJqiUTXUmSJNWSia4kSZJqyURXkiRJtWSiK0mSpFoy0ZUkSVItmehKkiSplkx0JUmSVEsmupIkSaolE11JkiTVkomuJEmSaslEV5IkSbVkoitJkqRaMtGVJElSLZnoSpIkqZZMdCVJklRLtUp0I2KliLg9Ih6LiDERcVw5fFhEvBARo8u/L1QdqyRJkmavhaoOoJtNBb6RmfdHRC9gVET8vRx3VmaeWWFskiRJmoNqlehm5kvAS+XrSRHxGLBCtVFJkiSpCrXqutAoIvoA6wP/KgcdExEPRcQFEbF0dZFJkiRpTqhlohsRSwBXA8dn5lvAb4DVgIEULb4/a2W+IyJiZESMHD9+/JwKV5IkSbNB7RLdiOhBkeRempnXAGTmK5k5LTM/BM4HNm5p3sw8LzMHZeag3r17z7mgJUmS1O1qlehGRAC/BR7LzJ83DP9Uw2S7A4/M6dgkSZI0Z9XqZjRgc+BA4OGIGF0OOwXYLyIGAgmMBY6sIjhJkiTNObVKdDPzbiBaGHXjnI5FkiRJ1apV1wVJkiSpiYmuJEmSaslEV5IkSbVkoitJkqRaMtGVJElSLZnoSpIkqZZMdCVJklRLJrqSJEmqJRNdSZIk1ZKJriRJkmrJRFeSJEm1ZKIrSZKkWjLRlSRJUi2Z6EqSJKmWTHQlSZJUSya6kiRJqiUTXUmSJNWSia4kSZJqaaGqA5AkzT/6nHRD1SHMt8aesVPVIUhznC26kiRJqiUTXUmSJNWSXRckSZofDFuy6gjmX8PerDqC+ZYtupIkSaolE11JkiTVkomuJEmSaslEV5IkSbVkoitJkqRaMtGVJElSLZnoSpIkqZZMdCVJklRLJrqSJEmqJRNdSZIk1ZKJriRJkmrJRFeSJEm1ZKIrSZKkWjLRlSRJUi2Z6EqSJKmWTHQlSZJUSya6kiRJqiUTXUmSJNXSfJXoRsTnI+KJiHgqIk6qOh5JkiTNPvNNohsRCwLnAjsCawP7RcTa1UYlSZKk2WW+SXSBjYGnMvOZzHwfuALYreKYJEmSNJvMT4nuCsDzDe/HlcMkSZJUQwtVHcAcFC0MyxkmiDgCOKJ8+3ZEPDHbo1JlApYDXqs6jvnSaS0djpJmJ+u8Cs2ZOm+VObGQec38lOiOA1ZqeL8i8GLjBJl5HnDenAxK1YmIkZk5qOo4JGlOsM7T/Gh+6rrwb2D1iOgbEQsD+wLXVRyTJEmSZpP5pkU3M6dGxDHA34AFgQsyc0zFYUmSJGk2mW8SXYDMvBG4seo4NNewm4qk+Yl1nuY7kZntTyVJkiTNY+anPrqSJEmaj8xXXRc094qIZYFby7efBKYB48v3G5c/8tHavIOAgzLz2E4sbywwqVwOwJ2dmb8D5b+dmUt0V3mS6qkrdV85/2Dg/cz8ZwvjhgI/BV5oGLx/Zj7atainlz8MeDszz+yO8qTZwURXc4XMnAAMhJYrz4hYKDOntjLvSGDkLCz2s5npMyUlVaa9uq8DBgNvAzMluqUrM/OYLoQozdPsuqC5VkRcFBE/j4jbgR9HxMYR8c+IeKD8v0Y53eCI+Ev5elhEXBARIyLimYjoVCttOd8vyvIfiYiNy+HLRMSfIuKhiLg3IgaUw5eIiAsj4uFy3J4NZZ0eEQ+W03+i2zaMpFqLiA0j4o6IGBURf4uIT5XDj42IR8u65oqI6AMcBZwQEaMjYssOlj84Iu6MiGvL8oZHxALluP3K+uyRiPhxwzyfj4j7yzrt1obi1p7V+laaE2zR1dyuH7BtZk6LiI8BW5WPitsW+CGwZwvzrAl8FugFPBERv8nMD1qY7vaIaOq6cHFmnlW+XjwzN4uIrYALgHWA04AHMvOLEfE54BKKVpjvAm9m5roAEbF0UxnAvZn57Yj4CXA48IOubAhJ84UAfgnslpnjI2If4HTgUOAkoG9mTomIpTJzYkQMp+1W4H0iYouG95uW/zcG1gaeA24C9oiIfwI/BjYE3gBujogvAv8Azqeof5+NiGUayutofStVwkRXc7s/ZmZTMrokcHFErE7x8809WpnnhsycAkyJiFeBT1D8Ml5zrXVduBwgM++MiI9FxFLAFpRJdWbeFhHLRsSSwLYUPz5COe6N8uX7wF/K16OA7Tq0tpLmd4tQfLn+e0RA8dz3l8pxDwGXRsSfgD91sLyZui6U5d6Xmc+U7y+nqOM+AEZk5vhy+KXAVhT9hu/MzGcBMvP1huI6Wt9KlTDR1dzunYbX/wvcnpm7l5fsRrQyz5SG19Po/H7e/Jl7SdHK0tJ00cL0AB/kR8/um5UYJM2fAhiTmZu2MG4nisRzV+C7EdG/C8vpaD3XFFNrzyLtan0rzVb20dW8ZEk+unt46Gxczj4A5eW+NzPzTeBOYEg5fDDwWma+BdwMTG8taei6IEmzYgrQOyI2BYiIHhHRv+xDu1Jm3g78D7AUsATF02N6zcJyNo6IvmW5+wB3A/8Cto6I5SJiQWA/4A7gnnJ43zKmZVorVJrbmOhqXvIT4EcR8Q+Ky3lddXt5A8foiLikYfgbZV+14cCXy2HDgEER8RBwBnBwOfwHwNLljRsPUvRVk6RZ9SGwF8UNuA8Co4HNKOq830fEw8ADwFmZORG4Hti9jZvR9mmo50ZHxGbl8Hso6rJHgGeBazPzJeBk4HbgQeD+zPxz2ZXhCOCaMqYrZ8uaS7OBv4wmNYiIEcA3y0eWSVLtlFelvpmZO1ccijTb2aIrSZKkWrJFV5IkSbVki64kSZJqyURXkiRJtWSiK0mSpFoy0ZUkSVItmehKkiSplkx0JUmSVEv/H5PyfAcOfleQAAAAAElFTkSuQmCC","text/plain":["<Figure size 720x504 with 1 Axes>"]},"metadata":{"needs_background":"light"},"output_type":"display_data"}],"source":["# Create a directory to save the multi-run figure to\n","os.makedirs(\"pytorch_2_results/figures\", exist_ok=True)\n","\n","# Create a path to save the figure for multiple runs\n","save_path_multi_run = f\"pytorch_2_results/figures/multi_run_{GPU_NAME}_{MODEL_NAME}_{DATASET_NAME}_{IMAGE_SIZE}_train_epoch_time.png\"\n","\n","# Plot the mean epoch times for experiment 3 and 4\n","plot_mean_epoch_times(non_compiled_results=non_compile_results_multiple_runs_df,\n","                      compiled_results=compile_results_multiple_runs_df,\n","                      multi_runs=True,\n","                      num_runs=NUM_RUNS,\n","                      save_path=save_path_multi_run,\n","                      save=True)"]},{"cell_type":"markdown","metadata":{"id":"8fhHuk9XkigX"},"source":["Nice!\n","\n","Looks like the compiled model edges out the non-compiled model across multiple runs.\n","\n","This is likely because on a single run (with a low amount of epochs), the compiling of the model takes quite a bit of time for the first epoch to run.\n","\n","However, when the model has already been compiled and starts training for longer, the speedups from the behind the scenes optimizations start to show.\n","\n","A possible extension would be to let the model train for a longer time, say 100 epochs, and see how the results compare."]},{"cell_type":"markdown","metadata":{"id":"-zF4IweJkigX"},"source":["\n","### 4.4 Save multi run results to file with GPU details\n","\n","Let's also save our results dataframes for experiments 3 and 4 to file to in case we'd like to inspect them later or compare them to other kinds of models."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TWUXsYMvkigX","outputId":"f707c11f-c7b7-4aaf-b8e0-6f3a3c0a3427"},"outputs":[{"name":"stdout","output_type":"stream","text":["[INFO] Saving experiment 3 non-compiled results to: pytorch_2_results/multi_run_results/single_run_non_compiled_results_CIFAR10_ResNet50_NVIDIA_TITAN_RTX.csv\n","[INFO] Saving experiment 4 compiled results to: pytorch_2_results/multi_run_results/single_run_compiled_results_CIFAR10_ResNet50_NVIDIA_TITAN_RTX.csv\n"]}],"source":["# Make a directory for multi_run results\n","import os\n","pytorch_2_results_dir = \"pytorch_2_results\"\n","pytorch_2_multi_run_results_dir = f\"{pytorch_2_results_dir}/multi_run_results\"\n","os.makedirs(pytorch_2_multi_run_results_dir, exist_ok=True)\n","\n","# Create filenames for each of the dataframes\n","save_name_for_multi_run_non_compiled_results = f\"multi_run_non_compiled_results_{NUM_RUNS}_runs_{DATASET_NAME}_{MODEL_NAME}_{GPU_NAME}.csv\"\n","save_name_for_multi_run_compiled_results = f\"multi_run_compiled_results_{NUM_RUNS}_runs_{DATASET_NAME}_{MODEL_NAME}_{GPU_NAME}.csv\"\n","\n","# Create filepaths to save the results to\n","multi_run_no_compile_save_path = f\"{pytorch_2_multi_run_results_dir}/{save_name_for_non_compiled_results}\"\n","multi_run_compile_save_path = f\"{pytorch_2_multi_run_results_dir}/{save_name_for_compiled_results}\"\n","print(f\"[INFO] Saving experiment 3 non-compiled results to: {multi_run_no_compile_save_path}\")\n","print(f\"[INFO] Saving experiment 4 compiled results to: {multi_run_compile_save_path}\")\n","\n","# Save the results\n","non_compile_results_multiple_runs_df.to_csv(multi_run_no_compile_save_path)\n","compile_results_multiple_runs_df.to_csv(multi_run_compile_save_path)"]},{"cell_type":"markdown","metadata":{"id":"62lw1UBnkiga"},"source":["## 5. Possible improvements and extensions\n","\n","We've explored the fundamentals of `torch.compile()` and wrote code for several experiments to test how it performs.\n","\n","But there's still more we could do.\n","\n","As we've discussed, many of the speedups in PyTorch 2.0 and `torch.compile()` come from using newer GPUs (e.g. A100 and above) and using as much of the GPU as possible (larger batch sizes, larger model sizes).\n","\n","For even more speedups, I'd recommend researching/trying the following:\n","\n","* **More powerful CPUs** - I have a sneaking suspicion that Google Colab instances are limited to 2 CPU cores, speedup numbers could be improved with more CPUs. This could be tracked via the [PyTorch Profiler](https://pytorch.org/tutorials/recipes/recipes/profiler_recipe.html) (a tool to find what processes take what time).\n","* **Using mixed precision training** - newer GPUs have the ability to handle difference precision types (e.g. [`torch.float16`](https://pytorch.org/docs/stable/tensors.html#data-types) and [`torch.bfloat16`](https://pytorch.org/docs/stable/generated/torch.Tensor.bfloat16.html)) which enable faster training and inference. I'd suspect you'll see an even larger speedup than we've seen here by using mixed precision training. For more on this, see the [PyTorch documentation for automatic mixed precision](https://pytorch.org/docs/stable/notes/amp_examples.html#amp-examples) (also called AMP) with PyTorch.\n","* **Transformer based models may see more *relative* speedups than convolutional models** - PyTorch 2.0 includes a [stable release for accelerated transformer models](https://pytorch.org/blog/pytorch-2.0-release/#stable-accelerated-pytorch-2-transformers) (models which use the attention mechanism). The main speedups come from an improved implementation of [`scaled_dot_product_attention()`](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html?highlight=scaled_dot_product#torch.nn.functional.scaled_dot_product_attention) which automatically selects the best version of attention to use based on the hardware you're computing on. You can see more in the [dedicated PyTorch tutorial](https://pytorch.org/tutorials/intermediate/scaled_dot_product_attention_tutorial.html).\n","* **Train for longer** - As previously discussed, the speedups from `torch.compile()` are likely to be more noticeable when training for longer. A great exercise would be to train over a longer number of epochs, potentially on a different dataset with a different model (e.g. a transformer) and see how the speedups compare."]},{"cell_type":"markdown","metadata":{"id":"xhpUXV_tkiga"},"source":["## 6. Resources to learn more\n","\n","I've found the following resources to be helpful learning about PyTorch 2.0 and it's upcoming features.\n","\n","* [PyTorch 2.0 launch blog post](https://pytorch.org/get-started/pytorch-2.0/).\n","* [PyTorch 2.0 release notes](https://pytorch.org/blog/pytorch-2.0-release/) (blog post).\n","    * As well as the [GitHub release notes](https://github.com/pytorch/pytorch/releases/tag/v2.0.0) (lots of info here!).\n","* [PyTorch default device context manager docs](https://github.com/pytorch/tutorials/pull/2220/files).\n","* [PyTorch 2.0 video introduction on YouTube](https://youtu.be/WqLKfta5Ijw) (created by yours truly).\n","* See a [tip by Sebastian Raschka](https://twitter.com/rasbt/status/1638297626385719297?s=20) to improve `torch.compile()` by performing an example batch first (warm-up the model) before continuing with further training (this explains the increased speedups with multiple runs)."]}]}