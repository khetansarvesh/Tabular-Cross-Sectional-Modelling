{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fk4o_LS-jc--"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/khetansarvesh/Tabular-Cross-Sectional-Modelling/blob/main/modelling/regression/ANN.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "NU_P4sNH-pky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Using Pytorch Library**"
      ],
      "metadata": {
        "id": "JLpBSPyVoKSi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Processing"
      ],
      "metadata": {
        "id": "bmDWxZux-IgG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(10, 5) # creating sample dataset with just 5 independent features - x1,x2,x3,x4,x5 and 2 rows\n",
        "x"
      ],
      "metadata": {
        "id": "05ekSgoi-Ld-",
        "outputId": "8dd6b52e-b250-416f-87a5-3b5aea427c62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 4.0891e-01, -4.4023e-01, -8.5300e-01,  9.7805e-01,  5.6652e-02],\n",
              "        [ 5.5181e-01, -1.3340e+00,  1.2891e+00,  9.7943e-01,  1.3466e-01],\n",
              "        [-1.6634e+00, -1.0699e+00, -6.9859e-01, -1.6662e+00,  1.3111e+00],\n",
              "        [-8.4231e-02,  2.6872e-01,  6.7192e-01, -1.3386e+00,  5.7466e-01],\n",
              "        [ 8.7959e-01, -2.2094e+00,  9.3218e-01,  2.1339e-01, -5.6719e-01],\n",
              "        [ 8.3101e-01, -2.9186e-01,  6.3879e-01, -7.6856e-01,  1.0273e+00],\n",
              "        [ 8.6117e-01, -1.3429e+00,  6.0611e-02,  9.8263e-02, -2.2480e-01],\n",
              "        [-5.7655e-01, -1.7413e-01, -4.1327e-01, -7.0351e-01, -1.1554e+00],\n",
              "        [ 3.0365e-02, -2.2806e+00, -6.8178e-01, -9.2001e-01, -9.4490e-01],\n",
              "        [-6.3160e-01, -1.0600e+00, -2.4375e+00,  2.1277e-03, -3.5008e-01]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the y data\n",
        "y = torch.randn(10, 1)\n",
        "y"
      ],
      "metadata": {
        "id": "k1tkYPjW-NBe",
        "outputId": "5c961add-e5f5-48e8-8223-bb1149f9dd5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1520],\n",
              "        [-2.3906],\n",
              "        [ 0.7636],\n",
              "        [-0.4156],\n",
              "        [-0.3451],\n",
              "        [ 0.6496],\n",
              "        [-1.1991],\n",
              "        [ 0.5058],\n",
              "        [-1.3132],\n",
              "        [-0.1949]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelling"
      ],
      "metadata": {
        "id": "WFknf05o-1NC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J2P7eZiMj32_"
      },
      "outputs": [],
      "source": [
        "# creating a one hidden layer FFNN for regression\n",
        "class SarveshANN(nn.Module):\n",
        "\n",
        "  def __init__(self, input_size = 5, hidden_size = 10, output_size = 1): # 5 cause we have 5 features, 10 hidden units in hiddlen layer1 and just 1 output unit\n",
        "    super(SarveshANN, self).__init__()\n",
        "    self.model = nn.Sequential(\n",
        "        nn.Linear(input_size, hidden_size),nn.ReLU(),\n",
        "        nn.Linear(hidden_size, output_size),nn.Identity() # nn.Sigmoid() and nn.LeakyReLU()\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.model(x)\n",
        "\n",
        "#Here is an alternative way to define the same class. You can see that we can replace nn.Sequential by defining the individual layers in the __init__ method and connecting the in the forward function.\n",
        "#class MultilayerPerceptron(nn.Module):\n",
        "\n",
        "#  def __init__(self, input_size = 5, hidden_size = 10, output_size = 1):\n",
        "#    super(MultilayerPerceptron, self).__init__()\n",
        "#    self.linear = nn.Linear(input_size, hidden_size)\n",
        "#    self.relu = nn.ReLU()\n",
        "#    self.linear2 = nn.Linear(hidden_size, output_size)\n",
        "#    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "#  def forward(self, x):\n",
        "#    linear = self.linear(x)\n",
        "#    relu = self.relu(linear)\n",
        "#    linear2 = self.linear2(relu)\n",
        "#    output = self.sigmoid(linear2)\n",
        "#    return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cXi0T0FZbV0y",
        "outputId": "1571e8ca-d38c-4a14-a805-86774c8d7291"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SarveshANN(\n",
            "  (model): Sequential(\n",
            "    (0): Linear(in_features=5, out_features=10, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=10, out_features=5, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=5, out_features=1, bias=True)\n",
            "    (5): Identity()\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = SarveshANN()\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d23soYIb2WZ",
        "outputId": "10f279d8-bace-45a0-fc21-53163ac40516"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('model.0.weight', Parameter containing:\n",
              "  tensor([[-0.2559,  0.2649, -0.0537, -0.0286,  0.3346],\n",
              "          [-0.0139,  0.4457,  0.3879,  0.4450, -0.3163],\n",
              "          [ 0.2954,  0.3767,  0.4416,  0.1291, -0.0380],\n",
              "          [-0.1830, -0.0354,  0.2354,  0.3366,  0.3842],\n",
              "          [-0.3997, -0.2525, -0.3182,  0.2405,  0.3490],\n",
              "          [-0.3522,  0.0324,  0.1795, -0.0540, -0.0234],\n",
              "          [ 0.3768, -0.0812,  0.3992,  0.3240,  0.4382],\n",
              "          [ 0.2240,  0.3083,  0.1131,  0.2188,  0.1478],\n",
              "          [ 0.1752,  0.2188, -0.0049, -0.3777,  0.4376],\n",
              "          [-0.2581,  0.0958,  0.3011, -0.3008,  0.2002]], requires_grad=True)),\n",
              " ('model.0.bias', Parameter containing:\n",
              "  tensor([-0.2308,  0.1004, -0.3939,  0.1638, -0.0436, -0.1687, -0.0675,  0.2156,\n",
              "           0.4307, -0.4149], requires_grad=True)),\n",
              " ('model.2.weight', Parameter containing:\n",
              "  tensor([[-0.2661, -0.0350,  0.3118,  0.0334,  0.0921,  0.0074,  0.2894, -0.0901,\n",
              "            0.0221,  0.2854],\n",
              "          [-0.2084, -0.1682, -0.2330, -0.1889, -0.2143, -0.1303, -0.1252, -0.1246,\n",
              "           -0.3036,  0.0225],\n",
              "          [-0.2381,  0.0817, -0.1722, -0.2472,  0.0660,  0.0480, -0.1902, -0.1604,\n",
              "            0.2971, -0.0143],\n",
              "          [-0.1932,  0.2533, -0.1191,  0.2800,  0.0295, -0.1017,  0.1125,  0.1617,\n",
              "           -0.1691, -0.1046],\n",
              "          [-0.1073,  0.0555, -0.1207,  0.1630,  0.1259,  0.1256,  0.1207, -0.2558,\n",
              "            0.0272,  0.3004]], requires_grad=True)),\n",
              " ('model.2.bias', Parameter containing:\n",
              "  tensor([-0.1192, -0.2402, -0.2444,  0.1181,  0.3136], requires_grad=True)),\n",
              " ('model.4.weight', Parameter containing:\n",
              "  tensor([[-0.2047, -0.2851,  0.0763,  0.0863, -0.3460]], requires_grad=True)),\n",
              " ('model.4.bias', Parameter containing:\n",
              "  tensor([0.1409], requires_grad=True))]"
            ]
          },
          "execution_count": 30,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(model.named_parameters()) #alternative to this is model.parameters() function -> these function gives the initial random parameters the model is taking"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "Ema1lnYp_A4L"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2oA2XsdsbN8p"
      },
      "outputs": [],
      "source": [
        "# Define the optimizer\n",
        "import torch.optim as optim\n",
        "adam = optim.Adam(model.parameters(), lr=1e-1)\n",
        "\n",
        "# Define loss using a predefined loss function\n",
        "loss_function = nn.MSELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogl6-Ctmuek6",
        "outputId": "b4e904f2-f4f9-44d7-ac1e-3b35aecb8de7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: traing loss: 0.46382957696914673\n",
            "Epoch 1: traing loss: 0.41810521483421326\n",
            "Epoch 2: traing loss: 0.33339884877204895\n",
            "Epoch 3: traing loss: 0.2539175748825073\n",
            "Epoch 4: traing loss: 0.2533775269985199\n",
            "Epoch 5: traing loss: 0.18649843335151672\n",
            "Epoch 6: traing loss: 0.14389298856258392\n",
            "Epoch 7: traing loss: 0.10955234616994858\n",
            "Epoch 8: traing loss: 0.07010544836521149\n",
            "Epoch 9: traing loss: 0.05134958028793335\n"
          ]
        }
      ],
      "source": [
        "# training for 10 epochs\n",
        "for epoch in range(10):\n",
        "\n",
        "  #Set the gradients to 0\n",
        "  adam.zero_grad()\n",
        "\n",
        "  #forward propagation\n",
        "  y_pred = model(x)\n",
        "  loss = loss_function(y_pred, y)\n",
        "  print(f\"Epoch {epoch}: traing loss: {loss}\")\n",
        "\n",
        "  #backward propagation to compute the gradients\n",
        "  loss.backward()\n",
        "\n",
        "  #Updating weights - Take a step to optimize the weights\n",
        "  adam.step()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrMJ8AmqeCY-",
        "outputId": "95614ecf-51da-4c33-97f2-f8ff7df735f8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([[ 0.0919,  0.4043,  0.0661, -0.2225,  0.8393],\n",
              "         [-0.1179,  1.2639,  0.4499, -0.0146, -0.3028],\n",
              "         [ 0.0106,  0.4343,  0.2046, -0.2028,  0.1867],\n",
              "         [ 0.0155, -0.7481,  0.4118, -0.2486, -0.0780],\n",
              "         [-0.4846, -0.0452, -0.1402,  0.1874,  0.0923],\n",
              "         [-0.0870, -0.1877, -0.2299, -0.1982,  0.5085],\n",
              "         [-0.1209,  0.0378,  0.5342,  0.3766,  0.1330],\n",
              "         [ 0.1514,  0.9357,  0.3862, -0.3541,  0.0459],\n",
              "         [ 0.4894, -0.0513,  0.0158, -0.6740,  0.0521],\n",
              "         [-0.3115, -0.4785,  0.4461, -0.8579,  0.1322]], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([-0.0245,  0.6163, -0.4797,  0.7681, -0.2806, -0.5909, -0.3918,  0.6035,\n",
              "          0.5538, -0.0950], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([[-0.8311, -0.2224,  0.0213, -0.0971, -0.0278,  0.3614,  0.2324, -0.2281,\n",
              "          -0.5414,  0.0140],\n",
              "         [-0.2084, -0.1682, -0.2330, -0.1889, -0.2143, -0.1303, -0.1252, -0.1246,\n",
              "          -0.3036,  0.0225],\n",
              "         [ 0.4306,  0.4888,  0.2491, -0.7698, -0.2877, -0.3806,  0.1916,  0.3423,\n",
              "           0.1539,  0.1210],\n",
              "         [ 0.1611,  0.7229,  0.2285,  0.1945,  0.1785, -0.2081,  0.1291,  0.7284,\n",
              "          -0.4471,  0.1541],\n",
              "         [-0.5596, -0.3539, -0.5394,  0.3081, -0.0510, -0.3657, -0.0052, -0.6545,\n",
              "           0.4191,  0.2309]], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([-0.3173, -0.2402, -0.5249, -0.0013,  0.6672], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([[ 0.0526, -0.2851,  0.0453,  0.4148, -0.6113]], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([-0.0621], requires_grad=True)]"
            ]
          },
          "execution_count": 34,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(model.parameters()) #parameters learnt after training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMCbroAus5H4"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRqE7P9EtvuS",
        "outputId": "20b9999e-f833-4429-d789-67ffc3899b86"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 0.9180],\n",
              "        [-0.5824],\n",
              "        [-0.5866],\n",
              "        [ 0.2653],\n",
              "        [ 0.9249],\n",
              "        [ 0.0908],\n",
              "        [ 1.2653],\n",
              "        [-1.4679],\n",
              "        [-0.4246],\n",
              "        [ 0.1427]], grad_fn=<AddmmBackward>)"
            ]
          },
          "execution_count": 35,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# See how our model performs on the training data\n",
        "y_pred = model(x)\n",
        "y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Without Using Pytorch Library**"
      ],
      "metadata": {
        "id": "Vz_BCzwpn1lT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing"
      ],
      "metadata": {
        "id": "owkG6ECPpEGR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "  # Generate synthetic data\n",
        "  X, y = make_classification(n_samples=1000, n_features=2, n_redundant=0, n_informative=2, n_clusters_per_class=1, random_state=42)\n",
        "\n",
        "  # Reshape y to be column vector\n",
        "  y = y.reshape(-1, 1)\n",
        "\n",
        "  # Split the data\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "6s_CgCLDpGfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelling"
      ],
      "metadata": {
        "id": "_r-YmbgcpHEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SingleHiddenLayerNN:\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        # Initialize weights and biases with small random values\n",
        "        # W1: weights from input to hidden layer (input_size x hidden_size)\n",
        "        # b1: biases for hidden layer (hidden_size x 1)\n",
        "        # W2: weights from hidden to output layer (hidden_size x output_size)\n",
        "        # b2: biases for output layer (output_size x 1)\n",
        "\n",
        "        self.W1 = np.random.randn(self.input_size, self.hidden_size) * 0.1\n",
        "        self.b1 = np.zeros((1, self.hidden_size))\n",
        "        self.W2 = np.random.randn(self.hidden_size, self.output_size) * 0.1\n",
        "        self.b2 = np.zeros((1, self.output_size))\n",
        "\n",
        "        # Store gradients for analysis\n",
        "        self.gradients = {}\n",
        "\n",
        "    def sigmoid(self, z):\n",
        "        \"\"\"Sigmoid activation function\"\"\"\n",
        "        z = np.clip(z, -500, 500) # Clip z to prevent overflow\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def sigmoid_derivative(self, a):\n",
        "        \"\"\"Derivative of sigmoid function\"\"\"\n",
        "        return a * (1 - a)\n",
        "\n",
        "    def forward_pass(self, X):\n",
        "        self.z1 = np.dot(X, self.W1) + self.b1\n",
        "        self.a1 = self.sigmoid(self.z1)\n",
        "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
        "        self.a2 = self.sigmoid(self.z2)\n",
        "        return self.a2\n",
        "\n",
        "    def compute_loss(self, y_true, y_pred):\n",
        "        \"\"\"Compute binary cross-entropy loss\"\"\"\n",
        "        m = y_true.shape[0]\n",
        "        # Clip predictions to prevent log(0)\n",
        "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
        "        loss = -np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)) / m\n",
        "        return loss\n",
        "\n",
        "    def backward_pass(self, X, y_true, y_pred):\n",
        "        m = X.shape[0]  # Number of training examples\n",
        "\n",
        "        # Output layer gradients\n",
        "        # dL/dz2 = a2 - y (for sigmoid + binary cross-entropy)\n",
        "        dz2 = y_pred - y_true\n",
        "\n",
        "        # Gradients for W2 and b2\n",
        "        # dL/dW2 = (1/m) * a1^T @ dz2\n",
        "        # dL/db2 = (1/m) * sum(dz2, axis=0)\n",
        "        dW2 = np.dot(self.a1.T, dz2) / m\n",
        "        db2 = np.sum(dz2, axis=0, keepdims=True) / m\n",
        "\n",
        "        # Hidden layer gradients\n",
        "        # dL/da1 = dz2 @ W2^T\n",
        "        da1 = np.dot(dz2, self.W2.T)\n",
        "\n",
        "        # dL/dz1 = dL/da1 * sigmoid'(z1) = da1 * a1 * (1 - a1)\n",
        "        dz1 = da1 * self.sigmoid_derivative(self.a1)\n",
        "\n",
        "        # Gradients for W1 and b1\n",
        "        # dL/dW1 = (1/m) * X^T @ dz1\n",
        "        # dL/db1 = (1/m) * sum(dz1, axis=0)\n",
        "        dW1 = np.dot(X.T, dz1) / m\n",
        "        db1 = np.sum(dz1, axis=0, keepdims=True) / m\n",
        "\n",
        "        # Store gradients\n",
        "        self.gradients = {\n",
        "            'dW2': dW2, 'db2': db2,\n",
        "            'dW1': dW1, 'db1': db1\n",
        "        }\n",
        "\n",
        "        return dW1, db1, dW2, db2\n",
        "\n",
        "    def update_weights(self):\n",
        "        \"\"\"Update weights using gradient descent\"\"\"\n",
        "        self.W1 -= self.learning_rate * self.gradients['dW1']\n",
        "        self.b1 -= self.learning_rate * self.gradients['db1']\n",
        "        self.W2 -= self.learning_rate * self.gradients['dW2']\n",
        "        self.b2 -= self.learning_rate * self.gradients['db2']\n",
        "\n",
        "    def train(self, X, y, epochs=1000, verbose=True):\n",
        "        \"\"\"Train the neural network\"\"\"\n",
        "        losses = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            # Forward pass\n",
        "            y_pred = self.forward_pass(X)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = self.compute_loss(y, y_pred)\n",
        "            losses.append(loss)\n",
        "\n",
        "            # Backward pass\n",
        "            self.backward_pass(X, y, y_pred)\n",
        "\n",
        "            # Update weights\n",
        "            self.update_weights()\n",
        "\n",
        "            # Print progress\n",
        "            if verbose and epoch % 100 == 0:\n",
        "                accuracy = self.compute_accuracy(y, y_pred)\n",
        "                print(f\"Epoch {epoch}, Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "        return losses\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Make predictions\"\"\"\n",
        "        return self.forward_pass(X)\n",
        "\n",
        "    def predict_binary(self, X):\n",
        "        \"\"\"Make binary predictions (0 or 1)\"\"\"\n",
        "        probabilities = self.predict(X)\n",
        "        return (probabilities > 0.5).astype(int)\n",
        "\n",
        "    def compute_accuracy(self, y_true, y_pred):\n",
        "        \"\"\"Compute classification accuracy\"\"\"\n",
        "        binary_pred = (y_pred > 0.5).astype(int)\n",
        "        return np.mean(binary_pred == y_true)"
      ],
      "metadata": {
        "id": "5RD1HsLSn209"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "a0sTRwkrpsNt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nn = SingleHiddenLayerNN(input_size=2, hidden_size=4, output_size=1, learning_rate=0.1)\n",
        "\n",
        "# Train the model\n",
        "losses = nn.train(X_train, y_train, epochs=1000, verbose=True)"
      ],
      "metadata": {
        "id": "8FdMxnjMpwuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference"
      ],
      "metadata": {
        "id": "2u4q4V09qAkT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_decision_boundary(model, X, y):\n",
        "    \"\"\"Plot the decision boundary of the model\"\"\"\n",
        "    h = 0.1\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                        np.arange(y_min, y_max, h))\n",
        "\n",
        "    mesh_points = np.c_[xx.ravel(), yy.ravel()]\n",
        "    Z = model.predict(mesh_points)\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    plt.contourf(xx, yy, Z, levels=50, alpha=0.8, cmap=plt.cm.RdYlBu)\n",
        "    scatter = plt.scatter(X[:, 0], X[:, 1], c=y.ravel(), cmap=plt.cm.RdYlBu, edgecolors='black')\n",
        "    plt.colorbar(scatter)"
      ],
      "metadata": {
        "id": "5GuB0aWzqJBb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "train_pred = nn.predict(X_train)\n",
        "test_pred = nn.predict(X_test)\n",
        "\n",
        "train_accuracy = nn.compute_accuracy(y_train, train_pred)\n",
        "test_accuracy = nn.compute_accuracy(y_test, test_pred)\n",
        "\n",
        "print(f\"\\nFinal Results:\")\n",
        "print(f\"Train Accuracy: {train_accuracy:.4f}\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "# Plot training loss\n",
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(losses)\n",
        "plt.title('Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True)\n",
        "\n",
        "# Plot decision boundary\n",
        "plt.subplot(1, 2, 2)\n",
        "plot_decision_boundary(nn, X_test, y_test)\n",
        "plt.title('Decision Boundary')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YDxRa3SIqCS8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "brppqfkbk4CY",
        "KxgAuUdIrHj5",
        "iiPb-LnPqQa7",
        "wpvpOzruq_8I",
        "GkJ81p3GUVPM",
        "ZHcL3owPs24z",
        "LMCbroAus5H4"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}