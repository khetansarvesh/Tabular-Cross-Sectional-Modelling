{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"u8v-fez28AbI"},"outputs":[],"source":["import torch\n","import torchvision.datasets as datasets\n","import torchvision.transforms as transforms\n","import torch.nn as nn\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","from pathlib import Path\n","import os\n","device = \"cpu\"\n","_ = torch.manual_seed(0) # Make torch deterministic"]},{"cell_type":"markdown","metadata":{"id":"glQATqUJ8AbJ"},"source":["# **Dataset**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x44BOPb_8AbK"},"outputs":[],"source":["transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n","\n","# Load the MNIST dataset\n","mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n","# Create a dataloader for the training\n","train_loader = torch.utils.data.DataLoader(mnist_trainset, batch_size=10, shuffle=True)\n","\n","# Load the MNIST test set\n","mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n","test_loader = torch.utils.data.DataLoader(mnist_testset, batch_size=10, shuffle=True)"]},{"cell_type":"markdown","metadata":{"id":"hWsP_2zn8AbK"},"source":["# **Modelling**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gO0DgFBF8AbK"},"outputs":[],"source":["class VerySimpleNet(nn.Module):\n","    def __init__(self, hidden_size_1=100, hidden_size_2=100):\n","        super(VerySimpleNet,self).__init__()\n","        self.linear1 = nn.Linear(28*28, hidden_size_1)\n","        self.linear2 = nn.Linear(hidden_size_1, hidden_size_2)\n","        self.linear3 = nn.Linear(hidden_size_2, 10)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, img):\n","        x = img.view(-1, 28*28)\n","        x = self.relu(self.linear1(x))\n","        x = self.relu(self.linear2(x))\n","        x = self.linear3(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rQ1sGT9j8AbL"},"outputs":[],"source":["net = VerySimpleNet().to(device)"]},{"cell_type":"markdown","metadata":{"id":"KRMNX31D8AbL"},"source":["# **Training**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WL7zUMik8AbM","outputId":"fad20a78-608d-44e8-ede8-6c6f78978877"},"outputs":[{"name":"stdout","output_type":"stream","text":["Loaded model from disk\n"]}],"source":["def train(train_loader, net, epochs=5, total_iterations_limit=None):\n","    cross_el = nn.CrossEntropyLoss()\n","    optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n","\n","    total_iterations = 0\n","\n","    for epoch in range(epochs):\n","        net.train()\n","\n","        loss_sum = 0\n","        num_iterations = 0\n","\n","        data_iterator = tqdm(train_loader, desc=f'Epoch {epoch+1}')\n","        if total_iterations_limit is not None:\n","            data_iterator.total = total_iterations_limit\n","        for data in data_iterator:\n","            num_iterations += 1\n","            total_iterations += 1\n","            x, y = data\n","            x = x.to(device)\n","            y = y.to(device)\n","            optimizer.zero_grad()\n","            output = net(x.view(-1, 28*28))\n","            loss = cross_el(output, y)\n","            loss_sum += loss.item()\n","            avg_loss = loss_sum / num_iterations\n","            data_iterator.set_postfix(loss=avg_loss)\n","            loss.backward()\n","            optimizer.step()\n","\n","            if total_iterations_limit is not None and total_iterations >= total_iterations_limit:\n","                return\n","\n","def print_size_of_model(model):\n","    torch.save(model.state_dict(), \"temp_delme.p\")\n","    print('Size (KB):', os.path.getsize(\"temp_delme.p\")/1e3)\n","    os.remove('temp_delme.p')\n","\n","MODEL_FILENAME = 'simplenet_ptq.pt'\n","\n","if Path(MODEL_FILENAME).exists():\n","    net.load_state_dict(torch.load(MODEL_FILENAME))\n","    print('Loaded model from disk')\n","else:\n","    train(train_loader, net, epochs=1)\n","    # Save the model to disk\n","    torch.save(net.state_dict(), MODEL_FILENAME)"]},{"cell_type":"markdown","metadata":{"id":"n2jf06TB8AbM"},"source":["# **Inference**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"itHgHHCF8AbM"},"outputs":[],"source":["def test(model: nn.Module, total_iterations: int = None):\n","    correct = 0\n","    total = 0\n","\n","    iterations = 0\n","\n","    model.eval()\n","\n","    with torch.no_grad():\n","        for data in tqdm(test_loader, desc='Testing'):\n","            x, y = data\n","            x = x.to(device)\n","            y = y.to(device)\n","            output = model(x.view(-1, 784))\n","            for idx, i in enumerate(output):\n","                if torch.argmax(i) == y[idx]:\n","                    correct +=1\n","                total +=1\n","            iterations += 1\n","            if total_iterations is not None and iterations >= total_iterations:\n","                break\n","    print(f'Accuracy: {round(correct/total, 3)}')"]},{"cell_type":"markdown","metadata":{"id":"m0Rv73y58AbM"},"source":["# Print weights and size of the model before quantization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UCeYub8N8AbM","outputId":"0d9603d6-bf2a-46be-c7e8-98830196fc34"},"outputs":[{"name":"stdout","output_type":"stream","text":["Weights before quantization\n","Parameter containing:\n","tensor([[-0.0068,  0.0126, -0.0359,  ...,  0.0154, -0.0028, -0.0045],\n","        [-0.0141, -0.0093, -0.0048,  ..., -0.0146, -0.0003, -0.0243],\n","        [ 0.0251,  0.0601,  0.0120,  ...,  0.0249,  0.0464,  0.0533],\n","        ...,\n","        [ 0.0564,  0.0601,  0.0255,  ...,  0.0201,  0.0394,  0.0024],\n","        [-0.0070,  0.0011,  0.0332,  ...,  0.0135,  0.0135,  0.0130],\n","        [ 0.0103,  0.0049, -0.0092,  ...,  0.0272, -0.0221, -0.0020]],\n","       requires_grad=True)\n","torch.float32\n"]}],"source":["# Print the weights matrix of the model before quantization\n","print('Weights before quantization')\n","print(net.linear1.weight)\n","print(net.linear1.weight.dtype)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iumm4XHn8AbM","outputId":"3d20194f-1c48-49cc-a128-628d7121b684"},"outputs":[{"name":"stdout","output_type":"stream","text":["Size of the model before quantization\n","Size (KB): 360.559\n"]}],"source":["print('Size of the model before quantization')\n","print_size_of_model(net)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pGoGm72z8AbN","outputId":"1f8d09bd-918b-4467-e779-78eda2b8f16e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy of the model before quantization: \n"]},{"name":"stderr","output_type":"stream","text":["Testing:  15%|█▌        | 150/1000 [00:00<00:01, 709.86it/s]"]},{"name":"stderr","output_type":"stream","text":["Testing: 100%|██████████| 1000/1000 [00:01<00:00, 799.89it/s]"]},{"name":"stdout","output_type":"stream","text":["Accuracy: 0.963\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["print(f'Accuracy of the model before quantization: ')\n","test(net)"]},{"cell_type":"markdown","metadata":{"id":"Gd8umoPK8AbN"},"source":["# Insert min-max observers in the model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iz6JdPlG8AbN"},"outputs":[],"source":["class QuantizedVerySimpleNet(nn.Module):\n","    def __init__(self, hidden_size_1=100, hidden_size_2=100):\n","        super(QuantizedVerySimpleNet,self).__init__()\n","        self.quant = torch.quantization.QuantStub()\n","        self.linear1 = nn.Linear(28*28, hidden_size_1)\n","        self.linear2 = nn.Linear(hidden_size_1, hidden_size_2)\n","        self.linear3 = nn.Linear(hidden_size_2, 10)\n","        self.relu = nn.ReLU()\n","        self.dequant = torch.quantization.DeQuantStub()\n","\n","    def forward(self, img):\n","        x = img.view(-1, 28*28)\n","        x = self.quant(x)\n","        x = self.relu(self.linear1(x))\n","        x = self.relu(self.linear2(x))\n","        x = self.linear3(x)\n","        x = self.dequant(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eibDmMWD8AbN","outputId":"b2907ba2-b856-4ac9-d4aa-fadc69e039e1"},"outputs":[{"data":{"text/plain":["QuantizedVerySimpleNet(\n","  (quant): QuantStub(\n","    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n","  )\n","  (linear1): Linear(\n","    in_features=784, out_features=100, bias=True\n","    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n","  )\n","  (linear2): Linear(\n","    in_features=100, out_features=100, bias=True\n","    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n","  )\n","  (linear3): Linear(\n","    in_features=100, out_features=10, bias=True\n","    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n","  )\n","  (relu): ReLU()\n","  (dequant): DeQuantStub()\n",")"]},"execution_count":53,"metadata":{},"output_type":"execute_result"}],"source":["net_quantized = QuantizedVerySimpleNet().to(device)\n","# Copy weights from unquantized model\n","net_quantized.load_state_dict(net.state_dict())\n","net_quantized.eval()\n","\n","net_quantized.qconfig = torch.ao.quantization.default_qconfig\n","net_quantized = torch.ao.quantization.prepare(net_quantized) # Insert observers\n","net_quantized"]},{"cell_type":"markdown","metadata":{"id":"4G-9TAa28AbN"},"source":["# Calibrate the model using the test set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UXEIcyYQ8AbN","outputId":"8b39ca24-650a-4d43-8b7a-d63cb1fabe03"},"outputs":[{"name":"stderr","output_type":"stream","text":["Testing: 100%|██████████| 1000/1000 [00:01<00:00, 750.89it/s]"]},{"name":"stdout","output_type":"stream","text":["Accuracy: 0.963\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["test(net_quantized)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EtRPa2vE8AbN","outputId":"fe5c500d-118b-4e35-cabc-0eb3be29e6a4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Check statistics of the various layers\n"]},{"data":{"text/plain":["QuantizedVerySimpleNet(\n","  (quant): QuantStub(\n","    (activation_post_process): MinMaxObserver(min_val=-0.4242129623889923, max_val=2.821486711502075)\n","  )\n","  (linear1): Linear(\n","    in_features=784, out_features=100, bias=True\n","    (activation_post_process): MinMaxObserver(min_val=-53.58397674560547, max_val=34.898128509521484)\n","  )\n","  (linear2): Linear(\n","    in_features=100, out_features=100, bias=True\n","    (activation_post_process): MinMaxObserver(min_val=-24.331275939941406, max_val=26.62542152404785)\n","  )\n","  (linear3): Linear(\n","    in_features=100, out_features=10, bias=True\n","    (activation_post_process): MinMaxObserver(min_val=-28.273700714111328, max_val=20.937761306762695)\n","  )\n","  (relu): ReLU()\n","  (dequant): DeQuantStub()\n",")"]},"execution_count":55,"metadata":{},"output_type":"execute_result"}],"source":["print(f'Check statistics of the various layers')\n","net_quantized"]},{"cell_type":"markdown","metadata":{"id":"ldhMl8oq8AbN"},"source":["# Quantize the model using the statistics collected"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qDEbz37r8AbN"},"outputs":[],"source":["net_quantized = torch.ao.quantization.convert(net_quantized)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6AKwuJHd8AbN","outputId":"b36e7c18-9a2f-4812-b2d7-9e2871a1328a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Check statistics of the various layers\n"]},{"data":{"text/plain":["QuantizedVerySimpleNet(\n","  (quant): Quantize(scale=tensor([0.0256]), zero_point=tensor([17]), dtype=torch.quint8)\n","  (linear1): QuantizedLinear(in_features=784, out_features=100, scale=0.6967094540596008, zero_point=77, qscheme=torch.per_tensor_affine)\n","  (linear2): QuantizedLinear(in_features=100, out_features=100, scale=0.40123382210731506, zero_point=61, qscheme=torch.per_tensor_affine)\n","  (linear3): QuantizedLinear(in_features=100, out_features=10, scale=0.3874918520450592, zero_point=73, qscheme=torch.per_tensor_affine)\n","  (relu): ReLU()\n","  (dequant): DeQuantize()\n",")"]},"execution_count":57,"metadata":{},"output_type":"execute_result"}],"source":["print(f'Check statistics of the various layers')\n","net_quantized"]},{"cell_type":"markdown","metadata":{"id":"40FjdzA58AbN"},"source":["# Print weights of the model after quantization"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LLH06F0x8AbN","outputId":"c8f5f29c-9365-4220-c346-8a8f1fdd1136"},"outputs":[{"name":"stdout","output_type":"stream","text":["Weights after quantization\n","tensor([[-2,  3, -8,  ...,  4, -1, -1],\n","        [-3, -2, -1,  ..., -3,  0, -6],\n","        [ 6, 14,  3,  ...,  6, 11, 12],\n","        ...,\n","        [13, 14,  6,  ...,  5,  9,  1],\n","        [-2,  0,  8,  ...,  3,  3,  3],\n","        [ 2,  1, -2,  ...,  6, -5,  0]], dtype=torch.int8)\n"]}],"source":["# Print the weights matrix of the model after quantization\n","print('Weights after quantization')\n","print(torch.int_repr(net_quantized.linear1.weight()))"]},{"cell_type":"markdown","metadata":{"id":"cD5LWgIu8AbN"},"source":["# Compare the dequantized weights and the original weights"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nM1EzEq_8AbN","outputId":"108ed82d-13f8-4ef6-9c2c-fa407f570ed4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Original weights: \n","Parameter containing:\n","tensor([[-0.0068,  0.0126, -0.0359,  ...,  0.0154, -0.0028, -0.0045],\n","        [-0.0141, -0.0093, -0.0048,  ..., -0.0146, -0.0003, -0.0243],\n","        [ 0.0251,  0.0601,  0.0120,  ...,  0.0249,  0.0464,  0.0533],\n","        ...,\n","        [ 0.0564,  0.0601,  0.0255,  ...,  0.0201,  0.0394,  0.0024],\n","        [-0.0070,  0.0011,  0.0332,  ...,  0.0135,  0.0135,  0.0130],\n","        [ 0.0103,  0.0049, -0.0092,  ...,  0.0272, -0.0221, -0.0020]],\n","       requires_grad=True)\n","\n","Dequantized weights: \n","tensor([[-0.0087,  0.0131, -0.0348,  ...,  0.0174, -0.0044, -0.0044],\n","        [-0.0131, -0.0087, -0.0044,  ..., -0.0131,  0.0000, -0.0261],\n","        [ 0.0261,  0.0609,  0.0131,  ...,  0.0261,  0.0479,  0.0522],\n","        ...,\n","        [ 0.0566,  0.0609,  0.0261,  ...,  0.0218,  0.0392,  0.0044],\n","        [-0.0087,  0.0000,  0.0348,  ...,  0.0131,  0.0131,  0.0131],\n","        [ 0.0087,  0.0044, -0.0087,  ...,  0.0261, -0.0218,  0.0000]])\n","\n"]}],"source":["print('Original weights: ')\n","print(net.linear1.weight)\n","print('')\n","print(f'Dequantized weights: ')\n","print(torch.dequantize(net_quantized.linear1.weight()))\n","print('')"]},{"cell_type":"markdown","metadata":{"id":"7gzHB7Eq8AbN"},"source":["# Print size and accuracy of the quantized model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nz5vseu18AbO","outputId":"75f894bd-e64d-4334-d603-11279692bd2a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Size of the model after quantization\n","Size (KB): 94.955\n"]}],"source":["print('Size of the model after quantization')\n","print_size_of_model(net_quantized)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t5UU1ytk8AbP","outputId":"f8aa268d-31b1-49a8-dd24-aa62f7f20701"},"outputs":[{"name":"stdout","output_type":"stream","text":["Testing the model after quantization\n"]},{"name":"stderr","output_type":"stream","text":["Testing: 100%|██████████| 1000/1000 [00:01<00:00, 774.94it/s]"]},{"name":"stdout","output_type":"stream","text":["Accuracy: 0.963\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["print('Testing the model after quantization')\n","test(net_quantized)"]}],"metadata":{"kernelspec":{"display_name":"pytorch-cuda","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}